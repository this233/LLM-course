{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa385611",
   "metadata": {},
   "source": [
    "# 企业内训策划案——大模型微调一日实训\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6ee6d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## A0 大模型技术演进及工程科研应用\n",
    "本次课程将简要阐述人工智能技术演进里程碑事件，系统梳理大模型技术从学术探索到产业落地的关键进程，结合国际前沿最新研究成果，深入剖析AI4S以及工程场景中的创新应用。通过典型案例揭示大模型技术如何驱动科研范式变革与工程效率提升，探讨技术瓶颈与未来趋势，为企业智能化转型提供战略参考与实践路径。\n",
    "### 1 大模型技术演进（以openai为例）\n",
    "<!-- https://openai.com/zh-Hans-CN/research/index/publication/?page=10 -->\n",
    "![](./image/1.jpg)\n",
    "\n",
    "2017年，Google引入Transformer模型。\n",
    "\n",
    "2018.06，发布GPT-1。 （GPT: Generative Pre-Training）\n",
    "- GPT1 为 GPT 系列模型**建立了核心架构**\n",
    "    - 仅解码器的 Transformer 架构\n",
    "- 建立了**对自然语言文本进行建模的底层原理**，即预测下一个单词\n",
    "    - GPT: 生成式预训练\n",
    "\n",
    "2019.02，发布GPT-2。\n",
    "认为每个 （NLP） 任务都可以被视为基于世界文本子集的单词预测问题。因此，如果无监督语言建模经过训练，具有足够的能力来恢复世界文本，则能够解决各种任务。（效果仍然较差）\n",
    "- **用一个大型网页数据集WebText进行训练**\n",
    "- **将参数规模从0.12B提高到1.5B**\n",
    "\n",
    "2020.05，发布GPT-3。\n",
    "经验证明，将神经网络扩展到相当大的规模可以导致模型容量的巨大增加。\n",
    "- 探索了scaling law，**大幅增加参数量，从1.5B提升到175B**\n",
    "- 正式引入**In-context Learning**（GPT-2已采用） ，LLM 的预训练和利用收敛到相同的语言建模范式\n",
    "\n",
    "2022.03，发布GPT-3.5。\n",
    "- 在代码数据训练，提升代码能力和思维链能力\n",
    "- 对GPT-3采用与 InstructGPT 类似的三阶段强人类反馈化学习（RLHF）算法\n",
    "    - 收集示范数据SFT，然后迭代进行：（1）用策略模型收集比较数据（比生成式易获取），训练奖励模型RM，从而可用于打分（2）使用2017年提出的PPO（近段策略优化）对策略模型强化学习\n",
    "    - SFT提高指令跟随能力，强化学习缓解有害有毒回答\n",
    "- 上下文长度为4k\n",
    "\n",
    "InstructGPT的三阶段强人类反馈化学习（RLHF）算法：《Training language models to follow instructions with human feedback》\n",
    "![](./image/2.png)\n",
    "\n",
    "\n",
    "2023.03，发布GPT-4（ChatGPT = GPT-3.5/GPT-4 + 交互式网页）。\n",
    "- 进一步探索scaling law，**从175B提升到1.76T**\n",
    "    - 在解决复杂任务方面的能力比 GPT-3.5 更强\n",
    "    - 构建了scaling law的预测方式，从小模型预测大模型的loss\n",
    "- 将文本输入扩展到图像/文本信号（详情可见后续2023.09发布的GPT-4V）\n",
    "- 上下文长度拓展至8k（GPT-4-32k支持32k）\n",
    "    - 2023.11发布的GPT-4 Turbo拓展到128k\n",
    "- 强化学习时引入额外的安全奖励信号，进一步减少有害输出\n",
    "\n",
    "2024.05，发布GPT-4o(omni) 。\n",
    "GPT‑4o 是一种**自回归全模态模型**（参数规模200B），它能够接受文本、音频、图像和视频的任意组合作为输入，并生成文本、音频和图像输出的任意组合。\n",
    "- 自回归全模态模型，音频输入响应接近人类响应时延（平均320ms）\n",
    "- 关键数据集（公开数据+数据合作伙伴）：\n",
    "    - 公开网页数据，视角多样\n",
    "    - 代码与数学数据，有助于结构化逻辑和推理，利于问题解决\n",
    "    - 多模态数据（图像、音频和视频），训练 LLM 如何解读输入的非文本内容并生成非文本输出\n",
    "\n",
    "2024.12，发布OpenAI o1模型（2024.09发布o1-preview）。\n",
    "发现Test-time Scaling，**随着强化学习（训练时间计算）和思考时间（测试时间计算）的增加，o1 的性能也在不断提高**。（参数规模300B）\n",
    "- 通过强化微调（RFT），o1 可以学会磨练自己的思维链，并完善自己使用的策略。\n",
    "\n",
    "2025.02，发布GPT-4.5\n",
    "扩大**无监督学习**的规模，得到更强的可控性、用户意图执行能力和**更高的“情商”**。\n",
    "- 为 GPT‑4.5 开发了可扩展的新技术，能够利用从较小模型提取的数据来训练规模更大、功能更强的模型\n",
    "\n",
    "2025.04，发布GPT-4.1\n",
    "上下文长度达到**1 million**，相比4o提升了编码能力、指令遵循能力、多模态长上下文能力。\n",
    "\n",
    "2025.04，发布OpenAI o3。\n",
    "观察到**大规模强化学习**展现出了与 GPT 系列预训练相同的“**计算量增加 = 性能提升**”的趋势。\n",
    "- 更大规模强化微调（提高一个数量级），并通过强化学习训练工具调用，提升编码、数学、科学\n",
    "- 首次能够基于图像进行思考，在视觉感知任务中表现出了最佳性能\n",
    "\n",
    "2025.08，发布GPT-5。\n",
    "参数规模从GPT-4的1.8T提升到**52T**，并且建立了**统一系统**，包含GPT-5-main（前身GPT‑4o）、GPT‑5 Thinking（前身OpenAI o3）和实时路由器。\n",
    "- 统一系统：路由+推理/非推理\n",
    "- 在**减少幻觉、提升指令遵循能力以及减少阿谀奉承**方面取得了显著进展\n",
    "- 常用场景提升性能（写作、编程和医疗）\n",
    "\n",
    "### 2 大模型的科研工程应用\n",
    "![](./image/4.png)\n",
    "<!-- 《A Survey of Large Language Models》 -->\n",
    "- **一句话把握**: 大模型像“通才顾问”，小模型像“专科医生”。通才适配多任务、交互自然；专科在特定任务上小而精、省成本。实际落地常是“两者协作+工具配套”。\n",
    "\n",
    "#### 2.1 面向研究共同体\n",
    "\n",
    "##### 2.1.1 经典NLP任务（语言理解与生成的“基本功”）\n",
    "- **词/句级任务（相似度、情感）**\n",
    "  - **怎么做**: 识别“这两句话像不像”“评论是好评还是差评”。\n",
    "  - **实际表现**: 小模型用标注数据“专门练”通常更省钱且强；大模型用少量示例快速上手，优势在“通用与省配置”。\n",
    "  - **适用建议**: 有数据、有固定任务→小模型；任务多变、样本很少→大模型。\n",
    "- **序列标注（NER、POS）**\n",
    "  - **怎么做**: 给每个词贴标签，如“人名/地名/机构名”。\n",
    "  - **难点**: 少见类别、名字模糊时，大模型容易理解偏差。\n",
    "  - **改进思路**: 提示里把类别含义讲清楚、给足示例，或让小模型做最后一层“严格判定”。\n",
    "- **信息抽取（关系/事件）**\n",
    "  - **怎么做**: 从句子中抽“谁与谁是什么关系”“发生了什么事”。\n",
    "  - **难点**: 一句话多层关系、跨句信息，纯靠大模型少样本容易漏掉或混淆。\n",
    "  - **常用做法**: 两步走（先粗抽再精炼），或“大模型提建议+小模型做精确判断”。\n",
    "- **文本生成（翻译、摘要）**\n",
    "  - **优势**: 大模型按提示生成流畅文本，能处理“文档级翻译”“带交互的摘要”。\n",
    "  - **短板**: 低资源语言/小众领域，因训练数据少，质量不稳。\n",
    "  - **应对**: 准备术语表/风格示例，必要时加入检索或后校对。\n",
    "\n",
    "- **选型总则**\n",
    "  - **看三件事**: 数据量（是否足够专门训练）、任务多变度（是否常变换目标）、成本要求（训练/推理花费）。\n",
    "  - **稳妥组合**: “大模型做理解与生成框架+小模型做精细判定与效率收口”。\n",
    "\n",
    "##### 2.1.2 信息检索（搜索升级）\n",
    "- **大模型当“重排员”**: 给它一小批候选文档，按相关度排序。\n",
    "  - **优点**: 不用训练，写好指令就能用（逐个打分、两两比较或一组排序都可）。\n",
    "  - **代价**: 算力开销不小，长文档列表容易吃力。\n",
    "- **大模型增强传统检索**\n",
    "  - **数据增强**: 自动标注“相关/不相关”、为文档生成典型查询，训练出更懂用户意图的检索器。\n",
    "  - **查询改写**: 把“含糊问题”改成系统更好懂的表达，或补充背景知识。\n",
    "  - **文档扩展**: 用“可能被搜索到的问法”丰富文档侧，让匹配更准。\n",
    "- **落地建议**: 高频、长文本场景优先用“增强检索”；重点问题页再用“大模型重排”兜底，兼顾体验与成本。\n",
    "\n",
    "##### 2.1.3 推荐系统（“猜你想要”）\n",
    "- **直接用大模型做推荐**\n",
    "  - **方式**: 用提示或指令微调，让它读历史点击“推下一个”；给物品/用户配“语义ID”，让协同关系可读。\n",
    "  - **现实**: 零/少样本往往不如传统ID推荐稳定；指令微调更好但成本高。\n",
    "- **大模型帮传统推荐变强**\n",
    "  - **意图推断**: 总结用户兴趣变化，辅助召回更准。\n",
    "  - **特征编码**: 读商品文案、用户评价，产出更有信息的向量特征。\n",
    "  - **知识“蒸馏”**: 训练时对齐“小模型的隐藏表征”到“大模型风格”，上线只用小模型，既快又省。\n",
    "- **推荐模拟器（Agent）**\n",
    "  - **作用**: 模拟“不同画像的用户”与“不同风格的商品”，反复交互，评估策略在“像真实世界”的环境里是否靠谱。\n",
    "- **实操要点**: 线上以小模型为主，前台延迟敏感；大模型用于离线增强与实验验证。\n",
    "\n",
    "##### 2.1.4 多模态大模型（看图又能说）\n",
    "- **它如何工作**: 先把图片“翻译”为大模型能读的向量，再与文字一起喂给模型，让它“边看边想边回答”。\n",
    "- **训练两步**\n",
    "  - **对齐预训练**: 让“图像表达”和“文字表达”合拍。数据少时只训“连接器”；数据优且细时可微调语言端；巨量数据下可微调视觉端。\n",
    "  - **视觉指令微调**: 用“带图任务说明+期望回答”教会它怎么按照指令完成复杂任务。\n",
    "- **如何评测**\n",
    "  - **看得准**: 识别物体/属性、读图中文字、避免“幻觉（看图说错）”。\n",
    "  - **想得明白**: 回答与图有关的推理题，如空间关系、常识结合图像。\n",
    "  - **评分方式**: 有标准答案的“闭卷打分”，也有由人或模型做“开放式评审”。\n",
    "- **关键点**: 指令数据要真且细；训练时平衡“保留原本语言能力”和“适配新任务”；对安全与事实性要有额外约束（如答案修订、RLHF）。\n",
    "\n",
    "##### 2.1.5 知识图谱增强（用结构化事实托底）\n",
    "- **为什么需要**: 大模型会“编”，而知识图谱存放“谁-关系-谁”的硬知识，能帮它讲真话、查事实。\n",
    "- **两种用法**\n",
    "  - **检索增强**: 先从图谱取一个小子图，转成文字喂给模型。难点是“别把结构信息丢了”，否则理解走样。\n",
    "  - **协同增强**: 把复杂问题拆解，多轮“查→推→再查”，用专用接口高效获取对应关系，逐步凑齐证据链。\n",
    "- **实操注意**: 统一不同知识源的接口、低成本更新事实、把事实当“对齐尺子”减幻觉。\n",
    "\n",
    "##### 2.1.6 用大模型来评测（打分与点评）\n",
    "- **两类结果**\n",
    "  - **分数/排序**: 快速比较好坏、可规模化。\n",
    "  - **文字点评**: 指出问题与改进方向，可反哺对齐训练。\n",
    "- **常见做法**: 多视角给提示（换顺序、换维度、要解释）、多模型讨论求共识、或训练“评测专用模型”。\n",
    "- **风险点**: 偏好长答案、喜欢自己风格等“固有偏见”；面对很强的模型与复杂任务，评测还不够敏锐。\n",
    "\n",
    "#### 2.2 面向具体领域（行业里的“用与管”）\n",
    "- **医疗**\n",
    "  - **能做**: 问诊建议、报告简化、术语解释、专业考试（如Med-PaLM）。\n",
    "  - **要管**: 严控事实错误与不当建议；保护隐私；重要结论需医生复核。\n",
    "- **教育**\n",
    "  - **能做**: 辅助解题、写作润色、个性化学习路径、作业/测验初评。\n",
    "  - **要管**: 防抄袭与依赖、内容偏见、非英语人群的公平获得。\n",
    "- **法律**\n",
    "  - **能做**: 文书检索与摘要、要点提取、写作草稿、法规解释与推理。\n",
    "  - **要管**: 版权合规、隐私保护、避免歧视性输出；关键判断由律师最终把关。\n",
    "- **金融**\n",
    "  - **能做**: 行情解读、情绪分析、事件抽取、风控线索生成；行业模型效果更稳。\n",
    "  - **要管**: 严格风控与合规审查，防止误导信息影响市场。\n",
    "- **科研**\n",
    "  - **能做**: 文献综述、灵感生成、数据探索与可视化、论文写作与初审。\n",
    "  - **要管**: 引用可追溯、数据与结论可复现，减少“似是而非”的表述。\n",
    "- **其他**: LLM for 心理/软件开发等\n",
    "\n",
    "#### 实操清单（通用建议）\n",
    "- **任务匹配**: 频繁、固定、可量化→小模型优先；多变、少样本、交互多→大模型优先。\n",
    "- **组合拳**: 大模型做“理解/生成/编排”，小模型做“判定/索引/提速”，配合检索与知识库兜底。\n",
    "- **数据为王**: 指令要清晰、示例要贴近真实；敏感领域加入术语表与硬性规则。\n",
    "- **成本与可靠性**: 高频路径用轻量方案，关键节点用“大模型精排/复核”；对外输出加校对与溯源。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da3466e",
   "metadata": {},
   "source": [
    "\n",
    "## A1 大模型架构与训练范式\n",
    "为了帮助后续理解大模型微调，此处基于国内顶尖开源模型Qwen和DeepSeek简要介绍大模型架构与训练范式。\n",
    "\n",
    "\n",
    "#### 2025.01，DeepSeek发布DeepSeek-R1\n",
    "\n",
    "\n",
    "#### 2025.01，阿里巴巴发布Qwen2.5-VL\n",
    "![](./image/5.png)\n",
    "\n",
    "\n",
    "#### 2025.04，阿里发布Qwen3\n",
    "![](./image/3.jpg)\n",
    "\n",
    "##### 预训练\n",
    "在预训练方面，Qwen3 的**数据集相比 Qwen2.5 有了显著扩展**。Qwen2.5是在 18T 个 token 上进行预训练的，而 Qwen3 使用的数据量几乎是其两倍，达到了约 36 T个 token，涵盖了 119 种语言和方言。\n",
    "为了构建这个庞大的数据集，我们不仅从网络上收集数据，还从 PDF 文档中提取信息。我们使用 Qwen2.5-VL 从这些文档中提取文本，并用 Qwen2.5 改进提取内容的质量。\n",
    "为了增加数学和代码数据的数量，我们利用 Qwen2.5-Math 和 Qwen2.5-Coder 这两个数学和代码领域的专家模型合成数据，合成了包括教科书、问答对以及代码片段等多种形式的数据。\n",
    "\n",
    "预训练过程分为三个阶段。\n",
    "1. 在第一阶段（S1），模型在超过 30 T个 token 上进行了预训练，上下文长度为 4K token。这一阶段为模型提供了基本的语言技能和通用知识。\n",
    "2. 在第二阶段（S2），我们通过增加知识密集型数据（如 STEM、编程和推理任务）的比例来改进数据集，随后模型又在额外的 5 万亿个 token 上进行了预训练。\n",
    "3. 在最后阶段，我们使用高质量的长上下文数据将上下文长度扩展到 32K token，确保模型能够有效地处理更长的输入。\n",
    "\n",
    "由于模型架构的改进、训练数据的增加以及更有效的训练方法，Qwen3 Dense 基础模型的整体性能与参数更多的Qwen2.5基础模型相当。\n",
    "- 例如，Qwen3-1.7B/4B/8B/14B/32B-Base 分别与 Qwen2.5-3B/7B/14B/32B/72B-Base 表现相当。\n",
    "- 特别是在 STEM、编码和推理等领域，Qwen3 Dense 基础模型的表现甚至超过了更大规模的 Qwen2.5 模型。\n",
    "- 对于 Qwen3 MoE 基础模型，它们在仅使用 10% 激活参数的情况下达到了与 Qwen2.5 Dense 基础模型相似的性能。这带来了训练和推理成本的显著节省。\n",
    "\n",
    "##### 后训练\n",
    "为了开发能够同时具备思考推理和快速响应能力的混合模型，我们实施了一个四阶段的训练流程。该流程包括：（1）长思维链冷启动，（2）长思维链强化学习，（3）思维模式融合，以及（4）通用强化学习。\n",
    "\n",
    "1. 在第一阶段，我们使用多样的的长思维链数据对模型进行了微调，涵盖了数学、代码、逻辑推理和 STEM 问题等多种任务和领域。这一过程旨在为模型配备基本的推理能力。\n",
    "2. 第二阶段的重点是大规模强化学习，利用基于规则的奖励来增强模型的探索和钻研能力。\n",
    "3. 在第三阶段，我们在一份包括长思维链数据和常用的指令微调数据的组合数据上对模型进行微调，将非思考模式整合到思考模型中。确保了推理和快速响应能力的无缝结合。\n",
    "4. 最后，在第四阶段，我们在包括指令遵循、格式遵循和 Agent 能力等在内的 20 多个通用领域的任务上应用了强化学习，以进一步增强模型的通用能力并纠正不良行为。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c41cd",
   "metadata": {},
   "source": [
    "## A1 基本原理：微调的核心价值与基本思路\n",
    "从迁移学习与分布偏移出发，说明监督微调（SFT）与对齐的关系，明确全参数微调（FPFT）与高效参数微调（PEFT）的本质差异与边界条件；通过小规模示例说明目标函数、学习率与批大小等关键超参对收敛与泛化的作用，并给出观察与记录要点。\n",
    "\n",
    "## 监督微调（SFT）与对齐\n",
    "一、从迁移学习与分布偏移看 SFT 与对齐的关系\n",
    "- 预训练语言模型可视为在大规模通用语料上学得一个“语言先验”，近似建模 p0(y|x) 与世界知识。部署到具体任务或产品场景时，数据分布发生多重偏移：\n",
    "  - 协变量偏移：输入形态从网页语料转为指令式、多轮对话、特定领域文体。\n",
    "  - 条件分布偏移：同样的输入，期望的输出风格与结构不同（更简洁、可执行、带步骤）。\n",
    "  - 标签/偏好偏移：用户对“有用、无害、诚实”的偏好与预训练语料暗含的分布不同。\n",
    "- 监督微调（SFT）是典型的迁移学习手段：在人工或高质量标注的指令数据上，用教师强迫的交叉熵目标，以较小学习率对模型进行后验更新，从 p0(y|x) 移向目标分布 pst(y|x)。它主要纠正协变量与条件分布的偏移，令模型变成“会按要求说话”的初始策略。\n",
    "- “对齐”（alignment）比 SFT 范围更广，目标是将模型行为与人类偏好与安全需求对齐。常见流程是：预训练 → SFT（学“怎么按指令说”）→ 偏好优化（如 RM+RLHF、DPO/KTO 等，学“说成我们想要的样子并避免不当内容”）。从分布角度，偏好优化继续在 pst 基础上沿偏好梯度移动，同时通常加 KL 正则，防止离开语言先验过远。\n",
    "- 限制与配套：\n",
    "  - 仅靠 SFT 不等同于完成对齐。SFT 学到的是“参考答案模仿”，难以处理多种可接受答案的偏好权衡，也容易放大训练集的风格或偏见。\n",
    "  - 为减少“语言漂移”和遗忘，可在 SFT 或偏好阶段加入 KL 正则、数据混合（少量通用语料）、长度控制与去重，缓解偏移导致的退化。\n",
    "\n",
    "二、全参数微调（FPFT）与高效参数微调（PEFT）的本质差异与边界\n",
    "- 定义与本质差异\n",
    "  - FPFT：更新模型中几乎所有可训练权重（包含注意力投影、MLP、归一化、嵌入及输出头）。优化自由度高，能大幅移动表示流形与缩放统计，容量上近似无约束；计算/显存开销最大，遗忘与不稳定风险更高。\n",
    "  - PEFT：仅对少量附加或选定参数求梯度（如 LoRA/IA3/Adapter、Prefix/Prompt Tuning、BitFit、仅调归一化尺度等）。本质上是低秩或低维的参数化更新，作为在冻结基座上的“增量映射”。优点是经济、稳定、可组合与易回滚；缺点是表达能力受限，难以进行“全局重构”。\n",
    "- 容量与等价边界\n",
    "  - 低秩近似视角：若目标更新矩阵的主奇异方向数量不大，LoRA 等低秩方法能很好逼近；当需要高秩、跨层协调的大幅结构性改变（例如系统性语义迁移、跨语言或跨模态跃迁）时，PEFT 容量可能不足。\n",
    "  - 理论上的等价需要在所有关键权重上放置足够高秩的适配器，且训练/优化到位；现实中受显存、稳定性与优化难度限制，很少达到。\n",
    "- 何时优先用 PEFT\n",
    "  - 任务分布与预训练邻近：指令跟随、领域小改写、风格化输出、受限资源场景（如单卡）、需要快速多任务切换/合成不同能力。\n",
    "  - 算法/工程考虑：想降低训练/部署成本，保留基座能力，减少遗忘与安全回退风险；量化基座（QLoRA）场景尤为合适。\n",
    "- 何时需要 FPFT\n",
    "  - 分布偏移大：跨语言跨脚本、专业术语体系差异巨大、长上下文机制重塑（如 RoPE/PE 配置与注意力策略调整）、多模态适配需要改动骨干。\n",
    "  - 需要更新嵌入/词表或归一化统计，或对输出头进行系统性重构；要显著改善困惑度与泛化而非仅行为外观。\n",
    "- 细节与边界条件\n",
    "  - 若要变更位置编码或长上下文插值策略，仅在注意力投影做 LoRA 可能不足，需要直接调整相应模块。\n",
    "  - 冻结层归一化和嵌入可提升稳定性，但当目标分布与原分布尺度统计差异大时会限制性能。\n",
    "  - PEFT 对学习率更宽容，但也更依赖适配位置选择（注意力投影 vs MLP）、秩与缩放超参；FPFT 则对优化器与正则更敏感，容易遗忘。\n",
    "\n",
    "三、小规模示例：目标函数、学习率与批大小对收敛与泛化的作用\n",
    "场景与设置\n",
    "- 模型与数据：以一个约百兆参数的解码器模型（如 125M）在 1–2 万条指令-答案配对上进行 SFT；构造三份集：训练集（InD）、验证集（InD-Dev）与域外测试集（OOD，题型/文体不同但主题相关）。\n",
    "- 评测指标：指令任务准确率/严格匹配率、长度与格式合规率、InD 与 OOD 的困惑度/损失、漂移度量（与基座输出的 KL 或自相似度）、灾难性遗忘度量（在通用验证语料上的困惑度变化）。\n",
    "\n",
    "1) 目标函数对行为与泛化的影响\n",
    "- 纯交叉熵 SFT（教师强迫，逐词对齐）：\n",
    "  - 优点：稳定、收敛快，易实现。适合先把模型“说顺”。\n",
    "  - 风险：过度贴合参考答案表述，可能冗长或脆弱；对多样等价答案容忍度低；若样本含噪，易放大偏差。\n",
    "  - 缓解：混入多式答案、轻微标签平滑、长度正则或样例去重；在后续用偏好优化（DPO/RLHF）矫正风格与安全。\n",
    "- 偏好优化（如 DPO/KTO，成对或单偏好）：\n",
    "  - 优点：更直接地优化“更好”的输出，而非唯一参考；对齐效果更好，减少啰嗦/幻觉倾向。\n",
    "  - 风险：需要偏好数据；不当设置会牺牲语言先验，出现退化或模式坍缩。需配 KL 约束与开发集早停。\n",
    "- 序列级目标（例如基于任务分数的最小化风险）：\n",
    "  - 优点：对任务指标更一致；缺点：实现复杂、估计噪声大，常用于成熟阶段或与 SFT 结合。\n",
    "\n",
    "2) 学习率对收敛稳定性与最终性能的影响\n",
    "- FPFT 常用基线：AdamW，学习率 1e-5 到 5e-5，warmup 1–3%，余弦退火；梯度裁剪 0.5–1.0。\n",
    "  - 太小：收敛慢，易停在旧先验附近，行为改变不足。\n",
    "  - 太大：先验被快速“覆盖”，InD 下降后 OOD 明显恶化，遗忘严重，甚至发散。\n",
    "- PEFT（以 LoRA 为例）常用基线：学习率 1e-4 到 1e-3，Lo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8674a9",
   "metadata": {},
   "source": [
    "## A2 微调范式对比：FPFT、LoRA、AdaLoRA、QLoRA\n",
    "建立“任务规模×算力×时限×维护成本”的选型框架：FPFT具备充分表达但资源开销较大；LoRA以低秩分解提升参数效率；AdaLoRA在秩自适应下改善表达与稳定性；QLoRA以量化降低显存占用但需关注量化误差。基于统一数据与评测口径，形成各范式在显存、吞吐与效果上的可比结论，用于实际决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b3f48",
   "metadata": {},
   "source": [
    "## A3标准微调流程：（数据→训练→评估→归档→上线）\n",
    "规范数据治理（清洗、去重、格式统一、审计留痕）、训练配置与日志记录、评估集构造与复现实验的要点；明确版本化管理、随机种子与环境固定等可复现要求，建立从实验到上线的最小闭环与风险控制清单。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7c54c",
   "metadata": {},
   "source": [
    "## A4框架选型与环境准备：unsloth / ms-swift\n",
    "讲解两框架在 SFT/PEFT/对齐中的接口与流水线差异，结合显存与吞吐的经验阈值给出适配建议；完成环境验证与基线运行，记录资源占用、收敛速度与稳定性，为下午的案例训练提供可靠起点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb5d3d",
   "metadata": {},
   "source": [
    "## A5 LoRA / QLoRA 案例（指令遵循小模型）\n",
    "在单卡（≤24GB）条件下完成一次基于 LoRA 或 QLoRA 的指令微调演示；比较不同秩、α、dropout、量化位宽对收敛与输出质量的影响；开展推理验证并记录延迟与效果的关键观察点，沉淀参数选择的经验规则。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600255d",
   "metadata": {},
   "source": [
    "## A6 Prompt 工程与数据合成（小样本强化）\n",
    "以任务结构化为中心设计可迁移的提示模板，采用变量化与多样化策略扩展样本；建立合成数据的质检规则与偏差控制方法，在小样本条件下验证对特定子任务的增益，并总结可复用的模板与筛选标准。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee600d1",
   "metadata": {},
   "source": [
    "## A7 对齐算法概览与入门实践：PPO / DPO / GRPO\n",
    "解析三者在信号来源、稳定性与工程代价上的差异：PPO偏在线、信号获取成本高；DPO基于偏好对，路径简洁、易复现；GRPO通过分组奖励提升可控性。完成一个入门级偏好对齐示例，关注偏好数据构造、质量控制与伦理边界。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3af9a4",
   "metadata": {},
   "source": [
    "## A8 评估与上线：指标体系、灰度与回滚\n",
    "构建任务正确率、遵循度、毒性与稳定性等指标体系，确定A/B与灰度发布流程及回滚触发条件；形成上线监控、异常处置与复盘机制的要点说明，确保从实验到生产的可追溯与可管控。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843b4dd",
   "metadata": {},
   "source": [
    "## A9 扩展课程-大模型幻觉缓解：语义熵与多模型合作\n",
    "采用同义改写得到条件分布，计算语义熵并设定阈值以触发“自动怀疑”；结合多模型一致性裁决降低错误风险；通过对照样例展示前后指标变化，并分析误报与漏报的权衡，给出部署时的建议阈值区间。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
