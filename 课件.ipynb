{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa385611",
   "metadata": {},
   "source": [
    "# 企业内训策划案——大模型微调一日实训\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6ee6d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## A0 大模型技术演进及工程科研应用\n",
    "本次课程将简要阐述人工智能技术演进里程碑事件，系统梳理大模型技术从学术探索到产业落地的关键进程，结合国际前沿最新研究成果，深入剖析AI4S以及工程场景中的创新应用。通过典型案例揭示大模型技术如何驱动科研范式变革与工程效率提升，探讨技术瓶颈与未来趋势，为企业智能化转型提供战略参考与实践路径。\n",
    "### 1 大模型技术演进（以openai为例）\n",
    "<!-- https://openai.com/zh-Hans-CN/research/index/publication/?page=10 -->\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/1.jpg\"  width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "2017年，Google引入Transformer模型。\n",
    "\n",
    "2018.06，发布GPT-1。 （GPT: Generative Pre-Training）\n",
    "- GPT1 为 GPT 系列模型**建立了核心架构**\n",
    "    - 仅解码器的 Transformer 架构\n",
    "- 建立了**对自然语言文本进行建模的底层原理**，即预测下一个单词\n",
    "    - GPT: 生成式预训练\n",
    "\n",
    "2019.02，发布GPT-2。\n",
    "认为每个 （NLP） 任务都可以被视为基于世界文本子集的单词预测问题。因此，如果无监督语言建模经过训练，具有足够的能力来恢复世界文本，则能够解决各种任务。（效果仍然较差）\n",
    "- **用一个大型网页数据集WebText进行训练**\n",
    "- **将参数规模从0.12B提高到1.5B**\n",
    "\n",
    "2020.05，发布GPT-3。\n",
    "经验证明，将神经网络扩展到相当大的规模可以导致模型容量的巨大增加。\n",
    "- 探索了scaling law，**大幅增加参数量，从1.5B提升到175B**\n",
    "- 正式引入**In-context Learning**（GPT-2已采用） ，LLM 的预训练和利用收敛到相同的语言建模范式\n",
    "\n",
    "2022.03，发布GPT-3.5。\n",
    "- 在代码数据训练，提升代码能力和思维链能力\n",
    "- 对GPT-3采用与 InstructGPT 类似的三阶段强人类反馈化学习（RLHF）算法\n",
    "    - 收集示范数据SFT，然后迭代进行：（1）用策略模型收集比较数据（比生成式易获取），训练奖励模型RM，从而可用于打分（2）使用2017年提出的PPO（近段策略优化）对策略模型强化学习\n",
    "    - SFT提高指令跟随能力，强化学习缓解有害有毒回答\n",
    "- 上下文长度为4k\n",
    "\n",
    "InstructGPT的三阶段强人类反馈化学习（RLHF）算法：《Training language models to follow instructions with human feedback》\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/2.png\"  width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "2023.03，发布GPT-4（ChatGPT = GPT-3.5/GPT-4 + 交互式网页）。\n",
    "- 进一步探索scaling law，**从175B提升到1.76T**\n",
    "    - 在解决复杂任务方面的能力比 GPT-3.5 更强\n",
    "    - 构建了scaling law的预测方式，从小模型预测大模型的loss\n",
    "- 将文本输入扩展到图像/文本信号（详情可见后续2023.09发布的GPT-4V）\n",
    "- 上下文长度拓展至8k（GPT-4-32k支持32k）\n",
    "    - 2023.11发布的GPT-4 Turbo拓展到128k\n",
    "- 强化学习时引入额外的安全奖励信号，进一步减少有害输出\n",
    "\n",
    "2024.05，发布GPT-4o(omni) 。\n",
    "GPT‑4o 是一种**自回归全模态模型**（参数规模200B），它能够接受文本、音频、图像和视频的任意组合作为输入，并生成文本、音频和图像输出的任意组合。\n",
    "- 自回归全模态模型，音频输入响应接近人类响应时延（平均320ms）\n",
    "- 关键数据集（公开数据+数据合作伙伴）：\n",
    "    - 公开网页数据，视角多样\n",
    "    - 代码与数学数据，有助于结构化逻辑和推理，利于问题解决\n",
    "    - 多模态数据（图像、音频和视频），训练 LLM 如何解读输入的非文本内容并生成非文本输出\n",
    "\n",
    "2024.12，发布OpenAI o1模型（2024.09发布o1-preview）。\n",
    "发现Test-time Scaling，**随着强化学习（训练时间计算）和思考时间（测试时间计算）的增加，o1 的性能也在不断提高**。（参数规模300B）\n",
    "- 通过强化微调（RFT），o1 可以学会磨练自己的思维链，并完善自己使用的策略。\n",
    "\n",
    "2025.02，发布GPT-4.5\n",
    "扩大**无监督学习**的规模，得到更强的可控性、用户意图执行能力和**更高的“情商”**。\n",
    "- 为 GPT‑4.5 开发了可扩展的新技术，能够利用从较小模型提取的数据来训练规模更大、功能更强的模型\n",
    "\n",
    "2025.04，发布GPT-4.1\n",
    "上下文长度达到**1 million**，相比4o提升了编码能力、指令遵循能力、多模态长上下文能力。\n",
    "\n",
    "2025.04，发布OpenAI o3。\n",
    "观察到**大规模强化学习**展现出了与 GPT 系列预训练相同的“**计算量增加 = 性能提升**”的趋势。\n",
    "- 更大规模强化微调（提高一个数量级），并通过强化学习训练工具调用，提升编码、数学、科学\n",
    "- 首次能够基于图像进行思考，在视觉感知任务中表现出了最佳性能\n",
    "\n",
    "2025.08，发布GPT-5。\n",
    "参数规模从GPT-4的1.8T提升到**52T**，并且建立了**统一系统**，包含GPT-5-main（前身GPT‑4o）、GPT‑5 Thinking（前身OpenAI o3）和实时路由器。\n",
    "- 统一系统：路由+推理/非推理\n",
    "- 在**减少幻觉、提升指令遵循能力以及减少阿谀奉承**方面取得了显著进展\n",
    "- 常用场景提升性能（写作、编程和医疗）\n",
    "\n",
    "### 2 大模型的科研工程应用\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/4.png\"  width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "《A Survey of Large Language Models》\n",
    "\n",
    "- **一句话把握**: 大模型像“通才顾问”，小模型像“专科医生”。通才适配多任务、交互自然；专科在特定任务上小而精、省成本。实际落地常是“两者协作+工具配套”。\n",
    "\n",
    "#### 2.1 面向研究共同体\n",
    "\n",
    "##### 2.1.1 经典NLP任务（语言理解与生成的“基本功”）\n",
    "- **词/句级任务（相似度、情感）**\n",
    "  - **怎么做**: 识别“这两句话像不像”“评论是好评还是差评”。\n",
    "  - **实际表现**: 小模型用标注数据“专门练”通常更省钱且强；大模型用少量示例快速上手，优势在“通用与省配置”。\n",
    "  - **适用建议**: 有数据、有固定任务→小模型；任务多变、样本很少→大模型。\n",
    "- **序列标注（NER、POS）**\n",
    "  - **怎么做**: 给每个词贴标签，如“人名/地名/机构名”。\n",
    "  - **难点**: 少见类别、名字模糊时，大模型容易理解偏差。\n",
    "  - **改进思路**: 提示里把类别含义讲清楚、给足示例，或让小模型做最后一层“严格判定”。\n",
    "- **信息抽取（关系/事件）**\n",
    "  - **怎么做**: 从句子中抽“谁与谁是什么关系”“发生了什么事”。\n",
    "  - **难点**: 一句话多层关系、跨句信息，纯靠大模型少样本容易漏掉或混淆。\n",
    "  - **常用做法**: 两步走（先粗抽再精炼），或“大模型提建议+小模型做精确判断”。\n",
    "- **文本生成（翻译、摘要）**\n",
    "  - **优势**: 大模型按提示生成流畅文本，能处理“文档级翻译”“带交互的摘要”。\n",
    "  - **短板**: 低资源语言/小众领域，因训练数据少，质量不稳。\n",
    "  - **应对**: 准备术语表/风格示例，必要时加入检索或后校对。\n",
    "\n",
    "- **选型总则**\n",
    "  - **看三件事**: 数据量（是否足够专门训练）、任务多变度（是否常变换目标）、成本要求（训练/推理花费）。\n",
    "  - **稳妥组合**: “大模型做理解与生成框架+小模型做精细判定与效率收口”。\n",
    "\n",
    "##### 2.1.2 信息检索（搜索升级）\n",
    "- **大模型当“重排员”**: 给它一小批候选文档，按相关度排序。\n",
    "  - **优点**: 不用训练，写好指令就能用（逐个打分、两两比较或一组排序都可）。\n",
    "  - **代价**: 算力开销不小，长文档列表容易吃力。\n",
    "- **大模型增强传统检索**\n",
    "  - **数据增强**: 自动标注“相关/不相关”、为文档生成典型查询，训练出更懂用户意图的检索器。\n",
    "  - **查询改写**: 把“含糊问题”改成系统更好懂的表达，或补充背景知识。\n",
    "  - **文档扩展**: 用“可能被搜索到的问法”丰富文档侧，让匹配更准。\n",
    "- **落地建议**: 高频、长文本场景优先用“增强检索”；重点问题页再用“大模型重排”兜底，兼顾体验与成本。\n",
    "\n",
    "##### 2.1.3 推荐系统（“猜你想要”）\n",
    "- **直接用大模型做推荐**\n",
    "  - **方式**: 用提示或指令微调，让它读历史点击“推下一个”；给物品/用户配“语义ID”，让协同关系可读。\n",
    "  - **现实**: 零/少样本往往不如传统ID推荐稳定；指令微调更好但成本高。\n",
    "- **大模型帮传统推荐变强**\n",
    "  - **意图推断**: 总结用户兴趣变化，辅助召回更准。\n",
    "  - **特征编码**: 读商品文案、用户评价，产出更有信息的向量特征。\n",
    "  - **知识“蒸馏”**: 训练时对齐“小模型的隐藏表征”到“大模型风格”，上线只用小模型，既快又省。\n",
    "- **推荐模拟器（Agent）**\n",
    "  - **作用**: 模拟“不同画像的用户”与“不同风格的商品”，反复交互，评估策略在“像真实世界”的环境里是否靠谱。\n",
    "- **实操要点**: 线上以小模型为主，前台延迟敏感；大模型用于离线增强与实验验证。\n",
    "\n",
    "##### 2.1.4 多模态大模型（看图又能说）\n",
    "- **它如何工作**: 先把图片“翻译”为大模型能读的向量，再与文字一起喂给模型，让它“边看边想边回答”。\n",
    "- **训练两步**\n",
    "  - **对齐预训练**: 让“图像表达”和“文字表达”合拍。数据少时只训“连接器”；数据优且细时可微调语言端；巨量数据下可微调视觉端。\n",
    "  - **视觉指令微调**: 用“带图任务说明+期望回答”教会它怎么按照指令完成复杂任务。\n",
    "- **如何评测**\n",
    "  - **看得准**: 识别物体/属性、读图中文字、避免“幻觉（看图说错）”。\n",
    "  - **想得明白**: 回答与图有关的推理题，如空间关系、常识结合图像。\n",
    "  - **评分方式**: 有标准答案的“闭卷打分”，也有由人或模型做“开放式评审”。\n",
    "- **关键点**: 指令数据要真且细；训练时平衡“保留原本语言能力”和“适配新任务”；对安全与事实性要有额外约束（如答案修订、RLHF）。\n",
    "\n",
    "##### 2.1.5 知识图谱增强（用结构化事实托底）\n",
    "- **为什么需要**: 大模型会“编”，而知识图谱存放“谁-关系-谁”的硬知识，能帮它讲真话、查事实。\n",
    "- **两种用法**\n",
    "  - **检索增强**: 先从图谱取一个小子图，转成文字喂给模型。难点是“别把结构信息丢了”，否则理解走样。\n",
    "  - **协同增强**: 把复杂问题拆解，多轮“查→推→再查”，用专用接口高效获取对应关系，逐步凑齐证据链。\n",
    "- **实操注意**: 统一不同知识源的接口、低成本更新事实、把事实当“对齐尺子”减幻觉。\n",
    "\n",
    "##### 2.1.6 用大模型来评测（打分与点评）\n",
    "- **两类结果**\n",
    "  - **分数/排序**: 快速比较好坏、可规模化。\n",
    "  - **文字点评**: 指出问题与改进方向，可反哺对齐训练。\n",
    "- **常见做法**: 多视角给提示（换顺序、换维度、要解释）、多模型讨论求共识、或训练“评测专用模型”。\n",
    "- **风险点**: 偏好长答案、喜欢自己风格等“固有偏见”；面对很强的模型与复杂任务，评测还不够敏锐。\n",
    "\n",
    "#### 2.2 面向具体领域（行业里的“用与管”）\n",
    "- **医疗**\n",
    "  - **能做**: 问诊建议、报告简化、术语解释、专业考试（如Med-PaLM）。\n",
    "  - **要管**: 严控事实错误与不当建议；保护隐私；重要结论需医生复核。\n",
    "- **教育**\n",
    "  - **能做**: 辅助解题、写作润色、个性化学习路径、作业/测验初评。\n",
    "  - **要管**: 防抄袭与依赖、内容偏见、非英语人群的公平获得。\n",
    "- **法律**\n",
    "  - **能做**: 文书检索与摘要、要点提取、写作草稿、法规解释与推理。\n",
    "  - **要管**: 版权合规、隐私保护、避免歧视性输出；关键判断由律师最终把关。\n",
    "- **金融**\n",
    "  - **能做**: 行情解读、情绪分析、事件抽取、风控线索生成；行业模型效果更稳。\n",
    "  - **要管**: 严格风控与合规审查，防止误导信息影响市场。\n",
    "- **科研**\n",
    "  - **能做**: 文献综述、灵感生成、数据探索与可视化、论文写作与初审。\n",
    "  - **要管**: 引用可追溯、数据与结论可复现，减少“似是而非”的表述。\n",
    "- **其他**: LLM for 心理/软件开发等\n",
    "\n",
    "#### 实操清单（通用建议）\n",
    "- **任务匹配**: 频繁、固定、可量化→小模型优先；多变、少样本、交互多→大模型优先。\n",
    "- **组合拳**: 大模型做“理解/生成/编排”，小模型做“判定/索引/提速”，配合检索与知识库兜底。\n",
    "- **数据为王**: 指令要清晰、示例要贴近真实；敏感领域加入术语表与硬性规则。\n",
    "- **成本与可靠性**: 高频路径用轻量方案，关键节点用“大模型精排/复核”；对外输出加校对与溯源。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b72b2",
   "metadata": {},
   "source": [
    "### 3 大模型架构与训练范式\n",
    "为了帮助后续理解大模型微调，此处基于国内顶尖开源模型Qwen和DeepSeek简要介绍大模型架构与训练范式。\n",
    "\n",
    "\n",
    "\n",
    "#### 2025.01，阿里巴巴发布Qwen2.5-VL\n",
    "##### 模型架构\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/5.png\"  width=\"1000\"/>\n",
    "</p>\n",
    "多模态模型主要由视觉编码器（Vision Encoder）、语言模型（LM）和多模态融合模块（Connector）三块构成，和Qwen2-VL一样，Qwen2.5-VL并没有巨大的Connector，仅用一个MLP完成特征投影。打印模型结构如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453c39b",
   "metadata": {},
   "source": [
    "\n",
    "```text\n",
    "Qwen2_5_VLForConditionalGeneration(\n",
    "  (model): Qwen2_5_VLModel(\n",
    "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
    "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
    "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
    "      )\n",
    "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
    "      (blocks): ModuleList(\n",
    "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
    "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
    "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
    "          (attn): Qwen2_5_VLVisionAttention(\n",
    "            (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
    "            (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          )\n",
    "          (mlp): Qwen2_5_VLMLP(\n",
    "            (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
    "            (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
    "            (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
    "            (act_fn): SiLU()\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "      (merger): Qwen2_5_VLPatchMerger(\n",
    "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
    "        (mlp): Sequential(\n",
    "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
    "          (1): GELU(approximate='none')\n",
    "          (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (language_model): Qwen2_5_VLTextModel(\n",
    "      (embed_tokens): Embedding(151936, 2048)\n",
    "      (layers): ModuleList(\n",
    "        (0-35): 36 x Qwen2_5_VLDecoderLayer(\n",
    "          (self_attn): Qwen2_5_VLAttention(\n",
    "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
    "            (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
    "            (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
    "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
    "          )\n",
    "          (mlp): Qwen2MLP(\n",
    "            (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
    "            (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
    "            (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
    "            (act_fn): SiLU()\n",
    "          )\n",
    "          (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
    "          (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
    "        )\n",
    "      )\n",
    "      (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
    "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
    "    )\n",
    "  )\n",
    "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e95b69",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Qwen2_5_VLMLP的SwiGLU结构：\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/7.png\"  width=\"500\"/>\n",
    "</p>\n",
    "<!-- https://zhuanlan.zhihu.com/p/24986805514 -->\n",
    "\n",
    "##### 训练范式\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/qwen2_5_vl_training.svg\"  width=\"1000\"/>\n",
    "</p>\n",
    "- 初始化（0）\n",
    "  - LLM 用 Qwen2.5 预训练权重启动，作为语言主干。\n",
    "  - 视觉端为重构的 ViT：原生分辨率输入、窗口注意力，少数层保留全局注意力；2D‑RoPE/3D patch（视频）以支持空间/时间位置。\n",
    "  - 视觉特征经“邻域分组 + MLP 合并器”压缩至与文本相同维度，便于接入 LLM。\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/8.png\"  width=\"600\"/>\n",
    "</p>\n",
    "- 预训练（1）（与Qwen2-VL，预训练数据量 1.2T -> 4T）\n",
    "  - 阶段 I：视觉预训练（仅训练 ViT）\n",
    "    - 数据：图像字幕、视觉知识、OCR。\n",
    "    - 目标：让视觉特征更好地对齐语言空间。\n",
    "  - 阶段 II：多模态预训练（全参数）\n",
    "    - 数据：图文交错、VQA、视频、数学、Agent、纯文本。\n",
    "    - 工程：按送入 LLM 的长度统一打包到 8,192 tokens；保持原生分辨率与窗口注意力，平衡算力。\n",
    "  - 阶段 III：长上下文预训练\n",
    "    - 数据：长视频、长文档、长序列代理任务。\n",
    "    - 配置：序列长度 32,768；动态 FPS；MRoPE 的时间维与绝对时间对齐，增强长程推理稳定性。\n",
    "\n",
    "- 数据管线（支撑面）\n",
    "  - 图文交错：先常规清洗，再由内部评估模型从文本质量、图文相关性、互补性、信息密度平衡四维打分筛选。\n",
    "  - Grounding：训练使用“原图绝对坐标”（框与点），同时扩展开放类目与多实例场景，提升定位泛化。\n",
    "  - 文档解析：将文本、表格、图表、公式等转换为含 bbox 的结构化 HTML，统一表示并保留版面信息。\n",
    "  - OCR/表格/图表/视频：多语种与高质量合成结合；视频动态采样 FPS，覆盖不同时间节奏。\n",
    "\n",
    "- 后训练对齐（2）\n",
    "  - SFT（冻结 ViT）\n",
    "    - 使用 ChatML 格式的多轮多模态指令数据（约 200 万条，文本/多模态各占 50%）。\n",
    "    - 两阶段过滤：领域分类 → 规则与模型打分；拒绝采样强化链式思考，并校验视觉信息是否被正确使用。\n",
    "  - DPO（冻结 ViT）\n",
    "    - 基于偏好数据（图文与纯文本），一次遍历对齐风格、帮助性与安全性，使生成更贴合人类偏好。\n",
    "\n",
    "- 推理流程（3）\n",
    "  - 图像/视频进入 ViT，按原生分辨率分块；邻域特征聚合并经 MLP 压缩为 2048 维视觉 token。\n",
    "  - 与文本拼接后送入 LLM，自回归生成答案、描述、结构化 HTML 或定位框等。\n",
    "\n",
    "- 讲解提示\n",
    "  - 强调“原生分辨率 + 窗口注意力 + MLP 合并”是效率关键。\n",
    "  - MRoPE 的“绝对时间对齐”使不同帧率的视频理解更稳健。\n",
    "  - “三阶段预训 + SFT + DPO”分别解决“看懂”“会用”“更合意”的三层目标。\n",
    "\n",
    "#### 2025.01，DeepSeek发布DeepSeek-R1\n",
    "<!-- https://blog.csdn.net/bylander/article/details/145524526 -->\n",
    "<!-- https://magazine.sebastianraschka.com/p/understanding-reasoning-llms?continueFlag=af07b1a0954d90469bc6f6584075da3b -->\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/6.png\"  width=\"800\"/>\n",
    "</p>\n",
    "要概述一下《DeepSeek R1 技术报告》中所描述的 DeepSeek R1 流程。这份报告既是一个有趣的案例研究，也是开发推理大型语言模型的蓝图。\n",
    "需要注意的是，DeepSeek 并没有发布单一的 R1 推理模型，而是推出了三种不同的变体：DeepSeek-R1-Zero、DeepSeek-R1 和 DeepSeek-R1-Distill。\n",
    "\n",
    "根据该技术报告中的描述，我在下面的图表中总结了这些模型的开发过程。\n",
    "DeepSeek R1 技术报告中所讨论的 DeepSeek 三种不同推理模型的开发过程。\n",
    "接下来，让我们简要回顾一下上面图表中展示的过程。更多细节将在下一部分中介绍，在那里我们会讨论构建和改进推理模型的四种主要方法。\n",
    "\n",
    "**（1）DeepSeek-R1-Zero：** 该模型基于 2024 年 12 月发布的 6710 亿参数预训练模型 DeepSeek-V3 基础模型。研究团队使用带有两种奖励的强化学习（RL）对其进行训练。这种方法被称为 “冷启动” 训练，因为它没有包含监督微调（SFT）步骤，而这一步骤通常是基于人类反馈的强化学习（RLHF）的一部分。\n",
    "\n",
    "**（2）DeepSeek-R1：** 这是 DeepSeek 的旗舰推理模型，以 DeepSeek-R1-Zero 为基础构建而成。该团队通过额外的监督微调阶段和进一步的强化学习训练对其进行了优化，在 “冷启动” 的 R1-Zero 模型基础上进行了改进。\n",
    "\n",
    "**（3）DeepSeek-R1-Distill：** 利用之前步骤中生成的监督微调数据，DeepSeek 团队对 Qwen 和 Llama 模型进行了微调，以增强它们的推理能力。虽然这并非传统意义上的蒸馏，但该过程包括使用更大的 DeepSeek-R1 6710 亿参数模型的输出对更小的模型（Llama 80 亿和 700 亿参数模型，以及 Qwen 15 亿–300 亿参数模型）进行训练。\n",
    "\n",
    "<!-- <p align = \"center\">    \n",
    "<img src=\"./image/9.png\"  width=\"1800\"/>\n",
    "</p> -->\n",
    "\n",
    "![](./image/9.png)\n",
    "<!-- https://chattools.cn/article/1502 -->\n",
    "\n",
    "\n",
    "#### 2025.04，阿里发布Qwen3\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/3.jpg\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "##### 预训练\n",
    "在预训练方面，Qwen3 的**数据集相比 Qwen2.5 有了显著扩展**。Qwen2.5是在 18T 个 token 上进行预训练的，而 Qwen3 使用的数据量几乎是其两倍，达到了约 36 T个 token，涵盖了 119 种语言和方言。\n",
    "为了构建这个庞大的数据集，我们不仅从网络上收集数据，还从 PDF 文档中提取信息。我们使用 Qwen2.5-VL 从这些文档中提取文本，并用 Qwen2.5 改进提取内容的质量。\n",
    "为了增加数学和代码数据的数量，我们利用 Qwen2.5-Math 和 Qwen2.5-Coder 这两个数学和代码领域的专家模型合成数据，合成了包括教科书、问答对以及代码片段等多种形式的数据。\n",
    "\n",
    "预训练过程分为三个阶段。\n",
    "1. 在第一阶段（S1），模型在超过 30 T个 token 上进行了预训练，上下文长度为 4K token。这一阶段为模型提供了基本的语言技能和通用知识。\n",
    "2. 在第二阶段（S2），我们通过增加知识密集型数据（如 STEM、编程和推理任务）的比例来改进数据集，随后模型又在额外的 5 万亿个 token 上进行了预训练。\n",
    "3. 在最后阶段，我们使用高质量的长上下文数据将上下文长度扩展到 32K token，确保模型能够有效地处理更长的输入。\n",
    "\n",
    "由于模型架构的改进、训练数据的增加以及更有效的训练方法，Qwen3 Dense 基础模型的整体性能与参数更多的Qwen2.5基础模型相当。\n",
    "- 例如，Qwen3-1.7B/4B/8B/14B/32B-Base 分别与 Qwen2.5-3B/7B/14B/32B/72B-Base 表现相当。\n",
    "- 特别是在 STEM、编码和推理等领域，Qwen3 Dense 基础模型的表现甚至超过了更大规模的 Qwen2.5 模型。\n",
    "- 对于 Qwen3 MoE 基础模型，它们在仅使用 10% 激活参数的情况下达到了与 Qwen2.5 Dense 基础模型相似的性能。这带来了训练和推理成本的显著节省。\n",
    "\n",
    "##### 后训练\n",
    "为了开发能够同时具备思考推理和快速响应能力的混合模型，我们实施了一个四阶段的训练流程。该流程包括：（1）长思维链冷启动，（2）长思维链强化学习，（3）思维模式融合，以及（4）通用强化学习。\n",
    "\n",
    "1. 在第一阶段，我们使用多样的的长思维链数据对模型进行了微调，涵盖了数学、代码、逻辑推理和 STEM 问题等多种任务和领域。这一过程旨在为模型配备基本的推理能力。\n",
    "2. 第二阶段的重点是大规模强化学习，利用基于规则的奖励来增强模型的探索和钻研能力。\n",
    "3. 在第三阶段，我们在一份包括长思维链数据和常用的指令微调数据的组合数据上对模型进行微调，将非思考模式整合到思考模型中。确保了推理和快速响应能力的无缝结合。\n",
    "4. 最后，在第四阶段，我们在包括指令遵循、格式遵循和 Agent 能力等在内的 20 多个通用领域的任务上应用了强化学习，以进一步增强模型的通用能力并纠正不良行为。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c41cd",
   "metadata": {},
   "source": [
    "## A1 基本原理：微调的核心价值与基本思路\n",
    "从迁移学习与分布偏移出发，说明监督微调（SFT）与对齐的关系，明确全参数微调（FPFT）与高效参数微调（PEFT）的本质差异与边界条件；通过小规模示例说明目标函数、学习率与批大小等关键超参对收敛与泛化的作用，并给出观察与记录要点。\n",
    "\n",
    "### 从迁移学习与分布偏移谈起\n",
    "- **迁移学习的本质**：先在大规模通用数据上预训练，再把已有能力“迁移”到目标任务/领域上。\n",
    "- **分布偏移的三类**：\n",
    "  - **输入分布偏移（covariate shift）**：用户提问的风格、格式、领域变了。\n",
    "  - **标签分布偏移（label shift）**：答案的总体分布变了（比如更偏短、更谨慎）。\n",
    "  - **概念偏移（concept shift）**：对同一输入，社会/规范上“什么是好答案”的判定变了。\n",
    "- 对应地，迁移阶段既要让模型适应新输入风格，也要让输出符合“新的好答案标准”。\n",
    "\n",
    "### SFT 与“对齐”的关系\n",
    "- **SFT（监督微调）是什么**：用高质量、人类示范的输入-输出对做“模仿学习”。它主要解决“看起来像”的问题——让模型学会在目标指令分布下，产出像样的答案。\n",
    "- **对齐（alignment）是什么**：让模型“做我们真正希望它做的事”，不仅要“像人类”，更要“符合人类偏好与安全规范”。常用方法含偏好优化（RLHF、DPO、KTO）、宪法式原则（Constitutional AI）、拒答/安全调优等。\n",
    "- **两者关系**：\n",
    "  - SFT常作为对齐的“地基”：先把模型带到“能按指令说清楚”的位置。\n",
    "  - 对齐在此基础上，解决“更合意、更安全”的目标，即更侧重偏好与规范层面的“概念偏移”校正。\n",
    "  - 只做SFT常提升“在分布内”的表现，但对偏好权衡、价值冲突和长尾安全不足；因此“对齐 ≠ 仅SFT”。\n",
    "\n",
    "### FPFT 与 PEFT 的本质差异\n",
    "- **FPFT（全参数微调）**：\n",
    "  - 更新全部参数，容量最大，可深度重塑表示与推理过程。\n",
    "  - 代价高（显存/算力/时间/存储），易遗忘通用能力，需要更稳健的训练与评估。\n",
    "  - 产出的是“一个新模型”，不易快速切换多领域变体。\n",
    "- **PEFT（高效参数微调，例如 LoRA/Adapter/Prefix/BitFit/QLoRA）**：\n",
    "  - 冻结骨干，只训练小量增量参数（常为低秩或小模块）。\n",
    "  - 训练/部署成本低、易维护，多域多版本可即插即用，保留基座的通用能力。\n",
    "  - 表达能力受限：巨大或结构性分布偏移时可能“力有未逮”，需要更高秩、更广覆盖，或最终转向FPFT。\n",
    "\n",
    "### 选择的边界条件与经验法则\n",
    "- **优先考虑 PEFT 的情形**：\n",
    "  - 资源有限、需要快速迭代或维护多套领域/品牌风格（可并行挂多适配器）。\n",
    "  - 主要是“输入风格/领域/格式”的偏移，或中等强度的偏好与安全校正（典型指令跟随、拒答策略、语气规范）。\n",
    "  - 数据量中小（如百万至数亿级标注/指令tokens），希望尽量保留基座能力、降低遗忘。\n",
    "  - 产业常态：SFT与偏好优化（RLHF/DPO）多数可用PEFT/QLoRA达到很强效果（即使在大模型上）。\n",
    "- **需要 FPFT 的情形**：\n",
    "  - **极大或结构性分布偏移**：需要系统性重塑内部表示与推理（如跨脚本低资源语言的深度能力、从零注入复杂算法/专业知识体系）。\n",
    "  - **单一高性能定制**：为某一任务/领域榨干上限，且算力/数据充足（十亿级以上tokens）；\n",
    "  - **架构/词表层改动**或跨模态深改（如新增模态投影层、系统性调整归一化/路由机制）。\n",
    "  - **广谱安全/价值修复**：需要内化到表示层的大规模纠偏，而非仅输出层面的策略修正。\n",
    "- **折中与进阶**：\n",
    "  - 扩大PEFT覆盖面与秩（更高rank、更广层数），或局部解冻（如LayerNorm、输出头）可显著拉近FPFT效果。\n",
    "  - 多阶段流程常见且有效：基座 → SFT（PEFT/QLoRA）→ 偏好优化（DPO/RLHF，仍可用PEFT）→ 必要时再做局部解冻或转FPFT。\n",
    "\n",
    "### 一句话把握\n",
    "- **SFT**让模型“像人类那样答”；**对齐**让模型“按人类真正想要的那样做”。  \n",
    "- **PEFT**高效、安全、易维护，适合多数指令与对齐工作；**FPFT**代价更高，但在巨大分布偏移或追求极限性能时更有把握。\n",
    "\n",
    "- 小结\n",
    "  - SFT解决“在新分布下学会表达”，对齐解决“按人类价值进行取舍与守则”。\n",
    "  - PEFT是工业界主力路径；FPFT用于超大改造、极致定制或架构层需求。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8674a9",
   "metadata": {},
   "source": [
    "## A2 微调范式对比：FPFT、LoRA、AdaLoRA、QLoRA\n",
    "建立“任务规模×算力×时限×维护成本”的选型框架：FPFT具备充分表达但资源开销较大；LoRA以低秩分解提升参数效率；AdaLoRA在秩自适应下改善表达与稳定性；QLoRA以量化降低显存占用但需关注量化误差。基于统一数据与评测口径，形成各范式在显存、吞吐与效果上的可比结论，用于实际决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55e6d5",
   "metadata": {},
   "source": [
    "### 1 模型训练时的内存消耗\n",
    "\n",
    "### 2 DeepSeed-ZeRO\n",
    "\n",
    "### 3 各微调范式介绍\n",
    "\n",
    "### 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b3f48",
   "metadata": {},
   "source": [
    "## A3标准微调流程：（数据→训练→评估→归档→上线）\n",
    "规范数据治理（清洗、去重、格式统一、审计留痕）、训练配置与日志记录、评估集构造与复现实验的要点；明确版本化管理、随机种子与环境固定等可复现要求，建立从实验到上线的最小闭环与风险控制清单。\n",
    "\n",
    "### 1 微调范式对比\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728d256",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "# conda环境\n",
    "conda create -p ./env python=3.10\n",
    "conda activate ./env\n",
    "\n",
    "# ms-swift训练环境\n",
    "git clone https://github.com/modelscope/ms-swift.git\n",
    "cd ms-swift\n",
    "pip install -e .\n",
    "cd ..\n",
    "\n",
    "# 下载模型文件\n",
    "apt-gpt update\n",
    "apt-get install git-lfs\n",
    "git lfs install\n",
    "mkdir models && cd models\n",
    "\n",
    "GIT_LFS_SKIP_SMUDGE=1 git clone https://hf-mirror.com/Qwen/Qwen2.5-VL-3B-Instruct\n",
    "cd Qwen2.5-VL-3B-Instruct/\n",
    "git lfs pull\n",
    "cd ..\n",
    "\n",
    "GIT_LFS_SKIP_SMUDGE=1 git clone https://hf-mirror.com/Qwen/Qwen2.5-VL-7B-Instruct\n",
    "cd Qwen2.5-VL-7B-Instruct/\n",
    "git lfs pull\n",
    "cd ..\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d58db1",
   "metadata": {},
   "source": [
    "### 2 标准微调流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7c54c",
   "metadata": {},
   "source": [
    "## A4框架选型与环境准备：unsloth / ms-swift / llama-factory\n",
    "讲解两框架在 SFT/PEFT/对齐中的接口与流水线差异，结合显存与吞吐的经验阈值给出适配建议；完成环境验证与基线运行，记录资源占用、收敛速度与稳定性，为下午的案例训练提供可靠起点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb5d3d",
   "metadata": {},
   "source": [
    "## A5 LoRA / QLoRA 案例（指令遵循小模型）\n",
    "在单卡（≤24GB）条件下完成一次基于 LoRA 或 QLoRA 的指令微调演示；比较不同秩、α、dropout、量化位宽、学习率、批大小对收敛与输出质量的影响；开展推理验证并记录延迟与效果的关键观察点，沉淀参数选择的经验规则。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600255d",
   "metadata": {},
   "source": [
    "## A6 Prompt 工程与数据合成（小样本强化）\n",
    "以任务结构化为中心设计可迁移的提示模板，采用变量化与多样化策略扩展样本；建立合成数据的质检规则与偏差控制方法，在小样本条件下验证对特定子任务的增益，并总结可复用的模板与筛选标准。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee600d1",
   "metadata": {},
   "source": [
    "## A7 对齐算法概览与入门实践：PPO / DPO / GRPO\n",
    "解析三者在信号来源、稳定性与工程代价上的差异：PPO偏在线、信号获取成本高；DPO基于偏好对，路径简洁、易复现；GRPO通过分组奖励提升可控性。完成一个入门级偏好对齐示例，关注偏好数据构造、质量控制与伦理边界。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3af9a4",
   "metadata": {},
   "source": [
    "## A8 评估与上线：指标体系、灰度与回滚\n",
    "构建任务正确率、遵循度、毒性与稳定性等指标体系，确定A/B与灰度发布流程及回滚触发条件；形成上线监控、异常处置与复盘机制的要点说明，确保从实验到生产的可追溯与可管控。\n",
    "\n",
    "### 一、指标体系（评什么、怎么评）\n",
    "- **任务正确率（Task Accuracy）**：回答是否正确、完整、可复现。\n",
    "  - 常用口径：精确匹配（Exact Match）、F1、基于证据的正确率（回答必须引用到的文档片段）、专家评分（Likert 1–5）。\n",
    "  - 面向开放回答：用“评测模型”打分（多维度：正确性、覆盖度、逻辑性），配合人审抽检校准偏差。\n",
    "- **指令遵循度（Instruction Following）**：回答是否严格满足约束与格式。\n",
    "  - 口径：格式遵循率（JSON/表格/字段齐全）、约束满足率（字数、语气、边界）、拒答合理率（该拒绝时能拒绝且给出理由）。\n",
    "- **毒性与合规（Safety & Compliance）**：是否出现有害、有毒或违规内容。\n",
    "  - 口径：毒性率（Toxicity Rate）、越权输出率（隐私、医疗/法律不当建议）、越狱率（被提示工程绕过防护）。\n",
    "  - 分层防护：前/后置安全审查模型 + 关键词/正则/规则库。\n",
    "- **稳定性与性能（Reliability & Performance）**：\n",
    "  - 时延：TTFT（首 token 延迟）、p50/p95/p99 响应时间；吞吐（RPS/QPS）。\n",
    "  - 稳定：超时率、错误率（5xx/4xx）、重试率、输出格式失败率。\n",
    "  - 变异：输出波动（多次生成一致性）、冷启动抖动。\n",
    "- **成本与运维（Cost & Ops）**：\n",
    "  - 每请求成本、每千 token 成本、缓存命中率、检索调用次数、第三方 API 费用。\n",
    "- **用户与业务（UX & Business）**：\n",
    "  - 用户满意度（CSAT/NPS）、人工复核占比与纠错成本、留存与转化。\n",
    "- 指标分层建议：\n",
    "  - **离线基准**（固定测试集，保证可复现）\n",
    "  - **在线护栏**（实时监控：毒性、错误、时延、成本）\n",
    "  - **人审抽检**（高优先级场景按日/周抽检，校准评测模型偏差）\n",
    "\n",
    "术语小贴士：\n",
    "- p95/p99：95%/99% 请求的时延不超过该值（衡量“尾部时延”）。\n",
    "- TTFT：Time To First Token，用户感知“开口速度”。\n",
    "- 评测模型：用强模型对回答多维打分，成本低于全量人审。\n",
    "\n",
    "### 二、A/B 测试与灰度发布（如何把风险降到最低）\n",
    "- **A/B 测试（对照实验）**：将流量随机分为两组比较新旧方案差异，确保除了“模型版本”外其他条件一致。\n",
    "  - 设计要点：\n",
    "    - 明确主指标（如任务正确率↑）与护栏指标（毒性率、时延、成本不得恶化）。\n",
    "    - 样本量计算（保证统计功效），随机化单元（用户/会话），固定分桶（避免跨天穿桶）。\n",
    "    - 实验时长覆盖业务周期（避免节假日/活动干扰）。\n",
    "  - 分析要点：显著性（p 值/置信区间）、效应量（Cohen’s d）、提前停止规则（护栏触发即停）。\n",
    "- **灰度发布（又称金丝雀发布/Canary）**：小流量逐步放大，动态健康检查。\n",
    "  - 典型节奏：影子测试（0%用户可见）→ 1% → 5% → 20% → 50% → 100%。\n",
    "  - 影子测试（Shadow）：“新模型并行生成但不回显”，离线对比质量/时延/成本，先发现问题再放量。\n",
    "  - 自动化判定：每个阶段设定放量阈值（主指标提升≥X%，护栏不恶化），否则停留或回滚。\n",
    "\n",
    "术语小贴士：\n",
    "- 护栏指标（Guardrails）：出现恶化即“叫停/回滚”的红线指标（如毒性、合规、成本上限）。\n",
    "- 固定分桶：同一用户在实验期内始终落在同一版本，消除个体差异干扰。\n",
    "\n",
    "### 三、回滚触发条件（何时立即撤回）\n",
    "- **安全/合规类**：毒性/越权/越狱率超过阈值；监管投诉或高风险样例出现。\n",
    "- **质量类**：主任务正确率显著下降；关键格式失败率上升（导致下游不可解析）。\n",
    "- **性能类**：p95/p99 时延、超时率、错误率突增；TTFT恶化明显。\n",
    "- **成本类**：单位成本越线；外部 API 费率异常上涨。\n",
    "- **运营类**：用户投诉激增、业务 KPI 下滑（转化、留存）。\n",
    "- 处置动作（自动化优先）：立即将流量切回稳定版本；切换旧提示/旧路由；降低并发、启用缓存/降级；冻结新增发布。\n",
    "\n",
    "### 四、上线监控与告警（看什么、怎么报）\n",
    "- **实时观测维度**：\n",
    "  - 请求级：QPS、TTFT、p95、错误/超时、重试、缓存命中。\n",
    "  - 质量近似：格式失败率、语义一致性/自洽率（多次生成一致）、引用一致性（RAG 回答是否引用来源）。\n",
    "  - 安全：毒性事件、拒答合理率、可疑越狱提示命中。\n",
    "  - 成本：每次请求成本、token 用量、外部 API 次数与单价。\n",
    "- **告警与阈值**：\n",
    "  - 严重级别分级（P0/P1/P2），分级通知（电话/IM/邮件）。\n",
    "  - 动态基线 + 异常检测（相对历史均值波动）。\n",
    "- **日志与隐私**：\n",
    "  - 结构化日志字段：请求ID、用户/会话ID（匿名化）、提示版本、模型版本、输入/输出摘要、引用来源、时延/成本、决策路由。\n",
    "  - 脱敏与采样：敏感信息屏蔽，生产只保留必要字段与可追溯链路。\n",
    "\n",
    "术语小贴士：\n",
    "- 语义一致性/自洽：同一问题多次生成是否相似（衡量稳定性）。\n",
    "- 引用一致性：回答声称的事实能否在知识库检索到对应证据。\n",
    "\n",
    "### 五、异常处置机制（Runbook）\n",
    "- **分级与责任**：设定值班与升级链路（谁在 15 分钟内响应、30 分钟内决策）。\n",
    "- **标准流程**：\n",
    "  1) 确认告警 → 2) 准实时止血（回滚/降级/限流/熔断） → 3) 影响评估与对外沟通 → 4) 根因分析与修复 → 5) 恢复与观察。\n",
    "- **常用技术手段**：\n",
    "  - Kill switch（全局开关）、特性开关（Feature Flag）、流量路由（老新模型并存）。\n",
    "  - 降级：关闭复杂工具/检索、降低采样/思考步数、启用缓存。\n",
    "  - 防扩散：限流、隔离问题租户/场景；动态屏蔽高风险提示模板。\n",
    "- **演练**：季度故障演习，验证回滚时延、告警有效性与人员协同。\n",
    "\n",
    "### 六、复盘与持续改进（Postmortem）\n",
    "- **无责复盘**：记录时间线、影响面、根因（5 Whys）、侥幸因素。\n",
    "- **行动项**：清晰责任人/截止日期/验收标准；新增回归测试与监控规则。\n",
    "- **知识沉淀**：更新“评测集/风险样例库/提示模板”，把故障转化为可重复监测与测试的资产。\n",
    "\n",
    "### 七、落地清单与基线 SLO 示例\n",
    "- **发布前检查清单**：\n",
    "  - 离线基准：正确率↑且毒性/成本/时延不劣于基线；关键场景通过人审。\n",
    "  - 影子测试：≥1 周业务周期，质量/时延/成本无异常。\n",
    "  - A/B 方案：样本量、随机化、指标/护栏、实验时长、停止规则均已配置。\n",
    "  - 回滚预案：一键回滚、演练通过；告警阈值与分级触发已验证。\n",
    "- **SLO（服务目标）示例**：\n",
    "  - 可用性≥99.9%；TTFT p95 ≤ 600ms，响应 p95 ≤ 2.5s。\n",
    "  - 任务正确率 ≥ 基线 +2%；格式失败率 ≤ 0.5%。\n",
    "  - 毒性率 ≤ 0.05%；越狱率 ≤ 0.1%（持续下降趋势）。\n",
    "  - 单次请求成本 ≤ 目标上限（按日/周平均监控）。\n",
    "- **自动化护栏（样例）**：\n",
    "  - 若毒性率 10 分钟滑窗 > 阈值 → 立即回滚并通知 P0。\n",
    "  - 若 p95 时延 > 阈值且错误率上升 → 自动降级与限流。\n",
    "  - 若成本/千 token 超上限 → 切换至备选路由或启用缓存策略。\n",
    "\n",
    "术语小贴士：\n",
    "- SLI/SLI/SLA：SLI 指标、SLO 目标、SLA 对外承诺（合同级）。生产优先达成 SLO，再根据业务制定 SLA。\n",
    "\n",
    "——\n",
    "\n",
    "- 关键要点\n",
    "  - 指标成体系：正确率、遵循度、安全、稳定性、成本、业务六维联动。\n",
    "  - 上线走两步：先影子后灰度；A/B 有护栏与停止规则。\n",
    "  - 监控有抓手：实时仪表盘 + 自动化告警 + 一键回滚。\n",
    "  - 闭环要完备：处置—复盘—固化到评测集与监控策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843b4dd",
   "metadata": {},
   "source": [
    "## A9 扩展课程-大模型幻觉缓解：语义熵与多模型合作\n",
    "采用同义改写得到条件分布，计算语义熵并设定阈值以触发“自动怀疑”；结合多模型一致性裁决降低错误风险；通过对照样例展示前后指标变化，并分析误报与漏报的权衡，给出部署时的建议阈值区间。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
