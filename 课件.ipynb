{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa385611",
   "metadata": {},
   "source": [
    "# 企业内训策划案——大模型微调一日实训\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6ee6d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## A0 大模型技术演进及工程科研应用\n",
    "本次课程将简要阐述人工智能技术演进里程碑事件，系统梳理大模型技术从学术探索到产业落地的关键进程，结合国际前沿最新研究成果，深入剖析AI4S以及工程场景中的创新应用。通过典型案例揭示大模型技术如何驱动科研范式变革与工程效率提升，探讨技术瓶颈与未来趋势，为企业智能化转型提供战略参考与实践路径。\n",
    "### 1 大模型技术演进（以openai为例）\n",
    "<!-- https://openai.com/zh-Hans-CN/research/index/publication/?page=10 -->\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/1.jpg\"  width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "2017年，Google引入Transformer模型。\n",
    "\n",
    "2018.06，发布GPT-1。 （GPT: Generative Pre-Training）\n",
    "- GPT1 为 GPT 系列模型**建立了核心架构**\n",
    "    - 仅解码器的 Transformer 架构\n",
    "- 建立了**对自然语言文本进行建模的底层原理**，即预测下一个单词\n",
    "    - GPT: 生成式预训练\n",
    "\n",
    "2019.02，发布GPT-2。\n",
    "认为每个 （NLP） 任务都可以被视为基于世界文本子集的单词预测问题。因此，如果无监督语言建模经过训练，具有足够的能力来恢复世界文本，则能够解决各种任务。（效果仍然较差）\n",
    "- **用一个大型网页数据集WebText进行训练**\n",
    "- **将参数规模从0.12B提高到1.5B**\n",
    "\n",
    "2020.05，发布GPT-3。\n",
    "经验证明，将神经网络扩展到相当大的规模可以导致模型容量的巨大增加。\n",
    "- 探索了scaling law，**大幅增加参数量，从1.5B提升到175B**\n",
    "- 正式引入**In-context Learning**（GPT-2已采用） ，LLM 的预训练和利用收敛到相同的语言建模范式\n",
    "\n",
    "2022.03，发布GPT-3.5。\n",
    "- 在代码数据训练，提升代码能力和思维链能力\n",
    "- 对GPT-3采用与 InstructGPT 类似的三阶段强人类反馈化学习（RLHF）算法\n",
    "    - 收集示范数据SFT，然后迭代进行：（1）用策略模型收集比较数据（比生成式易获取），训练奖励模型RM，从而可用于打分（2）使用2017年提出的PPO（近段策略优化）对策略模型强化学习\n",
    "    - SFT提高指令跟随能力，强化学习缓解有害有毒回答\n",
    "- 上下文长度为4k\n",
    "\n",
    "InstructGPT的三阶段强人类反馈化学习（RLHF）算法：《Training language models to follow instructions with human feedback》\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/2.png\"  width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "2023.03，发布GPT-4（ChatGPT = GPT-3.5/GPT-4 + 交互式网页）。\n",
    "- 进一步探索scaling law，**从175B提升到1.76T**\n",
    "    - 在解决复杂任务方面的能力比 GPT-3.5 更强\n",
    "    - 构建了scaling law的预测方式，从小模型预测大模型的loss\n",
    "- 将文本输入扩展到图像/文本信号（详情可见后续2023.09发布的GPT-4V）\n",
    "- 上下文长度拓展至8k（GPT-4-32k支持32k）\n",
    "    - 2023.11发布的GPT-4 Turbo拓展到128k\n",
    "- 强化学习时引入额外的安全奖励信号，进一步减少有害输出\n",
    "\n",
    "2024.05，发布GPT-4o(omni) 。\n",
    "GPT‑4o 是一种**自回归全模态模型**（参数规模200B），它能够接受文本、音频、图像和视频的任意组合作为输入，并生成文本、音频和图像输出的任意组合。\n",
    "- 自回归全模态模型，音频输入响应接近人类响应时延（平均320ms）\n",
    "- 关键数据集（公开数据+数据合作伙伴）：\n",
    "    - 公开网页数据，视角多样\n",
    "    - 代码与数学数据，有助于结构化逻辑和推理，利于问题解决\n",
    "    - 多模态数据（图像、音频和视频），训练 LLM 如何解读输入的非文本内容并生成非文本输出\n",
    "\n",
    "2024.12，发布OpenAI o1模型（2024.09发布o1-preview）。\n",
    "发现Test-time Scaling，**随着强化学习（训练时间计算）和思考时间（测试时间计算）的增加，o1 的性能也在不断提高**。（参数规模300B）\n",
    "- 通过强化微调（RFT），o1 可以学会磨练自己的思维链，并完善自己使用的策略。\n",
    "\n",
    "2025.02，发布GPT-4.5\n",
    "扩大**无监督学习**的规模，得到更强的可控性、用户意图执行能力和**更高的“情商”**。\n",
    "- 为 GPT‑4.5 开发了可扩展的新技术，能够利用从较小模型提取的数据来训练规模更大、功能更强的模型\n",
    "\n",
    "2025.04，发布GPT-4.1\n",
    "上下文长度达到**1 million**，相比4o提升了编码能力、指令遵循能力、多模态长上下文能力。\n",
    "\n",
    "2025.04，发布OpenAI o3。\n",
    "观察到**大规模强化学习**展现出了与 GPT 系列预训练相同的“**计算量增加 = 性能提升**”的趋势。\n",
    "- 更大规模强化微调（提高一个数量级），并通过强化学习训练工具调用，提升编码、数学、科学\n",
    "- 首次能够基于图像进行思考，在视觉感知任务中表现出了最佳性能\n",
    "\n",
    "2025.08，发布GPT-5。\n",
    "参数规模从GPT-4的1.8T提升到**52T**，并且建立了**统一系统**，包含GPT-5-main（前身GPT‑4o）、GPT‑5 Thinking（前身OpenAI o3）和实时路由器。\n",
    "- 统一系统：路由+推理/非推理\n",
    "- 在**减少幻觉、提升指令遵循能力以及减少阿谀奉承**方面取得了显著进展\n",
    "- 常用场景提升性能（写作、编程和医疗）\n",
    "\n",
    "### 2 大模型的科研工程应用\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/4.png\"  width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "《A Survey of Large Language Models》\n",
    "\n",
    "- **一句话把握**: 大模型像“通才顾问”，小模型像“专科医生”。通才适配多任务、交互自然；专科在特定任务上小而精、省成本。实际落地常是“两者协作+工具配套”。\n",
    "\n",
    "#### 2.1 面向研究共同体\n",
    "\n",
    "##### 2.1.1 经典NLP任务（语言理解与生成的“基本功”）\n",
    "- **词/句级任务（相似度、情感）**\n",
    "  - **怎么做**: 识别“这两句话像不像”“评论是好评还是差评”。\n",
    "  - **实际表现**: 小模型用标注数据“专门练”通常更省钱且强；大模型用少量示例快速上手，优势在“通用与省配置”。\n",
    "  - **适用建议**: 有数据、有固定任务→小模型；任务多变、样本很少→大模型。\n",
    "- **序列标注（NER、POS）**\n",
    "  - **怎么做**: 给每个词贴标签，如“人名/地名/机构名”。\n",
    "  - **难点**: 少见类别、名字模糊时，大模型容易理解偏差。\n",
    "  - **改进思路**: 提示里把类别含义讲清楚、给足示例，或让小模型做最后一层“严格判定”。\n",
    "- **信息抽取（关系/事件）**\n",
    "  - **怎么做**: 从句子中抽“谁与谁是什么关系”“发生了什么事”。\n",
    "  - **难点**: 一句话多层关系、跨句信息，纯靠大模型少样本容易漏掉或混淆。\n",
    "  - **常用做法**: 两步走（先粗抽再精炼），或“大模型提建议+小模型做精确判断”。\n",
    "- **文本生成（翻译、摘要）**\n",
    "  - **优势**: 大模型按提示生成流畅文本，能处理“文档级翻译”“带交互的摘要”。\n",
    "  - **短板**: 低资源语言/小众领域，因训练数据少，质量不稳。\n",
    "  - **应对**: 准备术语表/风格示例，必要时加入检索或后校对。\n",
    "\n",
    "- **选型总则**\n",
    "  - **看三件事**: 数据量（是否足够专门训练）、任务多变度（是否常变换目标）、成本要求（训练/推理花费）。\n",
    "  - **稳妥组合**: “大模型做理解与生成框架+小模型做精细判定与效率收口”。\n",
    "\n",
    "##### 2.1.2 信息检索（搜索升级）\n",
    "- **大模型当“重排员”**: 给它一小批候选文档，按相关度排序。\n",
    "  - **优点**: 不用训练，写好指令就能用（逐个打分、两两比较或一组排序都可）。\n",
    "  - **代价**: 算力开销不小，长文档列表容易吃力。\n",
    "- **大模型增强传统检索**\n",
    "  - **数据增强**: 自动标注“相关/不相关”、为文档生成典型查询，训练出更懂用户意图的检索器。\n",
    "  - **查询改写**: 把“含糊问题”改成系统更好懂的表达，或补充背景知识。\n",
    "  - **文档扩展**: 用“可能被搜索到的问法”丰富文档侧，让匹配更准。\n",
    "- **落地建议**: 高频、长文本场景优先用“增强检索”；重点问题页再用“大模型重排”兜底，兼顾体验与成本。\n",
    "\n",
    "##### 2.1.3 推荐系统（“猜你想要”）\n",
    "- **直接用大模型做推荐**\n",
    "  - **方式**: 用提示或指令微调，让它读历史点击“推下一个”；给物品/用户配“语义ID”，让协同关系可读。\n",
    "  - **现实**: 零/少样本往往不如传统ID推荐稳定；指令微调更好但成本高。\n",
    "- **大模型帮传统推荐变强**\n",
    "  - **意图推断**: 总结用户兴趣变化，辅助召回更准。\n",
    "  - **特征编码**: 读商品文案、用户评价，产出更有信息的向量特征。\n",
    "  - **知识“蒸馏”**: 训练时对齐“小模型的隐藏表征”到“大模型风格”，上线只用小模型，既快又省。\n",
    "- **推荐模拟器（Agent）**\n",
    "  - **作用**: 模拟“不同画像的用户”与“不同风格的商品”，反复交互，评估策略在“像真实世界”的环境里是否靠谱。\n",
    "- **实操要点**: 线上以小模型为主，前台延迟敏感；大模型用于离线增强与实验验证。\n",
    "\n",
    "##### 2.1.4 多模态大模型（看图又能说）\n",
    "- **它如何工作**: 先把图片“翻译”为大模型能读的向量，再与文字一起喂给模型，让它“边看边想边回答”。\n",
    "- **训练两步**\n",
    "  - **对齐预训练**: 让“图像表达”和“文字表达”合拍。数据少时只训“连接器”；数据优且细时可微调语言端；巨量数据下可微调视觉端。\n",
    "  - **视觉指令微调**: 用“带图任务说明+期望回答”教会它怎么按照指令完成复杂任务。\n",
    "- **如何评测**\n",
    "  - **看得准**: 识别物体/属性、读图中文字、避免“幻觉（看图说错）”。\n",
    "  - **想得明白**: 回答与图有关的推理题，如空间关系、常识结合图像。\n",
    "  - **评分方式**: 有标准答案的“闭卷打分”，也有由人或模型做“开放式评审”。\n",
    "- **关键点**: 指令数据要真且细；训练时平衡“保留原本语言能力”和“适配新任务”；对安全与事实性要有额外约束（如答案修订、RLHF）。\n",
    "\n",
    "##### 2.1.5 知识图谱增强（用结构化事实托底）\n",
    "- **为什么需要**: 大模型会“编”，而知识图谱存放“谁-关系-谁”的硬知识，能帮它讲真话、查事实。\n",
    "- **两种用法**\n",
    "  - **检索增强**: 先从图谱取一个小子图，转成文字喂给模型。难点是“别把结构信息丢了”，否则理解走样。\n",
    "  - **协同增强**: 把复杂问题拆解，多轮“查→推→再查”，用专用接口高效获取对应关系，逐步凑齐证据链。\n",
    "- **实操注意**: 统一不同知识源的接口、低成本更新事实、把事实当“对齐尺子”减幻觉。\n",
    "\n",
    "##### 2.1.6 用大模型来评测（打分与点评）\n",
    "- **两类结果**\n",
    "  - **分数/排序**: 快速比较好坏、可规模化。\n",
    "  - **文字点评**: 指出问题与改进方向，可反哺对齐训练。\n",
    "- **常见做法**: 多视角给提示（换顺序、换维度、要解释）、多模型讨论求共识、或训练“评测专用模型”。\n",
    "- **风险点**: 偏好长答案、喜欢自己风格等“固有偏见”；面对很强的模型与复杂任务，评测还不够敏锐。\n",
    "\n",
    "#### 2.2 面向具体领域（行业里的“用与管”）\n",
    "- **医疗**\n",
    "  - **能做**: 问诊建议、报告简化、术语解释、专业考试（如Med-PaLM）。\n",
    "  - **要管**: 严控事实错误与不当建议；保护隐私；重要结论需医生复核。\n",
    "- **教育**\n",
    "  - **能做**: 辅助解题、写作润色、个性化学习路径、作业/测验初评。\n",
    "  - **要管**: 防抄袭与依赖、内容偏见、非英语人群的公平获得。\n",
    "- **法律**\n",
    "  - **能做**: 文书检索与摘要、要点提取、写作草稿、法规解释与推理。\n",
    "  - **要管**: 版权合规、隐私保护、避免歧视性输出；关键判断由律师最终把关。\n",
    "- **金融**\n",
    "  - **能做**: 行情解读、情绪分析、事件抽取、风控线索生成；行业模型效果更稳。\n",
    "  - **要管**: 严格风控与合规审查，防止误导信息影响市场。\n",
    "- **科研**\n",
    "  - **能做**: 文献综述、灵感生成、数据探索与可视化、论文写作与初审。\n",
    "  - **要管**: 引用可追溯、数据与结论可复现，减少“似是而非”的表述。\n",
    "- **其他**: LLM for 心理/软件开发等\n",
    "\n",
    "#### 实操清单（通用建议）\n",
    "- **任务匹配**: 频繁、固定、可量化→小模型优先；多变、少样本、交互多→大模型优先。\n",
    "- **组合拳**: 大模型做“理解/生成/编排”，小模型做“判定/索引/提速”，配合检索与知识库兜底。\n",
    "- **数据为王**: 指令要清晰、示例要贴近真实；敏感领域加入术语表与硬性规则。\n",
    "- **成本与可靠性**: 高频路径用轻量方案，关键节点用“大模型精排/复核”；对外输出加校对与溯源。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b72b2",
   "metadata": {},
   "source": [
    "## A1 大模型架构与训练范式\n",
    "为了帮助后续理解大模型微调，此处基于国内顶尖开源模型Qwen和DeepSeek简要介绍大模型架构与训练范式。\n",
    "\n",
    "\n",
    "\n",
    "#### 2025.01，阿里巴巴发布Qwen2.5-VL\n",
    "##### 模型架构\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/5.png\"  width=\"1000\"/>\n",
    "</p>\n",
    "多模态模型主要由视觉编码器（Vision Encoder）、语言模型（LM）和多模态融合模块（Connector）三块构成，和Qwen2-VL一样，Qwen2.5-VL并没有巨大的Connector，仅用一个MLP完成特征投影。打印模型结构如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453c39b",
   "metadata": {},
   "source": [
    "\n",
    "```text\n",
    "Qwen2_5_VLForConditionalGeneration(\n",
    "  (model): Qwen2_5_VLModel(\n",
    "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
    "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
    "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
    "      )\n",
    "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
    "      (blocks): ModuleList(\n",
    "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
    "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
    "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
    "          (attn): Qwen2_5_VLVisionAttention(\n",
    "            (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
    "            (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          )\n",
    "          (mlp): Qwen2_5_VLMLP(\n",
    "            (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
    "            (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
    "            (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
    "            (act_fn): SiLU()\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "      (merger): Qwen2_5_VLPatchMerger(\n",
    "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
    "        (mlp): Sequential(\n",
    "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
    "          (1): GELU(approximate='none')\n",
    "          (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (language_model): Qwen2_5_VLTextModel(\n",
    "      (embed_tokens): Embedding(151936, 2048)\n",
    "      (layers): ModuleList(\n",
    "        (0-35): 36 x Qwen2_5_VLDecoderLayer(\n",
    "          (self_attn): Qwen2_5_VLAttention(\n",
    "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
    "            (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
    "            (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
    "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
    "          )\n",
    "          (mlp): Qwen2MLP(\n",
    "            (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
    "            (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
    "            (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
    "            (act_fn): SiLU()\n",
    "          )\n",
    "          (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
    "          (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
    "        )\n",
    "      )\n",
    "      (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
    "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
    "    )\n",
    "  )\n",
    "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e95b69",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Qwen2_5_VLMLP的SwiGLU结构：\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/7.png\"  width=\"500\"/>\n",
    "</p>\n",
    "<!-- https://zhuanlan.zhihu.com/p/24986805514 -->\n",
    "\n",
    "##### 训练范式\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/qwen2_5_vl_training.svg\"  width=\"1000\"/>\n",
    "</p>\n",
    "- 初始化（0）\n",
    "  - LLM 用 Qwen2.5 预训练权重启动，作为语言主干。\n",
    "  - 视觉端为重构的 ViT：原生分辨率输入、窗口注意力，少数层保留全局注意力；2D‑RoPE/3D patch（视频）以支持空间/时间位置。\n",
    "  - 视觉特征经“邻域分组 + MLP 合并器”压缩至与文本相同维度，便于接入 LLM。\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/8.png\"  width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "- 预训练（1）（与Qwen2-VL，预训练数据量 1.2T -> 4T）\n",
    "  - 阶段 I：视觉预训练（仅训练 ViT）\n",
    "    - 数据：图像字幕、视觉知识、OCR。\n",
    "    - 目标：让视觉特征更好地对齐语言空间。\n",
    "  - 阶段 II：多模态预训练（全参数）\n",
    "    - 数据：图文交错、VQA、视频、数学、Agent、纯文本。\n",
    "    - 工程：按送入 LLM 的长度统一打包到 8,192 tokens；保持原生分辨率与窗口注意力，平衡算力。\n",
    "  - 阶段 III：长上下文预训练\n",
    "    - 数据：长视频、长文档、长序列代理任务。\n",
    "    - 配置：序列长度 32,768；动态 FPS；MRoPE 的时间维与绝对时间对齐，增强长程推理稳定性。\n",
    "\n",
    "- 数据管线（支撑面）\n",
    "  - 图文交错：先常规清洗，再由内部评估模型从文本质量、图文相关性、互补性、信息密度平衡四维打分筛选。\n",
    "  - Grounding：训练使用“原图绝对坐标”（框与点），同时扩展开放类目与多实例场景，提升定位泛化。\n",
    "  - 文档解析：将文本、表格、图表、公式等转换为含 bbox 的结构化 HTML，统一表示并保留版面信息。\n",
    "  - OCR/表格/图表/视频：多语种与高质量合成结合；视频动态采样 FPS，覆盖不同时间节奏。\n",
    "\n",
    "- 后训练对齐（2）\n",
    "  - SFT（冻结 ViT）\n",
    "    - 使用 ChatML 格式的多轮多模态指令数据（约 200 万条，文本/多模态各占 50%）。\n",
    "    - 两阶段过滤：领域分类 → 规则与模型打分；拒绝采样强化链式思考，并校验视觉信息是否被正确使用。\n",
    "  - DPO（冻结 ViT）\n",
    "    - 基于偏好数据（图文与纯文本），一次遍历对齐风格、帮助性与安全性，使生成更贴合人类偏好。\n",
    "\n",
    "- 推理流程（3）\n",
    "  - 图像/视频进入 ViT，按原生分辨率分块；邻域特征聚合并经 MLP 压缩为 2048 维视觉 token。\n",
    "  - 与文本拼接后送入 LLM，自回归生成答案、描述、结构化 HTML 或定位框等。\n",
    "\n",
    "- 讲解提示\n",
    "  - 强调“原生分辨率 + 窗口注意力 + MLP 合并”是效率关键。\n",
    "  - MRoPE 的“绝对时间对齐”使不同帧率的视频理解更稳健。\n",
    "  - “三阶段预训 + SFT + DPO”分别解决“看懂”“会用”“更合意”的三层目标。\n",
    "\n",
    "#### 2025.01，DeepSeek发布DeepSeek-R1\n",
    "<!-- https://blog.csdn.net/bylander/article/details/145524526 -->\n",
    "<!-- https://magazine.sebastianraschka.com/p/understanding-reasoning-llms?continueFlag=af07b1a0954d90469bc6f6584075da3b -->\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/6.png\"  width=\"800\"/>\n",
    "</p>\n",
    "《DeepSeek R1 技术报告》既是一个有趣的案例研究，也是开发推理大型语言模型的蓝图。\n",
    "需要注意的是，DeepSeek 并没有发布单一的 R1 推理模型，而是推出了三种不同的变体：DeepSeek-R1-Zero、DeepSeek-R1 和 DeepSeek-R1-Distill。\n",
    "\n",
    "**（1）DeepSeek-R1-Zero：** 该模型基于 2024 年 12 月发布的 6710 亿参数预训练模型 DeepSeek-V3 基础模型。研究团队使用带有两种奖励的强化学习（RL）对其进行训练。这种方法被称为 “冷启动” 训练，因为它没有包含监督微调（SFT）步骤，而这一步骤通常是基于人类反馈的强化学习（RLHF）的一部分。\n",
    "\n",
    "**（2）DeepSeek-R1：** 这是 DeepSeek 的旗舰推理模型，以 DeepSeek-R1-Zero 为基础构建而成。该团队通过额外的监督微调阶段和进一步的强化学习训练对其进行了优化，在 “冷启动” 的 R1-Zero 模型基础上进行了改进。\n",
    "\n",
    "**（3）DeepSeek-R1-Distill：** 利用之前步骤中生成的监督微调数据，DeepSeek 团队对 Qwen 和 Llama 模型进行了微调，以增强它们的推理能力。虽然这并非传统意义上的蒸馏，但该过程包括使用更大的 DeepSeek-R1 6710 亿参数模型的输出对更小的模型（Llama 80 亿和 700 亿参数模型，以及 Qwen 15 亿–300 亿参数模型）进行训练。\n",
    "\n",
    "<!-- <p align = \"center\">    \n",
    "<img src=\"./image/9.png\"  width=\"1800\"/>\n",
    "</p> -->\n",
    "\n",
    "![](./image/9.png)\n",
    "<!-- https://chattools.cn/article/1502 -->\n",
    "\n",
    "\n",
    "#### 2025.04，阿里发布Qwen3\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/3.jpg\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "##### 预训练\n",
    "在预训练方面，Qwen3 的**数据集相比 Qwen2.5 有了显著扩展**。Qwen2.5是在 18T 个 token 上进行预训练的，而 Qwen3 使用的数据量几乎是其两倍，达到了约 36 T个 token，涵盖了 119 种语言和方言。\n",
    "为了构建这个庞大的数据集，Qwen3不仅从网络上收集数据，还从 PDF 文档中提取信息。Qwen3使用 Qwen2.5-VL 从这些文档中提取文本，并用 Qwen2.5 改进提取内容的质量。\n",
    "为了增加数学和代码数据的数量，Qwen3利用 Qwen2.5-Math 和 Qwen2.5-Coder 这两个数学和代码领域的专家模型合成数据，合成了包括教科书、问答对以及代码片段等多种形式的数据。\n",
    "\n",
    "预训练过程分为三个阶段。\n",
    "1. 在第一阶段（S1），模型在超过 30 T个 token 上进行了预训练，上下文长度为 4K token。这一阶段为模型提供了基本的语言技能和通用知识。\n",
    "2. 在第二阶段（S2），通过增加知识密集型数据（如 STEM、编程和推理任务）的比例来改进数据集，随后模型又在额外的 5 万亿个 token 上进行了预训练。\n",
    "3. 在最后阶段，使用高质量的长上下文数据将上下文长度扩展到 32K token，确保模型能够有效地处理更长的输入。\n",
    "\n",
    "由于模型架构的改进、训练数据的增加以及更有效的训练方法，Qwen3 Dense 基础模型的整体性能与参数更多的Qwen2.5基础模型相当。\n",
    "- 例如，Qwen3-1.7B/4B/8B/14B/32B-Base 分别与 Qwen2.5-3B/7B/14B/32B/72B-Base 表现相当。\n",
    "- 特别是在 STEM、编码和推理等领域，Qwen3 Dense 基础模型的表现甚至超过了更大规模的 Qwen2.5 模型。\n",
    "- 对于 Qwen3 MoE 基础模型，它们在仅使用 10% 激活参数的情况下达到了与 Qwen2.5 Dense 基础模型相似的性能。这带来了训练和推理成本的显著节省。\n",
    "\n",
    "##### 后训练\n",
    "为了开发能够同时具备思考推理和快速响应能力的混合模型，Qwen3实施了一个四阶段的训练流程。该流程包括：（1）长思维链冷启动，（2）长思维链强化学习，（3）思维模式融合，以及（4）通用强化学习。\n",
    "\n",
    "1. 在第一阶段，使用多样的的长思维链数据对模型进行了微调，涵盖了数学、代码、逻辑推理和 STEM 问题等多种任务和领域。这一过程旨在为模型配备基本的推理能力。\n",
    "2. 第二阶段的重点是大规模强化学习，利用基于规则的奖励来增强模型的探索和钻研能力。\n",
    "3. 在第三阶段，在一份包括长思维链数据和常用的指令微调数据的组合数据上对模型进行微调，将非思考模式整合到思考模型中。确保了推理和快速响应能力的无缝结合。\n",
    "4. 最后，在第四阶段，在包括指令遵循、格式遵循和 Agent 能力等在内的 20 多个通用领域的任务上应用了强化学习，以进一步增强模型的通用能力并纠正不良行为。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c41cd",
   "metadata": {},
   "source": [
    "## A2 基本原理：微调的核心价值与基本思路\n",
    "从迁移学习与分布偏移出发，说明监督微调（SFT）与对齐的关系，明确全参数微调（FPFT）与高效参数微调（PEFT）的本质差异与边界条件；通过小规模示例说明目标函数、学习率与批大小等关键超参对收敛与泛化的作用，并给出观察与记录要点。\n",
    "\n",
    "### 从迁移学习与分布偏移谈起\n",
    "- **迁移学习的本质**：先在大规模通用数据上预训练，再把已有能力“迁移”到目标任务/领域上。\n",
    "- **分布偏移的三类**：\n",
    "  - **输入分布偏移（covariate shift）**：用户提问的风格、格式、领域变了。\n",
    "  - **标签分布偏移（label shift）**：答案的总体分布变了（比如更偏短、更谨慎）。\n",
    "  - **概念偏移（concept shift）**：对同一输入，社会/规范上“什么是好答案”的判定变了。\n",
    "- 对应地，迁移阶段既要让模型适应新输入风格，也要让输出符合“新的好答案标准”。\n",
    "\n",
    "### SFT 与“对齐”的关系\n",
    "- **SFT（监督微调）是什么**：用高质量、人类示范的输入-输出对做“模仿学习”。它主要解决“看起来像”的问题——让模型学会在目标指令分布下，产出像样的答案。\n",
    "- **对齐（alignment）是什么**：让模型“做我们真正希望它做的事”，不仅要“像人类”，更要“符合人类偏好与安全规范”。常用方法含偏好优化（RLHF、DPO、KTO）、宪法式原则（Constitutional AI）、拒答/安全调优等。\n",
    "- **两者关系**：\n",
    "  - SFT常作为对齐的“地基”：先把模型带到“能按指令说清楚”的位置。\n",
    "  - 对齐在此基础上，解决“更合意、更安全”的目标，即更侧重偏好与规范层面的“概念偏移”校正。\n",
    "  - 只做SFT常提升“在分布内”的表现，但对偏好权衡、价值冲突和长尾安全不足；因此“对齐 ≠ 仅SFT”。\n",
    "\n",
    "### FPFT 与 PEFT 的本质差异\n",
    "- **FPFT（全参数微调）**：\n",
    "  - 更新全部参数，容量最大，可深度重塑表示与推理过程。\n",
    "  - 代价高（显存/算力/时间/存储），易遗忘通用能力，需要更稳健的训练与评估。\n",
    "  - 产出的是“一个新模型”，不易快速切换多领域变体。\n",
    "- **PEFT（高效参数微调，例如 LoRA/Adapter/Prefix/BitFit/QLoRA）**：\n",
    "  - 冻结骨干，只训练小量增量参数（常为低秩或小模块）。\n",
    "  - 训练/部署成本低、易维护，多域多版本可即插即用，保留基座的通用能力。\n",
    "  - 表达能力受限：巨大或结构性分布偏移时可能“力有未逮”，需要更高秩、更广覆盖，或最终转向FPFT。\n",
    "\n",
    "### 选择的边界条件与经验法则\n",
    "- **优先考虑 PEFT 的情形**：\n",
    "  - 资源有限、需要快速迭代或维护多套领域/品牌风格（可并行挂多适配器）。\n",
    "  - 主要是“输入风格/领域/格式”的偏移，或中等强度的偏好与安全校正（典型指令跟随、拒答策略、语气规范）。\n",
    "  - 数据量中小（如百万至数亿级标注/指令tokens），希望尽量保留基座能力、降低遗忘。\n",
    "  - 产业常态：SFT与偏好优化（RLHF/DPO）多数可用PEFT/QLoRA达到很强效果（即使在大模型上）。\n",
    "- **需要 FPFT 的情形**：\n",
    "  - **极大或结构性分布偏移**：需要系统性重塑内部表示与推理（如跨脚本低资源语言的深度能力、从零注入复杂算法/专业知识体系）。\n",
    "  - **单一高性能定制**：为某一任务/领域榨干上限，且算力/数据充足（十亿级以上tokens）；\n",
    "  - **架构/词表层改动**或跨模态深改（如新增模态投影层、系统性调整归一化/路由机制）。\n",
    "  - **广谱安全/价值修复**：需要内化到表示层的大规模纠偏，而非仅输出层面的策略修正。\n",
    "- **折中与进阶**：\n",
    "  - 扩大PEFT覆盖面与秩（更高rank、更广层数），或局部解冻（如LayerNorm、输出头）可显著拉近FPFT效果。\n",
    "  - 多阶段流程常见且有效：基座 → SFT（PEFT/QLoRA）→ 偏好优化（DPO/RLHF，仍可用PEFT）→ 必要时再做局部解冻或转FPFT。\n",
    "\n",
    "### 一句话把握\n",
    "- **SFT**让模型“像人类那样答”；**对齐**让模型“按人类真正想要的那样做”。  \n",
    "- **PEFT**高效、安全、易维护，适合多数指令与对齐工作；**FPFT**代价更高，但在巨大分布偏移或追求极限性能时更有把握。\n",
    "\n",
    "- 小结\n",
    "  - SFT解决“在新分布下学会表达”，对齐解决“按人类价值进行取舍与守则”。\n",
    "  - PEFT是工业界主力路径；FPFT用于超大改造、极致定制或架构层需求。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8674a9",
   "metadata": {},
   "source": [
    "## A3 训练显存计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55e6d5",
   "metadata": {},
   "source": [
    "### 1 模型训练时的内存消耗\n",
    "微软：《ZeRO: Memory optimizations Toward Training Trillion Parameter Models》\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/10.png\"  width=\"800\"/>\n",
    "</p>\n",
    "图 1：比较模型状态的每设备内存消耗，并采用三个阶段的 ZeRO-DP 优化。\n",
    "Ψ表示模型大小（参数数），K表示优化器状态的内存乘数，Nd表示DP度。\n",
    "在本例中，假设模型大小为 Ψ = 7.5B，DP 为 Nd = 64，K = 12，基于 Adam 优化器的混合精度训练。\n",
    "\n",
    "\n",
    "例如，一个拥有1.5B参数的GPT-2模型，在16位精度（FP16）下其参数（或权重）需要3GB的内存，但使用TensorFlow或PyTorch在单个GPU（显存为32GB）上训练时却无法实现。人们不禁会好奇，所有这些内存都去了哪里？\n",
    "在模型训练过程中，大部分内存被**模型状态**占用，即包含优化器状态、梯度和参数的张量。除这些模型状态外，其余的内存主要由激活值、临时缓冲区和碎片化的内存组成，统称为**残余状态**。以下将会详细探讨这些内存消耗的具体情况。\n",
    "\n",
    "#### A. 模型状态：优化器状态、梯度和参数\n",
    "在训练过程中，设备的绝大部分内存都被模型状态占用。例如，Adam优化器[6]是深度学习中最常用的优化器之一。Adam需要存储两个优化器状态：\n",
    "- 动量（Momentum）——对梯度的时间平均；\n",
    "- 方差（Variance）——梯度的二阶矩估计。\n",
    "\n",
    "因此，在使用Adam训练模型时，必须有足够的内存来保存这两个状态的副本。此外，还需要存储梯度和模型参数本身。在这三类与参数相关的张量中，优化器状态通常占用最多的内存，特别是在混合精度训练（Mixed-Precision Training）中。\n",
    "\n",
    "**混合精度训练**\n",
    "\n",
    "当前关于训练大模型的先进方法是采用混合精度（FP16/FP32）训练[26]，即将参数和激活值存储为FP16，从而利用GPU上的张量核心（Tensor Core）实现高吞吐量[27]。在混合精度训练中，前向传播和反向传播都使用FP16的权重和激活值。但为了有效地计算和应用更新（如梯度下降），优化器会保留FP32的参数副本和其他优化状态。\n",
    "\n",
    "以Adam优化器为例，训练拥有Ψ个参数的模型时，需要**同时存储**：\n",
    "- 参数的FP16副本和梯度（每个占用2Ψ字节）；\n",
    "- 参数的FP32副本、动量和方差（每个占用4Ψ字节）。\n",
    "  - 【注】当参数为FP16时，往往需要在优化器中再存一个梯度副本用于避免数值下溢为0（FP16主要用于减少激活值显存）；但**BF16**往往不用。具体情况具体分析\n",
    "\n",
    "定义“优化器状态所需的额外内存倍数”为K，即存储这些状态额外所需的内存与Ψ的乘积。例如，混合精度Adam的K值为12。\n",
    "因此，总的内存消耗为：\n",
    "\n",
    "`2Ψ（FP16参数） + 2Ψ（FP16梯度） + KΨ（FP32优化器状态） = 16Ψ 字节`\n",
    "\n",
    "对于参数量为15亿（1.5B）的GPT-2模型，内存需求至少为24GB。而仅存放FP16参数的内存需求仅为3GB，明显远远低于实际所需。\n",
    "\n",
    "#### B. 残余内存消耗\n",
    "\n",
    "**激活值**在训练中也会占用大量内存[8]。\n",
    "以1.5B参数、序列长度为1000、批次大小为32的GPT-2模型为例，大约需要60GB的内存。\n",
    "为了降低激活值的内存占用，通常会采用激活检查点（Activation Checkpointing）技术，即只存储部分激活值，待需要时再重新计算（Recomputation），将激活值内存削减至原来的平方根左右，但会带来约33%的额外计算开销[8]。\n",
    "使用激活检查点后，该模型的激活内存可以减至约8GB。\n",
    "虽然这种方法显著减少了激活值内存占用，但对于更大规模的模型（如参数达到100B级），即使采用激活检查点，内存需求仍然高达约60GB（批次大小为32）。\n",
    "\n",
    "此外，用于存储中间结果的**临时缓冲区**也会消耗不少内存。在某些操作中，例如梯度全规约（All-Reduce）或梯度范数计算，为了提升吞吐量，所有梯度通常会被“融合”到一个扁平缓冲区中进行操作。比如，跨设备的All-Reduce操作在大消息传输中效率更高。\n",
    "这些缓冲区通常是FP32的张量（即更高精度），虽然梯度本身多为FP16，但融合缓冲区的大小依操作而定。当模型参数很大时，这些临时缓冲区的大小是不可忽视的。以15亿参数的模型为例，扁平化的FP32缓冲区大约需要6GB。\n",
    "\n",
    "**内存碎片化**\n",
    "以上讨论了训练中实际的内存消耗。但是即使系统中有很多剩余内存，也可能由于碎片化导致“可用内存不足”而无法成功分配所需空间。\n",
    "这是因为内存的碎片化现象：请求的连续内存块不足，而实际上总剩余内存远大于请求值。特别是在训练极大模型时，碎片化问题尤为严重，甚至在仍有30%以上总内存未被使用的情况下也会出现“内存溢出”问题。\n",
    "\n",
    "\n",
    "### 2 DeepSeed-ZeRO\n",
    "简要介绍当前广泛使用的分布式训练范式，由微软提出的DeepSpeed-ZeRO-DP。\n",
    "\n",
    "虽然现有的DP方法在每台设备上复制模型状态，并带来显著的内存开销，ZeRO-DP通过将优化器状态、梯度和参数划分到各个数据并行进程中，消除了这种内存冗余。图1量化并可视化了有无ZeRO-DP情况下的内存需求。该图显示了在每个优化阶段（Pos、Pg 和 Pp）后，经过划分后内存占用的变化情况，即优化的三个阶段。  \n",
    "#### A. Pos：优化器状态划分  （ZeRO-1: Pos）\n",
    "对于数据并行度为 Nd 的情况，将优化器状态平均划分为 Nd 个部分，使第 i 个数据并行进程只更新对应第 i 个划分的优化器状态。\n",
    "因此，每个数据并行进程仅需存储和更新总优化器状态的 1/Nd ，以及参数的 1/Nd。\n",
    "为确保所有参数都已更新，在每个训练步骤结束后，进行一次全体收集（all-gather）操作，以获得所有数据并行进程中更新的完整参数。  \n",
    "内存节省：如图1所示，经过状态划分后，内存消耗从 4Ψ + KΨ 减少到 4Ψ + (KΨ)/Nd。以图中示例为例，一个参数为75亿的模型在64路DP（Nd=64）下使用Pos优化后，需要31.4GB内存，而标准DP则需要120GB。此外，当Nd值较大时，模型状态的内存需求从 16Ψ（即4Ψ + 12Ψ）减少到约4Ψ，减少了约4倍。\n",
    "\n",
    "#### B. Pg：梯度划分  （ZeRO-2: Pos+g）\n",
    "每个数据并行进程只更新对应参数部分的梯度，因此它只需要对应参数的缩减梯度。在反向传播过程中，梯度会在对应的进程上被归约（reduce），随后不再需要这些梯度，相关内存可以释放。这将原本占用 2Ψ 内存的梯度空间，缩减到 2Ψ/Nd。  \n",
    "这基本上是一个Reduce-Scatter操作，不同参数对应的梯度被分散到不同的进程进行减少。为了提高效率，采用分桶策略：将所有对应同一分区的梯度归入一个桶，并对整个桶进行减少操作。这类似于NVIDIA AMP优化中的梯度桶化，旨在Overlap通信和计算。在本方法中，在分区边界执行reduce操作，以减少内存占用并实现通信与计算的重叠。  \n",
    "内存节省：通过去除梯度和优化器状态的冗余，内存需求进一步降低到 2Ψ + (14Ψ)/Nd。以图中的例子为例，使用Pos+g在64路DP下，参数为75亿的模型只需约16.6GB内存，而标准DP则需要120GB。当Nd较大时，模型状态的内存从 16Ψ 下降到约2Ψ。\n",
    "\n",
    "#### C. Pp：参数划分  （ZeRO-3: Pos+g+p）\n",
    "参数的处理方式与优化器状态和梯度相似。每个进程只存储所属分区的参数，当需要其他分区的参数进行前向或反向传播时，相关参数通过广播（broadcast）从对应的进程获取。虽然这似乎会带来较大通信开销，但实际上仅会使总通信量变为基线DP的1.5倍，同时实现的内存节省与模型复制（MP）相当，但通信开销只有其一部分，具体在第七章C节详细讨论。  \n",
    "内存节省：采用参数划分后，模型参数为 Ψ 时的存储需求从 16Ψ 降到 16Ψ/Nd。以示例模型为例，在64路DP下使用Pos+p+g，只需约1.9GB模型状态内存，而非标准DP下的120GB。这意味着，ZeRO显著扩展了DP的能力——只要有足够多的设备共享模型状态，就能训练规模更大的模型。\n",
    "\n",
    "#### D. 模型规模的影响  \n",
    "Pos、Pos+g 和 Pos+g+p三个划分阶段，分别将模型状态的内存需求在每个数据并行进程内部，减少至原来的4倍、8倍和Nd倍。表一分析了不同模型在这三个阶段的内存消耗情况。没有ZeRO时，无论DP程度如何，内存都等于第一行的值。当Nd=64时，使用Pos、Pos+g和Pos+g+p，ZeRO可以训练多达7.5亿、14亿和128亿参数的模型。若Nd=1024，启用所有ZeRO优化后（Pos+g+p），甚至能训练参数达一万亿（1万亿）的模型。没有ZeRO，DP最大的模型只能训练不到15亿参数。  \n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/11.png\"  width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "如图为ZeRO-DP三阶段都启用的伪代码。先给每个进程都分配一部分层，前向传播时逐层*收集模型切片*然后计算然后*释放模型切片*，反向传播时逐层*收集模型切片*然后计算然后规约，然后*释放模型切片+梯度切片*。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee3ec7",
   "metadata": {},
   "source": [
    "\n",
    "## A4 微调范式对比与框架选型\n",
    "<!-- 建立“任务规模×算力×时限×维护成本”的选型框架：FPFT具备充分表达但资源开销较大；LoRA以低秩分解提升参数效率；AdaLoRA在秩自适应下改善表达与稳定性；QLoRA以量化降低显存占用但需关注量化误差。基于统一数据与评测口径，形成各范式在显存、吞吐与效果上的可比结论，用于实际决策。 -->\n",
    "<!-- https://zhuanlan.zhihu.com/p/1903385294344021244 -->\n",
    "### 1 FPFT、LoRA、QLoRA 微调范式选型思路\n",
    "围绕三种主流范式，给出原理、显存估算、示例与适用场景，最后给出简明选型准则。\n",
    "\n",
    "#### 一、全参数微调（FPFT）\n",
    "- **原理（做什么）**  \n",
    "  - 更新模型全部权重，能够系统性重塑表示与推理过程；常配合混合精度与分布式（ZeRO-2/3）、激活检查点。\n",
    "- **显存估算（怎么算）**  \n",
    "  - 核心占用（FP16/BF16 + AdamW）：参数 2X + 梯度 2X + 优化器状态 8X = $12X$ GB  \n",
    "  - 含激活与开销的经验预算：下限约 14X–18X GB；更稳妥按 ≥ 20X GB 估。  \n",
    "  - 激活粗估：$ \\text{ActMem} \\approx \\text{batch} \\times \\text{seq} \\times \\text{hidden} \\times \\text{layers} \\times \\text{bytes} \\times f $，bytes≈2（FP16），f 常取 2–3。\n",
    "- **示例（多大卡能跑）**  \n",
    "  - 7B：核心 ≈ 12×7=84 GB；加激活后 >100 GB。常用 2×80GB（ZeRO-2/3）。  \n",
    "  - 70B：核心 ≈ 840 GB；总需求 >1 TB，需要多机多卡集群。\n",
    "- **适用场景（何时用）**  \n",
    "  - 极大/结构性分布偏移（跨语种深改、专业知识体系注入）。  \n",
    "  - 追求单一领域极限性能；需要内化价值/安全到表示层。  \n",
    "  - 做架构/词表/模态侧改动。\n",
    "- **注意点（风险与建议）**  \n",
    "  - 成本高、易遗忘通用能力；需严格评测与回滚预案。  \n",
    "  - 建议：BF16 优先、激活检查点、梯度累积、ZeRO 分片、梯度裁剪与稳定的 LR 计划。\n",
    "\n",
    "#### 二、LoRA（Low-Rank Adaptation）\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/13.png\"  width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "- **原理（做什么）**  \n",
    "  - 冻结基座，给部分权重矩阵注入低秩增量 $\\Delta W = A B$，仅训练少量适配器参数，显著降显存/存储/部署成本。\n",
    "- **显存估算（怎么算）**  \n",
    "  - 基座（FP16/BF16）≈ $2X$ GB；LoRA 参数/梯度/优化器相对很小。  \n",
    "  - 总显存 ≈ $2X$ GB + 激活 + 少量开销；经验范围 ≈ (2.5–4)×X GB（取决于激活）。\n",
    "- **示例（多大卡能跑）**  \n",
    "  - 7B：基座 ≈14 GB；激活（有检查点、适中 batch/seq）≈5–15 GB → 总计 ≈20–30+ GB（24–40GB 卡可行）。  \n",
    "  - 70B：基座 ≈140 GB；总计 ≈160–190+ GB（2–3×80GB）。\n",
    "- **适用场景（何时用）**  \n",
    "  - 资源有限、需快速迭代/多域多版本（易插拔、便于合并/切换）。  \n",
    "  - 输入风格/格式偏移、中等强度偏好与安全对齐（SFT/DPO/RLHF 的主力路径）。\n",
    "- **注意点（风险与建议）**  \n",
    "  - 表达受限：rank/覆盖层数不足时上限受约束；必要时提高 rank/层覆盖或局部解冻。  \n",
    "  - 关键超参：  \n",
    "    - **rank r**（表达力/容量）、**alpha**（缩放）、**dropout**（正则）、  \n",
    "    - **target modules**（常见 q/k/v/o、MLP 投影层）、  \n",
    "    - **学习率/调度**（LoRA 参数可用较大学习率，注意稳定性）。\n",
    "\n",
    "#### 三、QLoRA（4-bit 基座 + LoRA）\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/14.png\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "- **原理（做什么）**  \n",
    "  - 将基座权重量化到 4-bit（如 NF4，常配 Double Quant），计算仍用 FP16/BF16，训练仅 LoRA 适配器；可配“分页优化器”降低峰值显存。\n",
    "- **显存估算（怎么算）**  \n",
    "  - 基座 4-bit 近似 ≈ $0.5X$ GB（含量化开销近似）；总显存多在 (0.7–1.5)×X GB。  \n",
    "  - 激活仍是大头，需同样依赖梯度检查点与 batch/seq 控制。\n",
    "- **示例（多大卡能跑）**  \n",
    "  - 7B：基座 ≈3.5–5 GB；总计常 10–16 GB（单卡 12–24GB 友好）。  \n",
    "  - 70B：在 80GB 单卡上有机会跑通（batch/seq 受限，吞吐较低）。\n",
    "- **适用场景（何时用）**  \n",
    "  - 单卡显存紧张（10–24GB）也要完成指令微调/原型验证。  \n",
    "  - 多域多版本、成本敏感、需要快速试错与上线演示。\n",
    "- **注意点（风险与建议）**  \n",
    "  - 量化误差可能影响难任务/细粒度对齐，建议：NF4 + 双量化、计算类型 BF16、谨慎量化嵌入/LayerNorm。  \n",
    "  - 可能需要提高 LoRA rank/扩大覆盖弥补量化损失；强烈建议激活检查点与小 batch 试跑观测峰值显存。\n",
    "\n",
    "#### 四、选型总览（快速决策）\n",
    "- **先看算力/时限**  \n",
    "  - 单卡 ≤24GB：优先 QLoRA；24–80GB：优先 LoRA；充足多卡、追极限：FPFT。  \n",
    "- **再看任务“偏移强度”**  \n",
    "  - 结构性/极大偏移、架构改动：FPFT；中等或输入风格偏移：LoRA/QLoRA。  \n",
    "- **再看业务诉求**  \n",
    "  - 单一高性能定制与深度内化：FPFT；多版本快速迭代与低成本维护：LoRA/QLoRA。  \n",
    "- **共同工程建议**  \n",
    "  - 激活检查点（几乎必备）、梯度累积、监控 `torch.cuda.max_memory_allocated()`、必要时用 ZeRO-2/3 分片；小 batch/短 seq 先试跑再放大。\n",
    "\n",
    "——\n",
    "\n",
    "- 总结要点\n",
    "  - FPFT：能力最强、成本最高；按 ≥20X GB 估显存，适合极大偏移与极限性能需求。  \n",
    "  - LoRA：性价比高、上线友好；显存 ≈(2.5–4)×X，适合多数 SFT/对齐工作。  \n",
    "  - QLoRA：极致节省；显存 ≈(0.7–1.5)×X，适合单卡/低成本快速验证与多版本维护。\n",
    "\n",
    "\n",
    "\n",
    "### 2 框架选型\n",
    "\n",
    "  <!-- 框架选型与环境准备：unsloth / ms-swift / llama-factory -->\n",
    "<!-- 讲解两框架在 SFT/PEFT/对齐中的接口与流水线差异，结合显存与吞吐的经验阈值给出适配建议；完成环境验证与基线运行，记录资源占用、收敛速度与稳定性，为下午的案例训练提供可靠起点。 -->\n",
    "\n",
    "#### 1 LLaMA-Factory / Unsloth / ms-SWIFT 对比表\n",
    "\n",
    "| 维度 | LLaMA-Factory | Unsloth | ms-SWIFT |\n",
    "| --- | --- | --- | --- |\n",
    "| 上手难度 | 中等；提供 Gradio UI 与 CLI，适合不同层次用户 | 低；API 简洁、文档详细，新手友好 | 中等；依赖 ModelScope 生态，需了解框架集成，文档可能提供支持 |\n",
    "| 资源需求 | 支持多硬件（NVIDIA/AMD GPU、Ascend NPU）；量化可降内存；全参微调显存高 | 显存占用优化显著；手工优化 GPU 内核；资源效率高 | 集成 PEFT；显存需求可能降低；优化依赖 ModelScope 底层 |\n",
    "| 显存占用 | 量化可降显存，但全参微调占用高 | 显著优化，适合有限硬件 | 取决于 PEFT 与 ModelScope 实现 |\n",
    "| 性能表现 | 支持 LoRA/DPO、FlashAttention 等；速度较 Unsloth 慢 | 微调速度最快（快 2.5–10×）；大数据场景优势明显 | 未知；依赖 PEFT，特定任务可能高效，缺少直观对比 |\n",
    "| 训练成本 | 支持 4/8 位量化与分布式；全参微调成本高但灵活 | 显存与时间成本低；适合快速迭代与受限资源 | PEFT 减少可调参数，成本或更低；生态依赖性强 |\n",
    "| 模型支持 | 覆盖广（100+）；含多模态 LLaVA、MoE（Mixtral）；社区活跃 | 兼容主流（Llama、Mistral 等）；覆盖略少于 LLaMA-Factory | 支持多架构与训练范式；依赖 ModelScope，可能对特定模型优化 |\n",
    "| 分布式训练 | 支持多 GPU/多节点，适合大规模任务 | 未明确；侧重单卡优化，或需外部工具扩展 | 未明确；可能依赖 ModelScope 基础设施 |\n",
    "| 适用场景 | 多硬件环境、复杂任务（多模态/RLHF）、需灵活配置 | 资源有限、追求快速微调、单卡高效运行 | ModelScope 生态内任务、PEFT 需求、特定领域适配 |\n",
    "\n",
    "- 核心结论：Unsloth 速度与显存效率最佳；LLaMA-Factory 最全面与灵活；ms-SWIFT 依托 ModelScope，适合生态内的 PEFT/定制场景\n",
    "- 不过，目前LLaMA-Factory还不支持GRPO；unsloth主要仅限于单卡场景\n",
    "\n",
    "#### 2 配置环境\n",
    "命令行执行：\n",
    "```bash\n",
    "# conda环境\n",
    "conda create -p ./env python=3.10\n",
    "conda activate ./env\n",
    "\n",
    "# ms-swift训练环境\n",
    "git clone https://github.com/modelscope/ms-swift.git\n",
    "cd ms-swift\n",
    "pip install -e .\n",
    "cd ..\n",
    "\n",
    "# 下载模型文件\n",
    "apt-gpt update\n",
    "apt-get install git-lfs\n",
    "git lfs install\n",
    "mkdir models && cd models\n",
    "\n",
    "\n",
    "GIT_LFS_SKIP_SMUDGE=1 git clone https://hf-mirror.com/Qwen/Qwen2.5-VL-7B-Instruct\n",
    "cd Qwen2.5-VL-7B-Instruct/\n",
    "git lfs pull\n",
    "cd ..\n",
    "\n",
    "GIT_LFS_SKIP_SMUDGE=1 git clone https://hf-mirror.com/Qwen/Qwen2.5-7B-Instruct\n",
    "cd Qwen2.5-7B-Instruct/\n",
    "git lfs pull\n",
    "cd ..\n",
    "\n",
    "mkdir data && cd data\n",
    "git lfs install\n",
    "pip install torchvision\n",
    "\n",
    "pip install qwen-vl-utils[decord]\n",
    "git clone https://www.modelscope.cn/datasets/swift/self-cognition.git\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6686130",
   "metadata": {},
   "source": [
    "## A5 LoRA / QLoRA 案例（指令遵循小模型）\n",
    "在单卡（≤24GB）条件下完成一次基于 LoRA 或 QLoRA 的指令微调演示；比较不同秩、α、dropout、量化位宽、学习率、批大小对收敛与输出质量的影响；开展推理验证并记录延迟与效果的关键观察点，沉淀参数选择的经验规则。\n",
    "\n",
    "### 1 一般性结论\n",
    "以下结论适用于 LoRA 与 QLoRA（Q 表示底座权重被量化，通常 8/4-bit）。Q 会引入量化噪声，使得训练更“脆弱”，因此在相同设置下往往需要更保守的学习率、更小的有效更新尺度或更大的 rank 来弥补容量。\n",
    "\n",
    "- 有效更新尺度近似由三者共同决定：学习率 × 适配器步幅比例 × LoRA 缩放，其中 LoRA 缩放约为 \\( \\alpha / r \\)。因此 r、alpha、学习率强耦合，调一个要联动另两个。\n",
    "\n",
    "#### 各超参影响与建议\n",
    "\n",
    "- **rank（r）容量/表达力**\n",
    "  - 影响：r↑ → 适配器容量↑，可拟合复杂分布、提升上限；但过大更易过拟合、显存与算力开销↑、收敛更慢。\n",
    "  - LoRA：r=4–16 常够用；大任务/多域可用 32–64。\n",
    "  - QLoRA：量化引入噪声，常用 r 比 LoRA 略大一档以抵消噪声（如 16–64）。\n",
    "  - 观察：验证集指标是否随 r 提升而仍有收益；若收益趋平或过拟合增大，r 已过大。\n",
    "\n",
    "- **alpha（缩放系数）与有效步幅**\n",
    "  - 影响：控制注入主干的更新幅度；有效缩放约为 \\( \\alpha / r \\)。在相同学习率下，alpha↑ → 有效更新↑，收敛更快但更易震荡。\n",
    "  - 经验：保持 \\( \\alpha / r \\in [1, 8] \\) 较稳健。常见组合：\n",
    "    - r=8, alpha=16/32（\\( \\alpha/r=2/4 \\)）\n",
    "    - r=16, alpha=16/32（\\( \\alpha/r=1/2 \\)）\n",
    "  - QLoRA：倾向更保守（或适当增大 r 而不过度增大 \\( \\alpha/r \\)）。\n",
    "\n",
    "- **dropout（LoRA 适配器上的丢弃）**\n",
    "  - 影响：正则化、防过拟合；过大则抑制有效容量、降低上限并放慢收敛。\n",
    "  - 经验：0–0.1 常用；小数据/窄域建议 0.05–0.1，数据大或强正则不需要时设 0。\n",
    "  - QLoRA：量化已带噪，通常不需要太大 dropout（0–0.05）。\n",
    "\n",
    "- **量化位宽（QLoRA 特有）**\n",
    "  - 影响：位宽↓ → 记忆/带宽省、速度↑，但量化误差↑，训练更脆弱，可能降低收敛速度与最终质量。\n",
    "  - 建议：4-bit（NF4）为 QLoRA 主流，质量接近 8-bit 且显存大幅下降；8-bit 更稳但省存效果差；≤3-bit 质量显著下降，不推荐。\n",
    "  - 配置联动：位宽↓ → 适当减小学习率或 \\( \\alpha/r \\)，或增大 r；训练更需要梯度裁剪与稳定的优化器设置。\n",
    "\n",
    "- **学习率（lr）**\n",
    "  - 影响：lr↑ → 早期收敛更快但更易发散/欠拟合细节；lr↓ → 稳定但慢、可能陷入欠学习。\n",
    "  - 经验范围（指适配器参数的 lr；使用 AdamW/Adam8bit/Adafactor 等）：\n",
    "    - 指令微调：1e-4–3e-4 常见；小数据或高噪声（QLoRA/低位宽）用 5e-5–2e-4。\n",
    "    - 生成质量优先且数据干净：1e-4 左右更稳。\n",
    "  - 联动：若 \\( \\alpha/r \\) 较大，应相应降低 lr；若 r 较大且 \\( \\alpha/r \\) 保守，可维持或略升 lr。\n",
    "\n",
    "- **批大小（batch size / 有效 batch，包括梯度累计）**\n",
    "  - 影响：batch↑ → 梯度方差↓，收敛更平滑；但过大可能泛化变差。小 batch 训练噪声大，需要更小 lr 或更长训练。\n",
    "  - 经验：遵循线性缩放规律可作为起点：总有效 batch×lr 近似保持不变（再用 warmup/余弦退火微调）。\n",
    "  - QLoRA：为稳健可优先增大有效 batch（通过梯度累计）而非单步 lr。\n",
    "  - 实操：关注“每步处理的 token 数”作为更稳定的缩放度量。\n",
    "\n",
    "#### 常用方案\n",
    "\n",
    "- LoRA（7B–13B，指令微调，中等数据量）\n",
    "  - r=8–16，alpha=16–32（\\( \\alpha/r=1–4 \\)），dropout=0–0.05\n",
    "  - lr=1e-4–2e-4，warmup 1–3% 步数，梯度裁剪 0.5–1.0\n",
    "  - 有效 batch 512–4096 序列-令牌（按显存与吞吐折中）\n",
    "\n",
    "- QLoRA（4-bit NF4，7B–13B）\n",
    "  - r=16–64（比 LoRA 略大），alpha 选择使 \\( \\alpha/r=1–2 \\)\n",
    "  - dropout=0–0.05\n",
    "  - lr=5e-5–2e-4，优先用较大有效 batch 平滑训练\n",
    "  - 需要：稳定量化实现、梯度裁剪、较小 weight decay（如 0–0.05）\n",
    "\n",
    "#### 交互与取舍要点\n",
    "- 调 r 与 alpha 时盯紧 \\( \\alpha/r \\) 与 lr 的乘积；这三者共同决定“有效更新强度”。\n",
    "- 量化位宽越低，越倾向：r↑、lr↓、dropout 保守、batch↑。\n",
    "- 小数据/分布窄：更小 r 与更强正则（或早停）；大数据/多域：更大 r 与更长训练。\n",
    "- 先保证稳定（不发散、不震荡），再逐步增加有效更新强度以提升上限。\n",
    "\n",
    "#### 观察哪些指标来做决策\n",
    "- 训练/验证损失是否平稳下降、是否出现长时间震荡或发散。\n",
    "- 生成质量（人工/自动评测）、长度控制、重复/幻觉率。\n",
    "- 梯度范数与更新范数是否频繁尖峰（过大 lr 或 \\( \\alpha/r \\)）。\n",
    "- 不同 r 的边际收益是否已饱和。\n",
    "\n",
    "### 2 实验\n",
    "\n",
    "#### 2.1 OCR文字识别（多模态sft）-lora\n",
    "```\n",
    "# 20GB lora\n",
    "CUDA_VISIBLE_DEVICES=1 \\\n",
    "MAX_PIXELS=1003520 \\\n",
    "swift sft \\\n",
    "    --model models/Qwen2.5-VL-7B-Instruct \\\n",
    "    --dataset 'data/LaTeX_OCR:human_handwrite' \\\n",
    "    --split_dataset_ratio 0.01 \\\n",
    "    --train_type lora \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 32 \\\n",
    "    --target_modules all-linear \\\n",
    "    --freeze_vit true \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --eval_steps 1 \\\n",
    "    --save_steps 50 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --max_length 4096 \\\n",
    "    --output_dir output \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --dataloader_num_workers 4\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"image/eval_token_acc_1.png\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "- **总体收敛**：`eval_loss` 从 0.828 → 0.0206，约下降 97.5%；`eval_token_acc` 从 0.8835 → 0.9899，提升约 +10.6 个百分点（token 误差率由 11.6% 降至 1.0%，相对误差下降约 91%）。  \n",
    "- **最佳点与回升**：最优 `eval_loss` 出现在 step 30（约 36% 进度）为 0.01388，随后缓慢回升并在 0.020–0.031 之间波动，最终值较最优高约 48%（轻微过拟合/平台期信号）。  \n",
    "- **训练态势**：最终 `train_loss`=0.0816，训练稳定、显存约 18 GiB、速度 ~0.113 it/s；`grad_norm` 全程较小（<1），无明显梯度不稳。  \n",
    "- **学习率调度**：预热后升至 1e-4（step 5），随后余弦/线性衰减至 3.6e-7；最优点出现在 LR 仍较高但已开始下降的早中期，之后收益趋于饱和。  \n",
    "- **指标平台**：自 step 30 起 `eval_token_acc` 基本稳定在 ~0.99，继续训练收益极小，表明已达任务可达上限或验证集可分性较高。\n",
    "\n",
    "\n",
    "#### 2.2 OCR文字识别（多模态sft）-qlora\n",
    "```\n",
    "pip install bitsandbytes\n",
    "\n",
    "# qlora 8.15G\n",
    "CUDA_VISIBLE_DEVICES=1 \\\n",
    "MAX_PIXELS=1003520 \\\n",
    "swift sft \\\n",
    "    --model models/Qwen2.5-VL-7B-Instruct \\\n",
    "    --dataset 'data/LaTeX_OCR:human_handwrite' \\\n",
    "    --split_dataset_ratio 0.01 \\\n",
    "    --train_type lora \\\n",
    "    --quant_bits 4 \\\n",
    "    --quant_method bnb \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --lora_rank 16 \\\n",
    "    --lora_alpha 32 \\\n",
    "    --target_modules all-linear \\\n",
    "    --freeze_vit true \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --eval_steps 1 \\\n",
    "    --save_steps 50 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --max_length 4096 \\\n",
    "    --output_dir output \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --dataloader_num_workers 4\n",
    "```\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"image/eval_token_acc_2.png\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "- **总体收敛**：`eval_loss` 从 0.8928 → 0.00890，约下降 99.0%；`eval_token_acc` 从 0.8608 → 0.9975，提升约 +13.7 个百分点（token 误差率由 13.9% 降至 0.25%，相对误差下降约 98.2%）。\n",
    "- **最佳点与回升**：最优 `eval_loss` 出现在 step 81（约 97.6% 进度）为 0.00848；之后在 0.0085–0.0117 之间小幅波动，最终值较最优高约 4.9%（基本为平台期，过拟合迹象不明显）。\n",
    "- **训练态势**：最终 `train_loss`=0.0903，训练稳定；显存约 8.15 GiB；速度 ~0.095 it/s（总时长 ~14m38s）。`grad_norm` 预热初期最高 ~2.88，随后稳定在 <1 并逐步降至 ~0.12，无梯度震荡。\n",
    "- **学习率调度**：预热后在 step 5 升至 1e-4，随后持续衰减至 ~3.6e-7（step 80）；最优点出现在极低 LR 的后期，说明后期微小步长仍能带来细微泛化收益。\n",
    "- **指标平台**：自 ~step 33 起 `eval_token_acc` 基本稳定在 ~0.995–1.0；继续训练对 `eval_loss` 的改进极小，表明任务已接近可达上限或验证集可分性较高。\n",
    "\n",
    "#### 2.3 自我认知微调-lora\n",
    "```\n",
    "# 17.2GiB lora\n",
    "CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift sft \\\n",
    "    --model ./models/Qwen2.5-VL-7B-Instruct \\\n",
    "    --train_type lora \\\n",
    "    --dataset './data/self-cognition#1000' \\\n",
    "    --split_dataset_ratio 0.02 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 32 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --eval_steps 1 \\\n",
    "    --save_steps 100 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --model_author lwh \\\n",
    "    --model_name lwh-robot\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"image/eval_token_acc_3.png\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"image/eval_loss_3.png\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "##### 训练与评测概览\n",
    "- 模型与训练方式：Qwen2.5-VL-7B-Instruct，PEFT/LoRA（可训练参数约 20.19M，占比约 0.24%）\n",
    "- 训练配置与进度：1 epoch，63 steps，学习率从 ~1e-4 逐步衰减到 ~5.6e-7\n",
    "- 资源与速度：显存占用 ~16.7 GiB；训练速度 ~0.149 iter/s；总时长 ~7 分钟\n",
    "\n",
    "##### 指标走势与拐点\n",
    "- eval_loss：从 1.914 初始快速下降，至 step 36 降至最低 1.1857（最佳），之后缓慢回升，在最终 step（63）为 1.2597\n",
    "- eval token_acc：从 0.580 上升至峰值约 0.7037（step 16–23 附近），随后轻微回落并在后半程徘徊于 0.642–0.691\n",
    "- train_loss：持续下降（2.17 → 0.40 左右），与后半程 eval_loss 走弱形成背离，显示轻微过拟合\n",
    "\n",
    "##### 关键里程碑（摘录）\n",
    "| Step | eval_loss | token_acc | train_loss | lr |\n",
    "|---|---|---|---|---|\n",
    "| 1 | 1.9140 | 0.5803 | 2.1731 | 9.99e-05 |\n",
    "| 20 | 1.2444 | 0.7037 | 0.7735 | 7.71e-05 |\n",
    "| 36 | 1.1857 | 0.6914 | 0.5684 | 4.13e-05 |\n",
    "| 50 | 1.2491 | 0.6667 | 0.3973 | 1.01e-05 |\n",
    "| 63 | 1.2597 | 0.6420 | — | ~5.6e-07（至 step 60） |\n",
    "\n",
    "##### 收益量化\n",
    "- 最佳点（step 36）相对首评估改善：\n",
    "  - eval_loss：下降约 38.1%（1.914 → 1.186）\n",
    "  - token_acc：+11.1–12.4 个百分点后回落至 ~0.691（区间峰值 ~0.704）\n",
    "- 最终点（step 63）相对首评估仍有明显提升：\n",
    "  - eval_loss：下降约 34.2%（1.914 → 1.260）\n",
    "  - token_acc：+6.2 个百分点（0.580 → ~0.642）\n",
    "\n",
    "##### 诊断与结论\n",
    "- 中后期出现轻微过拟合信号：train_loss 继续下降而 eval_loss 回升、token_acc 回落\n",
    "- 学习率长尾趋近于零后并未带来额外泛化收益；最佳泛化出现在中段 lr 仍较高时（step 36，lr≈4.13e-05）\n",
    "- 最优 checkpoint 出现在 step 36（best_metric=1.1857），但当前未启用 “best 模型回载”（best_model_checkpoint=null）\n",
    "\n",
    "#### 2.3 自我认知微调-qlora\n",
    "```\n",
    "# qlora 7.66G\n",
    "CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift sft \\\n",
    "    --model ./models/Qwen2.5-VL-7B-Instruct \\\n",
    "    --train_type lora \\\n",
    "    --dataset './data/self-cognition#1000' \\\n",
    "    --split_dataset_ratio 0.02 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 32 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --eval_steps 1 \\\n",
    "    --save_steps 100 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --model_author lwh \\\n",
    "    --model_name lwh-robot \\\n",
    "    --quant_bits 4 \\\n",
    "    --quant_method bnb\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"output/Qwen2.5-VL-7B-Instruct//v12-20250915-121041/images/eval_token_acc.png\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"output/Qwen2.5-VL-7B-Instruct/v12-20250915-121041/images/eval_loss.png\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "##### 评测指标与变化分析\n",
    "\n",
    "- **整体收敛**：单轮训练共 63 步，约 8m27s 完成。训练端持续下降（最终 `train_loss≈0.73`，后段单步 `loss≈0.41~0.57`；`train_token_acc≈0.86`），验证端在中前期快速改善，随后进入震荡平台期。\n",
    "\n",
    "##### 核心指标轨迹\n",
    "- **Eval loss**\n",
    "  - 初始：1.94（step 1）\n",
    "  - 快速下降阶段：至约 1.25（step 20 附近）\n",
    "  - 最优点：**1.2217（step 27）**\n",
    "  - 后续：维持在 1.24~1.30 小幅震荡，末尾 1.3023（step 63）\n",
    "  - 解读：前半段收益显著，约 step 25~33 为拐点区；之后学习率已很低，指标无进一步提升，且略有回退，出现轻度过拟合迹象。\n",
    "- **Eval token_acc**\n",
    "  - 起点：0.543\n",
    "  - 上升并平台：升至 0.67~0.70 区间后基本持平（多数在 0.679~0.704）\n",
    "  - 解读：模型在当前数据/配置下的可达上限约 0.70，继续训练未带来显著增益。\n",
    "- **训练/验证差距**\n",
    "  - 训练端 `token_acc≈0.86` 明显高于验证端 ~0.70；训练端 loss 明显低于验证端\n",
    "  - 解读：典型轻度过拟合，建议采用早停或更强正则化。\n",
    "- **学习率与稳定性**\n",
    "  - 学习率自 ~1e-4 逐步衰减至 ~5.6e-07（step 60），后段基本耗尽更新幅度\n",
    "  - `grad_norm≈1.4~2.08`，稳定无爆炸；显存 ~7.66 GiB，迭代速率 ~0.124 it/s，运行稳定\n",
    "\n",
    "##### 里程碑与建议操作\n",
    "- **最佳模型点**：建议采用约 step 27（eval_loss=1.2217）附近的检查点作为最佳权重。\n",
    "  - 当前日志显示 `best_metric=1.2217`，但 `best_model_checkpoint=null`，说明未启用“最优模型保存”。下次训练建议：\n",
    "    - 开启 `--load_best_model_at_end`\n",
    "    - 设置 `--metric_for_best_model eval_loss`，`--greater_is_better False`\n",
    "    - 配合 `--save_total_limit` 限制磁盘占用\n",
    "- **早停策略**：在 eval 连续若干次（如 5 次）无改善后停止（patience=5），可节省 40%~50% 训练时间同时保持最优点性能。\n",
    "- **正则与超参微调**\n",
    "  - 轻度过拟合：考虑 `weight_decay=0.01`、增加 `lora_dropout`（如 0.05→0.1）、启用 `label_smoothing`（如 0.05）\n",
    "  - 学习率：当前 1e-4 有效，但可试 `5e-5` 或更长余弦尾部，提高后段稳健性\n",
    "  - 批次/梯度：若显存允许，提高有效 batch 或使用更长 `grad_accum`，提升统计稳定性\n",
    "- **数据与评测**\n",
    "  - 若 token-level acc 已平台，进一步提升多来自数据：扩大/清洗/去重，提升指令多样性与难度\n",
    "  - 增加更贴近任务的序列级指标（如 exact match、rouge/bleu、人评）补充 token_acc 的局限\n",
    "\n",
    "##### 结论\n",
    "- 训练有效收敛，验证性能在 step 25~33 达到平台并出现轻度过拟合；最佳点在 step 27（eval_loss≈1.2217，eval_token_acc≈0.667）。建议启用“最优模型保存”与早停，并通过轻量正则与小幅学习率/批次调整，提升后段稳健性；更大幅提升需要从数据与任务对齐的评测指标入手。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600255d",
   "metadata": {},
   "source": [
    "\n",
    "## A6 Prompt 工程与数据合成（小样本强化）\n",
    "\n",
    "### 目标\n",
    "- 在明确任务结构的前提下，设计可迁移的提示模板。\n",
    "- 通过变量化与多样化扩展少量样本，实现低成本数据放大。\n",
    "- 建立自动+人工的质检与偏差控制，确保合成数据可用且稳健。\n",
    "- 在小样本条件下验证对特定子任务的实际增益，并沉淀可复用资产。\n",
    "\n",
    "### 一、任务结构化与可迁移模板\n",
    "- **任务卡（Task Card）最小字段**\n",
    "  - **任务定义**：问题类型（抽取/分类/生成/归纳/推理）。\n",
    "  - **输入要素**：文本/表格/图像描述、上下文、可用外部知识。\n",
    "  - **约束与规则**：格式（如 JSON schema）、边界（不可编造/必须引用）、长度与语气。\n",
    "  - **输出结构**：字段名、类型、取值域、必填/可选。\n",
    "  - **评测口径**：正确性、完整性、格式通过率、引用一致性等。\n",
    "- **通用模板骨架（可迁移）**\n",
    "  - 系统位（原则/安全/风格） → 任务说明 → 约束与输出 schema → 辅助示例（可选）→ 待处理输入。\n",
    "- **代表性模板（示例）**\n",
    "```text\n",
    "[信息抽取·结构化输出]\n",
    "你是一名信息抽取系统。请从输入中抽取{领域}要素，遵循下述 JSON Schema。\n",
    "约束：\n",
    "- 不得编造缺失字段；无法确定时输出 null。\n",
    "- 返回严格 JSON，禁止多余文本。\n",
    "Schema：\n",
    "{schema}\n",
    "输入：\n",
    "{input}\n",
    "输出：\n",
    "```\n",
    "\n",
    "```text\n",
    "[分类判定·多标签]\n",
    "任务：将文本判定为以下标签集合的子集：{labels}。\n",
    "规则：\n",
    "- 定义遵循：{label_definitions}\n",
    "- 若证据不足则输出 []，并给出\"rationale\"说明（50字以内）。\n",
    "输出 JSON：{\"labels\": string[], \"rationale\": string}\n",
    "文本：\n",
    "{input}\n",
    "```\n",
    "\n",
    "```text\n",
    "[摘要与格式遵循]\n",
    "请生成{风格}摘要，满足：\n",
    "- 字数 ≤ {max_tokens}；必须覆盖：{key_points}\n",
    "- 输出 Markdown 列表，每项一句话\n",
    "输入：\n",
    "{input}\n",
    "```\n",
    "\n",
    "### 二、变量化与多样化策略\n",
    "- **变量槽位定义**\n",
    "  - 领域 {domain}、角色 {role}、风格 {style}、长度 {len}、难度 {difficulty}、语言 {lang}、标签集 {labels}、Schema {schema}、示例 {few_shots} 等。\n",
    "- **值域与采样**\n",
    "  - 分层采样（按领域/难度/长度），覆盖表控制每一维的最小占比（如每域≥10%、每难度级≥15%）。\n",
    "  - 组合设计优先“正交覆盖”（尽量少的组合覆盖多维度）。\n",
    "- **多样化维度**\n",
    "  - 语义保持改写（同义、语序、指代、信息焦点变换）。\n",
    "  - 风格切换（客观/摘要/要点式/说明书式）。\n",
    "  - 结构扰动（字段顺序、可选字段缺省），训练鲁棒解析。\n",
    "  - 受控噪声（拼写/口语/混排单位/中英夹杂）与负样本（无关文本）。\n",
    "  - 跨语言与领域迁移（{lang} ∈ zh/en；{domain} 在财经/医药/政务间切换）。\n",
    "  - 复杂度分层（单句→多句→跨段落）。\n",
    "- **生成配方（最小流程）**\n",
    "  1) 准备10–50条高质量“种子样本”。  \n",
    "  2) 设定模板与变量值域，分层采样生成“任务指令”。  \n",
    "  3) 产出候选回答（温度0.2–0.5，多样性受控）。  \n",
    "  4) 进行自动质检+交叉核验，通过者入库；不通过者重写或剔除。\n",
    "\n",
    "### 三、合成数据质检与偏差控制\n",
    "- **自动化结构与格式**\n",
    "  - JSON 严格校验（字段、类型、必填、取值域）。\n",
    "  - 解析失败率阈值（如 ≤ 1%）；超阈即回退并修正模板。\n",
    "- **正确性与一致性**\n",
    "  - 可编程判定：可由规则/脚本核验（如正则、数值范围、枚举）。\n",
    "  - 证据对齐：若需要引用，将回答片段与来源文本做重叠率/相似度校验（阈值如≥0.8）。\n",
    "  - 语义蕴含检查：输入→输出为“蕴含/中立/矛盾”，拒收矛盾样本。\n",
    "- **质量信号与去噪**\n",
    "  - 语言流畅度/长度异常、重复率（n-gram 去重）、困惑度异常；自动降权或剔除。\n",
    "  - 近重复与数据泄漏：基于嵌入相似度去重（阈值如≥0.95 视为重复）。\n",
    "- **安全与隐私**\n",
    "  - 敏感字段屏蔽（PII/密钥/医疗隐私），域内关键词黑白名单。\n",
    "  - 毒性/违规检测通过率≥99%（不过者全量剔除）。\n",
    "- **分布与偏差控制**\n",
    "  - 覆盖表监控：领域/语言/长度/难度/标签分布与目标分布的 KL 差异在阈值内。\n",
    "  - 头部样本限额与长尾补齐；正/负、易/难比例可控（如 6:4）。\n",
    "- **人审抽检**\n",
    "  - 分层抽检率（总体1–5%，新模板首批20%），使用统一评分表（正确性/完整性/遵循/安全性），不合格样本反馈回修。\n",
    "\n",
    "### 四、小样本条件下的增益验证\n",
    "- **实验设计**\n",
    "  - 基线：零样本或少样本（K=10–50）直接训练/推理。\n",
    "  - 处理组：在基线上加入经质检的合成数据（比例如1:1、1:3 对真实）。\n",
    "  - 统一评测集与指标：任务正确率、格式通过率、指令遵循度、引用一致性。\n",
    "- **统计与消融**\n",
    "  - 记录 Δ指标（如正确率+X%、格式失败率−Y%）。\n",
    "  - 消融：去掉多样化、去掉负样本、去掉跨域，看单项贡献。\n",
    "  - 小样本检验：随机种子固定；重复N次取均值±方差，必要时做显著性检验。\n",
    "- **推荐配比（经验）**\n",
    "  - 抽取/分类：真实:合成 ≈ 1:1～1:3；  \n",
    "  - 生成/摘要：真实:合成 ≈ 1:0.5～1:2；  \n",
    "  - 跨域迁移优先保证“结构与格式”的合成比例。\n",
    "\n",
    "### 五、可复用模板与筛选标准\n",
    "- **模板清单（可直接复用）**\n",
    "  - 信息抽取模板（严格 JSON、不可编造）。\n",
    "  - 多标签分类模板（定义+证据化 rationale）。\n",
    "  - 指令摘要模板（风格与长度可控）。\n",
    "  - 步骤化推理模板（逐步列点，禁外部知识）。\n",
    "  - 错误定位与修复模板（生成错误说明与修正建议）。\n",
    "  - 对比评审模板（在A/B回答间做结构化打分）。\n",
    "  - 数据合成器模板（按变量表批量产出种子/难例）。\n",
    "  - 规范化重写模板（把自由文本改写为标准 schema）。\n",
    "- **入库筛选标准（建议阈值）**\n",
    "  - 结构通过率≥99%；  \n",
    "  - 正确性（可编程校验）≥95%；  \n",
    "  - 指令遵循度≥98%；  \n",
    "  - 去重后相似度<0.9；  \n",
    "  - 安全/隐私 0 容忍（命中即剔除）。\n",
    "\n",
    "### 六、最小可行流程（落地清单）\n",
    "- 定义任务卡 → 选模板骨架 → 设变量表与覆盖目标 → 批量合成 → 自动质检 → 分层人审 → 分布与偏差校准 → 小样本训练 → 统一评测与消融 → 归档与版本化（模板/变量/评测）。\n",
    "\n",
    "### 附：示例模板片段\n",
    "```text\n",
    "[数据合成器·变量化生成]\n",
    "目标：生成{domain}领域的信息抽取样本，按{difficulty}难度，语言{lang}。\n",
    "输出：包含 input（原文）、target（严格JSON，遵循{schema}）、meta（domain, difficulty, lang）。\n",
    "约束：不得虚构超出原文的信息；若缺失则置 null。\n",
    "请一次生成 {n} 条，确保不同写作风格与长度分布。\n",
    "```\n",
    "\n",
    "```text\n",
    "[对比评审·质量筛选]\n",
    "任务：比较候选输出 A/B 对输入的符合度，返回评分 JSON：\n",
    "{\"winner\": \"A|B|tie\", \"scores\": {\"correctness\": 1-5, \"completeness\": 1-5, \"format\": 1-5}, \"reason\": \"简述\"}\n",
    "规则：若两者均未满足 schema 或编造事实，则判为 tie 并给出原因。\n",
    "输入：{input}\n",
    "A：{output_a}\n",
    "B：{output_b}\n",
    "```\n",
    "\n",
    "——\n",
    "\n",
    "- 以上方法聚焦“任务结构化→模板可迁移→变量化扩展→严谨质检→小样本增益验证”的闭环，能在有限标注成本下稳定提升特定子任务表现，且便于复制到新领域与新语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee600d1",
   "metadata": {},
   "source": [
    "## A7 对齐算法概览与入门实践：PPO / DPO / GRPO\n",
    "解析三者在信号来源、稳定性与工程代价上的差异：PPO偏在线、信号获取成本高；DPO基于偏好对，路径简洁、易复现；GRPO通过分组奖励提升可控性。完成一个入门级偏好对齐示例，关注偏好数据构造、质量控制与伦理边界。\n",
    "\n",
    "### 1 引言\n",
    "\n",
    "大型语言模型（LLM）的进化速度令人瞩目。然而，如何让这些模型不仅“能说”，更能“会道”，使其回答更符合人类的价值观、偏好和期望，成为了业界的核心挑战。为了实现这一目标，一系列精巧的算法应运而生，它们如同“驯兽师”，引导着模型的行为，使其更好地为人类服务。\n",
    "\n",
    "在众多对齐（Alignment）技术中，强化学习扮演了至关重要的角色。其中，近端策略优化（Proximal Policy Optimization, PPO）、直接偏好优化（Direct Preference Optimization, DPO）以及最近备受关注的组别相对策略优化（Group Relative Policy Optimization, GRPO）是三颗璀璨的明星。它们代表了从“间接”到“直接”，从“个体”到“群体”的不同优化思路，深刻影响着当今大模型的训练范式。\n",
    "\n",
    "### 2 为何需要强化学习与人类反馈？\n",
    "\n",
    "在深入探讨具体算法之前，我们有必要先理解它们所要解决的根本问题：如何让模型变得“更好”？\n",
    "\n",
    "传统的监督微调（Supervised Fine-Tuning, SFT）是训练大模型的第一步。我们用大量高质量的“指令-回答”对来教模型模仿人类的说话方式。然而，SFT有其局限性。首先，高质量的 SFT 数据集构建成本高昂；其次，人类的偏好是复杂且主观的，很难用唯一的“正确答案”来概括。例如，对于“请写一首关于秋天的诗”这个指令，存在无数种优秀的回答，SFT 很难覆盖所有可能性，也无法告诉模型哪种风格的诗更受欢迎。\n",
    "\n",
    "为了解决这个问题，研究者们引入了**从人类反馈中进行强化学习（Reinforcement Learning from Human Feedback, RLHF）**的框架。RLHF 的核心思想是，不再直接告诉模型“正确答案是什么”，而是让模型生成一些回答，然后由人类来评判这些回答的好坏（例如，对多个回答进行排序），再利用这些偏好数据来“奖励”或“惩罚”模型，从而引导其生成更符合人类偏好的内容。\n",
    "\n",
    "这个过程通常分为三个阶段：\n",
    "\n",
    "1. **监督微调 (SFT)**：使用高质量的标注数据对预训练模型进行初步微调，使其适应特定的指令格式。\n",
    "2. **奖励模型 (RM) 训练**：让 SFT 模型对同一个指令生成多个不同的回答。人类标注者对这些回答进行排序，形成偏好数据（例如，回答 A > 回答 B）。然后，用这些偏好数据训练一个奖励模型，该模型能够对任意一个“指令-回答”对打分，分数高低代表了人类的偏好程度。\n",
    "3. **强化学习 (RL) 优化**：将奖励模型作为环境的“裁判”，使用强化学习算法（如 PPO）来微调 SFT 模型。模型（即 RL 中的“智能体”或“策略”）会不断生成回答，奖励模型会为其打分，RL 算法则根据分数来更新模型的参数，目标是最大化奖励模型的总得分。\n",
    "\n",
    "在这个框架中，PPO 长期以来都是第三阶段的绝对主力。然而，随着技术的发展，DPO 和 GRPO 提供了新的、可能更高效的路径。现在，让我们正式进入这三种算法的探索之旅。\n",
    "\n",
    "### 3 近端策略优化（PPO）\n",
    "OpenAI《Proximal Policy Optimization Algorithms》\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/17.png\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "PPO 是 OpenAI 在 2017 年提出的一种强化学习算法，其设计的初衷是为了解决传统策略梯度算法（Policy Gradient）中训练不稳定、更新步长难以确定的问题。在 RLHF 的背景下，PPO 的稳定性和可靠性使其成为优化语言模型的首选。\n",
    "\n",
    "#### 3.1 PPO 的核心直觉\n",
    "\n",
    "想象一下你在教一个孩子玩一个游戏。如果孩子每次尝试后，你都给予他非常剧烈的反馈（要么是极高的赞扬，要么是严厉的批评），他可能会感到困惑和不知所措，甚至放弃学习。一个更好的方法是，在他当前行为的基础上，温和地引导他向更好的方向改进，每次只做小小的调整。\n",
    "\n",
    "PPO 的核心思想与此类似。它认为，在更新模型的策略（即模型生成文本的方式）时，更新的步伐不应该太大。如果新策略与旧策略相差过大，可能会导致模型性能的急剧下降，即“掉下悬崖”。PPO 通过一个巧妙的机制，将策略更新限制在一个“近端”的、可信赖的区域内，从而保证了学习过程的稳定。\n",
    "\n",
    "#### 3.2 PPO 的关键机制：Clipped Surrogate Objective\n",
    "\n",
    "PPO 的魔法在于其目标函数的设计。标准的策略梯度算法的目标是最大化期望回报。PPO 在此基础上引入了两个关键概念：概率比（Probability Ratio）和优势函数（Advantage Function）。\n",
    "\n",
    "- **优势函数**：代表在状态 $s$ 下，采取动作 $a$ 相较于平均水平有多好。在 RLHF 中，它通常由奖励模型（RM）的输出减去一个基线（Baseline，通常是另一个叫做“价值模型”的网络的输出）来估计。如果 $A > 0$，说明这个回答比预期的要好，我们应该增加生成它的概率；反之则减少。\n",
    "- **概率比**：表示新策略 $\\pi_{\\theta}$ 和旧策略 $\\pi_{\\theta_{\\text{old}}}$ 对于同一个动作的输出概率之比。如果 $r_t > 1$，说明新策略更倾向于采取这个动作。\n",
    "\n",
    "PPO 的目标函数（简化版）如下：\n",
    "\n",
    "$$\n",
    "L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t \\right) \\right]\n",
    "$$\n",
    "\n",
    "让我们来解读这个公式：\n",
    "\n",
    "- 我们希望最大化概率比 $r_t$ 和优势函数 $A_t$ 的乘积。当优势 $A_t$ 为正时（好回答），我们希望增大 $r_t$；当优势为负时（坏回答），我们希望减小 $r_t$。这很直观。\n",
    "- 关键在于 clip 函数。clip(x, min, max) 会将 x 限制在 [min, max] 区间内。这里的 $\\epsilon$ 是一个超参数（通常取 0.1 或 0.2），它定义了一个“信任区域”。\n",
    "- min 函数的作用：\n",
    "  - 当优势 $A_t > 0$ 时：我们希望增大 $r_t$，但 clip 函数将其上界限制在 $1 + \\epsilon$。这意味着，即使某个回答非常好，我们对策略的更新也不会过于激进，防止“一步走得太大扯着蛋”。\n",
    "  - 当优势 $A_t < 0$ 时：我们希望减小 $r_t$，clip 函数将其下界限制在 $1 - \\epsilon$。这意味着，我们也不会因为一个糟糕的回答而过度惩罚模型，给了模型“改过自新”的机会。\n",
    "\n",
    "通过这种方式，PPO 像一个温和而有耐心的老师，确保模型在每次学习后都能稳定进步，而不会因为某次剧烈的更新而崩溃。\n",
    "\n",
    "#### 3.3 PPO 在 RLHF 中的角色\n",
    "\n",
    "在 RLHF 流程中，PPO 的工作流程如下：\n",
    "\n",
    "1. **初始化**：用 SFT 模型的权重初始化策略模型（Policy Model），并通常也用它来初始化价值模型（Value Model）。\n",
    "2. **采样**：从一个指令数据集中随机抽取一个指令（Prompt）。\n",
    "3. **生成**：策略模型根据指令生成一个回答。\n",
    "4. **评估**：奖励模型（RM）对“指令-回答”对打分，得到奖励（Reward）。价值模型（Value Model）对指令进行评估，得到价值（Value）。\n",
    "5. **计算优势**：根据奖励和价值计算优势函数。\n",
    "6. **更新**：使用 PPO 的 Clipped Surrogate Objective 计算损失，并更新策略模型和价值模型的参数。\n",
    "7. **循环**：重复步骤 2-6，直到模型收敛。\n",
    "\n",
    "同时，为了防止模型在优化过程中“忘记”SFT 阶段学到的知识，或者为了防止模型生成一些虽然奖励高但内容乱七八糟的文本，PPO 的损失函数中通常还会加入一个 KL 散度惩罚项。这个惩罚项用来衡量当前策略与初始 SFT 策略的差异，差异越大，惩罚越重，确保模型在追求高奖励的同时，不会偏离其原始的语言能力。\n",
    "\n",
    "#### 3.4 PPO 的简单代码示例（概念性）\n",
    "\n",
    "下面的 Python 代码使用 PyTorch 风格展示了 PPO 损失函数的核心计算逻辑，旨在帮助理解，而非一个可直接运行的完整训练脚本。\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def ppo_loss(new_log_probs, old_log_probs, advantages, rewards, values, epsilon=0.2):\n",
    "    \"\"\"\n",
    "    计算 PPO 的核心损失。\n",
    "\n",
    "    参数:\n",
    "    - new_log_probs: 新策略下，采取动作的对数概率\n",
    "    - old_log_probs: 旧策略下，采取动作的对数概率\n",
    "    - advantages: 优势函数值\n",
    "    - rewards: 实际获得的奖励\n",
    "    - values: 价值模型的预测值\n",
    "    - epsilon: PPO clip 的超参数\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 计算概率比 r_t(theta)\n",
    "    # log_probs 是 log(probability)，所以用 exp() 转换回 probability\n",
    "    # r_t = pi_theta(a|s) / pi_theta_old(a|s)\n",
    "    # 在对数空间中，这等价于 exp(log_pi_theta - log_pi_theta_old)\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "    # 2. 计算 PPO 的 policy loss (actor loss)\n",
    "    # Surrogate Objective 1: r_t * A_t\n",
    "    surr1 = ratio * advantages\n",
    "    \n",
    "    # Surrogate Objective 2: clip(r_t, 1-eps, 1+eps) * A_t\n",
    "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "\n",
    "    # PPO 的目标是最大化这个 E[...]，在梯度下降中，我们最小化其相反数\n",
    "    # L_clip = - E[min(surr1, surr2)]\n",
    "    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "    # 3. 计算 value loss (critic loss)\n",
    "    # 通常使用均方误差 (MSE)\n",
    "    # 价值模型的目标是准确预测未来的累积奖励\n",
    "    value_loss = F.mse_loss(rewards, values).mean()\n",
    "    \n",
    "    # 总损失是两者的加权和\n",
    "    # loss = policy_loss + c1 * value_loss - c2 * entropy_bonus\n",
    "    # 这里为了简化，只返回 policy loss 和 value loss\n",
    "    return policy_loss, value_loss\n",
    "\n",
    "# --- 模拟数据 ---\n",
    "# 假设我们有一个 batch 的数据\n",
    "batch_size = 4\n",
    "# 新策略的对数概率\n",
    "new_log_probs = torch.randn(batch_size, requires_grad=True)\n",
    "# 旧策略的对数概率 (在更新前计算并固定)\n",
    "old_log_probs = torch.randn(batch_size)\n",
    "# 优势函数 (从 RM 和 Value Model 计算得到)\n",
    "advantages = torch.randn(batch_size)\n",
    "# 实际回报\n",
    "rewards = torch.randn(batch_size)\n",
    "# 价值模型的预测\n",
    "values = torch.randn(batch_size, requires_grad=True)\n",
    "\n",
    "# 计算损失\n",
    "p_loss, v_loss = ppo_loss(new_log_probs, old_log_probs, advantages, rewards, values)\n",
    "\n",
    "print(f\"Policy Loss: {p_loss.item()}\")\n",
    "print(f\"Value Loss: {v_loss.item()}\")\n",
    "\n",
    "# 在实际训练中，你会计算总损失并调用 loss.backward()\n",
    "```\n",
    "\n",
    "#### 3.5 PPO 的挑战\n",
    "\n",
    "尽管 PPO 非常成功，但 RLHF 中的 PPO 流程相当复杂。它需要同时维护和训练多个模型（策略模型、价值模型、奖励模型、SFT 参考模型），这使得训练过程非常消耗计算资源和内存，且超参数调整也颇具挑战。正是这些挑战，催生了更简洁的替代方案——DPO。\n",
    "\n",
    "### 4 直接偏好优化（DPO）\n",
    "\n",
    "Standford《Direct Preference Optimization: Your Language Model is Secretly a Reward Model》\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/16.png\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "DPO 由斯坦福大学的研究者于 2023 年提出，它以一种惊人的简洁性，对传统的 RLHF 流程发起了挑战。DPO 的核心洞见是：我们完全可以绕过奖励模型建模这一中间步骤，直接利用人类的偏好数据来优化语言模型。\n",
    "\n",
    "#### 4.1 DPO 的核心直觉\n",
    "\n",
    "传统 RLHF 是一个“两步走”的过程：先用偏好数据（A 比 B 好）训练一个能给绝对分数（A 得 90 分，B 得 60 分）的奖励模型，然后再用这个分数去指导强化学习。\n",
    "\n",
    "DPO 的提出者反思道：我们最终的目标不就是让模型知道“A 比 B 好”吗？为什么非要先把它变成“A=90分，B=60分”，再回头去学习这个偏好呢？这个中间的奖励建模步骤不仅复杂，还可能引入误差。我们能不能直接建立一个从“偏好”到“策略更新”的数学桥梁？\n",
    "\n",
    "DPO 做到了这一点。它巧妙地将偏好数据和语言模型的策略联系起来，推导出了一个简单的、可以用分类损失函数直接优化的目标。\n",
    "\n",
    "#### 4.2 DPO ：从偏好到损失\n",
    "\n",
    "DPO 的理论推导略显复杂，但其最终的损失函数却异常优雅。它始于一个名为 Bradley-Terry 的模型，该模型常用于根据成对比较来估计事物的排名。DPO 假设人类的偏好概率 $P(w > l | x)$ 可以用一个潜在的奖励模型 $R$ 来建模：\n",
    "\n",
    "$$\n",
    "P(w > l | x) = \\frac{\\exp(R(w, x))}{\\exp(R(w, x)) + \\exp(R(l, x))}\n",
    "$$\n",
    "\n",
    "这里，$w$ 是被偏好的回答（winner），$l$ 是不被偏好的回答（loser），$x$ 是指令。 $\\beta$ 是一个控制敏感度的常数。这个公式本质上是一个 Sigmoid 函数，表示 $w$ 的奖励比 $l$ 高得越多，人类偏好 $w$ 的概率就越大。\n",
    "\n",
    "接下来是 DPO 最关键的一步。它通过一系列精妙的数学变换，证明了优化语言模型 $\\pi$ 以最大化这个奖励，等价于最小化以下损失函数：\n",
    "\n",
    "$$\n",
    "L(\\pi) = -\\sum_{(x, w, l) \\in D} \\log \\sigma(\\beta (\\log \\pi(w|x) - \\log \\pi(l|x)))\n",
    "$$\n",
    "\n",
    "让我们来解读这个公式：\n",
    "\n",
    "- $D$ 是我们的偏好数据集，包含了大量的 $(x, w, l)$ 三元组。\n",
    "- $\\pi$ 是我们正在训练的模型。\n",
    "- $\\pi_{\\text{ref}}$ 是一个参考模型，通常就是 SFT 阶段得到的模型。它的作用和 PPO 中的 KL 散度惩罚项类似，是为了防止模型“跑偏”。\n",
    "- $\\log \\pi(w|x) - \\log \\pi(l|x)$ 衡量了当前模型相对于参考模型，生成回答 $w$ 的概率增加了多少。我们可以把 $\\log \\pi(w|x) - \\log \\pi(l|x)$ 看作是模型对回答 $w$ 的“隐式奖励”。\n",
    "\n",
    "核心部分是括号里的差值：隐式奖励。DPO 的目标就是最大化这个差值。也就是说，它希望当前模型生成“获胜”回答的概率相对于参考模型的增幅，要远大于生成“失败”回答的概率的增幅。\n",
    "\n",
    "最外层的 $\\log-\\sigma$ (log-sigmoid) 结构，使得这个目标变成了一个标准的二元交叉熵损失。这正是 DPO 的绝妙之处，它将复杂的强化学习问题，转化为了一个我们非常熟悉的分类问题。\n",
    "\n",
    "#### 4.3 DPO 的工作流程\n",
    "\n",
    "相比 PPO，DPO 的流程大大简化：\n",
    "\n",
    "1. **准备数据**：收集偏好数据集，每条数据是 $(\\text{指令}, \\text{胜出回答}, \\text{失败回答})$ 的形式。\n",
    "2. **初始化**：加载 SFT 模型作为训练的初始模型 $\\pi$，并复制一份作为参考模型 $\\pi_{\\text{ref}}$（在训练中其参数被冻结）。\n",
    "3. **训练**：使用上述 DPO 损失函数进行训练。在每个训练步骤中：\n",
    "   - 取一个批次（batch）的偏好数据。\n",
    "   - 分别计算模型 $\\pi$ 和 $\\pi_{\\text{ref}}$ 在指令下，生成 $w$ 和 $l$ 的概率。\n",
    "   - 代入 DPO 损失函数公式，计算损失。\n",
    "   - 反向传播，更新 $\\pi$ 的参数。\n",
    "4. **完成**：训练结束后，$\\pi$ 就是对齐好的模型。\n",
    "\n",
    "整个过程不需要训练一个独立的奖励模型，也不需要复杂的采样和优势计算，更没有价值模型。这使得 DPO 在实现上更简单，训练更稳定，也更节省计算资源。\n",
    "\n",
    "#### 4.4 DPO 的简单代码示例\n",
    "\n",
    "Hugging Face 的 trl 库提供了 DPOTrainer，使得 DPO 的实现变得非常简单。以下是一个概念性的代码片段，展示了其核心逻辑。\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.nn.functional import log_sigmoid\n",
    "\n",
    "def dpo_loss(policy_chosen_logps, policy_rejected_logps,\n",
    "             reference_chosen_logps, reference_rejected_logps,\n",
    "             beta):\n",
    "    \"\"\"\n",
    "    计算 DPO 损失。\n",
    "\n",
    "    参数:\n",
    "    - policy_chosen_logps: 策略模型对胜出回答的对数概率\n",
    "    - policy_rejected_logps: 策略模型对失败回答的对数概率\n",
    "    - reference_chosen_logps: 参考模型对胜出回答的对数概率\n",
    "    - reference_rejected_logps: 参考模型对失败回答的对数概率\n",
    "    - beta: DPO 的温度超参数\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 计算策略模型和参考模型的“隐式奖励”\n",
    "    # pi_logratios = log(pi_policy / pi_ref) = log(pi_policy) - log(pi_ref)\n",
    "    pi_logratios_chosen = policy_chosen_logps - reference_chosen_logps\n",
    "    pi_logratios_rejected = policy_rejected_logps - reference_rejected_logps\n",
    "    \n",
    "    # 2. 计算 DPO 损失函数的核心部分\n",
    "    # DPO 的目标是最大化 (pi_logratios_chosen - pi_logratios_rejected)\n",
    "    logits = pi_logratios_chosen - pi_logratios_rejected\n",
    "    \n",
    "    # 3. 应用 log-sigmoid 形式的二元交叉熵损失\n",
    "    # 我们希望 logits 尽可能大，等价于最小化 -log_sigmoid(logits)\n",
    "    loss = -log_sigmoid(beta * logits).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# --- 模拟数据 ---\n",
    "batch_size = 4\n",
    "# 假设我们已经从模型中获取了这些对数概率\n",
    "policy_chosen_logps = torch.randn(batch_size, requires_grad=True)\n",
    "policy_rejected_logps = torch.randn(batch_size, requires_grad=True)\n",
    "reference_chosen_logps = torch.randn(batch_size)\n",
    "reference_rejected_logps = torch.randn(batch_size)\n",
    "beta = 0.1\n",
    "\n",
    "# 计算损失\n",
    "loss = dpo_loss(policy_chosen_logps, policy_rejected_logps,\n",
    "                reference_chosen_logps, reference_rejected_logps,\n",
    "                beta)\n",
    "\n",
    "print(f\"DPO Loss: {loss.item()}\")\n",
    "\n",
    "# 实际使用中，DPOTrainer 会帮你处理好这一切\n",
    "```\n",
    "\n",
    "#### 4.5 DPO 的权衡\n",
    "\n",
    "DPO 虽然简洁高效，但也有其适用场景和潜在的局限。DPO 强依赖于高质量的成对偏好数据。如果偏好数据的质量不高，或者标注不一致，DPO 的效果可能会受到影响。此外，由于其直接优化的特性，它对于数据分布的变化可能比 PPO 更敏感。在某些需要精细控制奖励函数的复杂场景下，PPO 的灵活性可能依然具有优势。\n",
    "\n",
    "### 5 组别相对策略优化（GRPO）\n",
    "DeepSeek-AI《DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models》\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"./image/15.png\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "就在 PPO 和 DPO 的讨论如火如荼之时，DeepSeek-AI 在其模型（如 DeepSeekMath 和 DeepSeek-R1）的训练中，提出并使用了一种名为 GRPO 的新方法，为 RLHF 带来了新的视角。GRPO 可以看作是 PPO 的一个变种，它通过一种新颖的方式来估计优势函数，从而省去了 PPO 中昂贵的价值模型（Critic Model）。\n",
    "\n",
    "#### 5.1 GRPO 的核心直觉\n",
    "\n",
    "PPO 的核心是优势函数 $A_t$，它需要一个奖励模型 $R$ 来提供奖励，还需要一个价值模型 $V$ 来提供基线（Baseline），即在状态 $s$ 下的平均期望回报。训练和维护这个价值模型是 PPO 流程中主要的复杂性和成本来源之一。\n",
    "\n",
    "GRPO 的提出者思考：我们能不能找到一种更简单的方式来估计这个“平均水平”呢？\n",
    "\n",
    "他们的答案是：利用群体智慧。对于同一个指令，我们不只生成一个回答，而是生成一组（Group）回答。然后，我们假设这组回答的平均奖励，就可以近似地作为当前策略下的“平均水平”，也就是价值 $V(s)$ 的一个估计。\n",
    "\n",
    "这个想法非常直观。想象一下，要评价一个学生这次考试的成绩（某个回答的奖励）是好是坏（优势），我们不需要知道他历史上的平均分（价值模型的输出），我们可以直接看他这次在班级里（一组回答中）的排名。如果他的分数远高于班级平均分，那么他的优势就是正的，反之亦然。\n",
    "\n",
    "#### 5.2 GRPO 的关键机制：组内优势估计\n",
    "\n",
    "GRPO 的核心是对 PPO 中优势函数的计算方式进行了修改。其步骤如下：\n",
    "\n",
    "- **组采样 (Group Sampling)**：对于一个给定的指令 $x$，使用当前的策略模型 $\\pi$ 生成一个包含 $N$ 个回答的组 $\\{a_1, a_2, \\ldots, a_N\\}$。\n",
    "- **组评估 (Group Evaluation)**：使用一个奖励函数（可以是一个训练好的奖励模型，也可以是某种可计算的启发式规则，例如代码的执行结果、数学题的答案是否正确等）为组内的每一个回答 $a_i$ 打分，得到奖励 $R(a_i, x)$。\n",
    "- **组内优势计算 (Group-Relative Advantage Estimation)**：计算组内所有回答的平均奖励 $\\bar{R}$ 和标准差 $\\sigma_R$。对于组内的每一个回答 $a_i$，其优势被定义为其归一化后的奖励：\n",
    "\n",
    "$$\n",
    "A(a_i, x) = \\frac{R(a_i, x) - \\bar{R}}{\\sigma_R}\n",
    "$$\n",
    "\n",
    "这种方法被称为组内奖励归一化（Group-wise Reward Normalization）。它直接用组内的统计量（均值和标准差）来替代了 PPO 中需要专门训练的价值模型所扮演的角色。\n",
    "\n",
    "- **策略更新**：一旦计算出了每个样本的优势 $A(a_i, x)$，接下来的步骤就和 PPO 非常相似了。GRPO 同样使用 Clipped Surrogate Objective 来更新策略模型。\n",
    "\n",
    "$$\n",
    "L^{\\text{GRPO}}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t \\right) \\right]\n",
    "$$\n",
    "\n",
    "其中 $r_t$ 是概率比，$A_t$ 是上面计算出的组内相对优势。\n",
    "\n",
    "#### 5.3 GRPO 的优势与特点\n",
    "\n",
    "- **高效性**：GRPO 最显著的优势是无需价值模型。价值模型通常和策略模型一样大，去掉它可以节省近一半的训练内存和计算量，这对于训练超大规模模型来说意义重大。\n",
    "- **灵活性**：GRPO 对奖励函数的定义非常灵活。它不一定需要一个端到端训练的神经网络奖励模型。在某些任务中（如代码生成、数学推理），我们可以设计出可验证的奖励函数（Verifiable Reward Functions）。例如，如果生成的代码能成功运行并通过所有单元测试，就给予高奖励；如果数学题的最终答案正确，也给予高奖励。这种方式使得奖励信号更客观、更廉价。\n",
    "- **稳定性**：通过组内归一化，GRPO 使得优势函数的尺度保持在一个稳定的范围内，这有助于稳定训练过程，减少了对超参数的敏感性。\n",
    "\n",
    "#### 5.4 GRPO 的简单代码示例（概念性）\n",
    "\n",
    "下面的代码片段展示了 GRPO 中计算组内相对优势的核心逻辑。\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def grpo_advantage_estimation(rewards_in_group):\n",
    "    \"\"\"\n",
    "    计算 GRPO 的组内相对优势。\n",
    "\n",
    "    参数:\n",
    "    - rewards_in_group: 一个组内所有样本的奖励，形状为 [group_size]\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 计算组内奖励的均值和标准差\n",
    "    mean_reward = rewards_in_group.mean()\n",
    "    # 添加一个小的 epsilon 防止除以零\n",
    "    std_reward = rewards_in_group.std() + 1e-8\n",
    "    \n",
    "    # 2. 计算归一化后的优势\n",
    "    advantages = (rewards_in_group - mean_reward) / std_reward\n",
    "    \n",
    "    return advantages\n",
    "\n",
    "# --- 模拟数据 ---\n",
    "# 假设对于一个 prompt，我们生成了 8 个回答 (group_size=8)\n",
    "group_size = 8\n",
    "# 奖励模型为这 8 个回答打分\n",
    "rewards = torch.tensor([10.0, 5.0, 12.0, 8.0, 15.0, 9.0, 7.0, 11.0])\n",
    "\n",
    "# 计算 GRPO 优势\n",
    "advantages = grpo_advantage_estimation(rewards)\n",
    "\n",
    "print(\"Rewards in group:\", rewards)\n",
    "print(\"GRPO Advantages:\", advantages)\n",
    "\n",
    "# 得到的 advantages 随后会像 PPO 一样，被用于计算 clipped surrogate loss\n",
    "# 例如，对于奖励为 15.0 的那个样本，它的优势是最高的 (正值)\n",
    "# 对于奖励为 5.0 的那个样本，它的优势是最低的 (负值)\n",
    "# 优势的均值近似为 0\n",
    "print(\"Mean of advantages:\", advantages.mean().item()) \n",
    "```\n",
    "\n",
    "#### 5.5 GRPO 的适用场景\n",
    "\n",
    "GRPO 特别适用于以下场景：\n",
    "\n",
    "- **计算资源受限**：当你希望以更低的成本进行 RLHF 训练时，GRPO 是一个极具吸引力的选择。\n",
    "- **存在客观评价标准**：在代码、数学、科学等领域，可以通过程序化、确定性的方式来评估生成内容的质量，GRPO 可以充分利用这种廉价而准确的奖励信号。\n",
    "- **需要提升模型推理能力**：DeepSeek 的实践表明，通过精心设计的奖励函数和 GRPO 训练，可以显著提升模型在复杂推理任务上的表现。\n",
    "\n",
    "### 6 PPO、DPO 与 GRPO 的全方位对比\n",
    "\n",
    "经过前面的详细介绍，我们现在可以从多个维度对这三种算法进行一个全面的比较，以帮助您在实际应用中做出更合适的选择。\n",
    "\n",
    "| 特性维度 | PPO (Proximal Policy Optimization) | DPO (Direct Preference Optimization) | GRPO (Group Relative Policy Optimization) |\n",
    "|----------|-----------------------------------|-------------------------------------|------------------------------------------|\n",
    "| 核心思想 | 在可信区域内小步更新策略，以最大化奖励模型给出的分数。 | 直接将成对的偏好数据转化为一个分类损失，绕过奖励建模。 | 通过组内回答的相对好坏来估计优势，从而指导策略更新。 |\n",
    "| 方法论   | On-policy 强化学习 | Off-policy 偏好学习 (类似于分类) | On-policy 强化学习 |\n",
    "| 所需模型 | 4个：策略模型、价值模型 (Critic)、奖励模型 (RM)、SFT 参考模型。 | 2个：策略模型、SFT 参考模型。 | 3个 (通常)：策略模型、奖励函数/模型、SFT 参考模型。无价值模型。 |\n",
    "| 数据需求 | 需要奖励模型能给出的绝对分数。 | 需要成对的偏好数据 。 | 需要奖励函数/模型能给出的分数 (可以是相对的)。 |\n",
    "| 计算成本 | 最高。需要维护和训练多个大型模型，且有复杂的采样循环。 | 最低。流程类似监督微调，非常简洁高效。 | 中等。比 PPO 成本低（省去了价值模型），但比 DPO 复杂（仍有 RL 循环）。 |\n",
    "| 主要优势 | 灵活、鲁棒。适用于各种复杂的奖励函数，是久经考验的工业标准。 | 简洁、高效。训练稳定，实现简单，大大降低了 RLHF 的门槛。 | 高效、灵活。显著降低了 PPO 的成本，且能灵活利用可验证奖励。 |\n",
    "| 主要挑战 | 复杂、昂贵。实现和调试难度大，对资源消耗极高。 | 依赖数据质量。对偏好数据的一致性和质量要求高。 | 组采样开销。需要为每个指令生成多个样本，会增加推理开销。 |\n",
    "| 适用场景 | 需要精细控制奖励函数、追求极限性能的通用场景。 | 数据集是成对偏好形式，希望快速、低成本地进行模型对齐的场景。 | 计算资源受限，或任务存在客观、可程序化验证的奖励标准的场景 (如代码、数学)。 |\n",
    "| **训练流程** | 需要两个阶段：先训练 Reward Model，再进行策略对齐。 | 只有一个训练流程，不依赖 Reward Model。 | 需要两个阶段：先训练 Reward Model，再进行策略对齐。 |\n",
    "| **显卡资源需求** | 高：需要加载策略模型、价值模型、奖励模型、参考模型。 | 低：需要加载策略模型和可选的参考模型。 | 中：需要加载策略模型、奖励模型、参考模型。 |\n",
    "| **对样本依赖** | 对样本标注的精度和数据量依赖相对较小，因 Reward Model 提供泛化。 | 对样本标注的精度和数据量依赖较大，因缺少 Reward Model 的泛化。 | 对样本标注的精度和数据量依赖相对较小，因 Reward Model 提供泛化。 |\n",
    "| **灵活扩展性** | 高：可以通过多个 Reward Model 进行扩展。 | 低：需要重新构建数据，灵活性和可扩展性较差。 | 高：可以通过多个 Reward Model 进行扩展。 |\n",
    "| **优点** | 灵活、鲁棒，适用于复杂的奖励函数。 | 训练流程短，资源要求低，稳定性高，训练难度低。 | 高效、灵活，显著降低了训练成本。 |\n",
    "| **缺点** | 复杂、昂贵，调试难度大。 | 容易过拟合，需求更大标注数据量，多任务适配较难。 | 组采样开销大，推理开销增加。 |\n",
    "\n",
    "#### 6.1 详细分析\n",
    "\n",
    "##### 1）PPO (Proximal Policy Optimization)\n",
    "- **优点**: 灵活性和鲁棒性强，适用于各种复杂的奖励函数，是久经考验的工业标准。\n",
    "- **缺点**: 训练过程复杂且昂贵，需要维护多个大型模型，调试难度大。\n",
    "- **适用场景**: 需要精细控制奖励函数、追求极限性能的通用场景。\n",
    "\n",
    "##### 2）DPO (Direct Preference Optimization)\n",
    "- **优点**: 训练流程短，不需要 Reward Model，资源要求低，稳定性高，训练难度低。\n",
    "- **缺点**: 容易过拟合，依赖高质量的偏好数据，适应多任务较难。\n",
    "- **适用场景**: 数据集是成对偏好形式，希望快速、低成本地进行模型对齐的场景。\n",
    "\n",
    "##### 3）GRPO (Group Relative Policy Optimization)\n",
    "- **优点**: 高效、灵活，显著降低了训练成本，能灵活利用可验证奖励。\n",
    "- **缺点**: 组采样开销大，需要为每个指令生成多个样本，推理开销增加。\n",
    "- **适用场景**: 计算资源受限，或任务存在客观、可程序化验证的奖励标准的场景 (如代码、数学)。\n",
    "\n",
    "通过以上对比，可以更清晰地理解这三种方法的优缺点及其适用场景，从而在实际应用中做出更合适的选择。\n",
    "\n",
    "#### 6.2 一个生动的比喻\n",
    "\n",
    "- **PPO** 像是一位经验丰富的全科医生。他会给你做全面的检查（奖励模型评估），参考你的历史病历（价值模型），然后非常谨慎地给你开药方（Clipped Update），确保疗效的同时将副作用降到最低。这个过程非常完备，但也最耗时耗力。\n",
    "- **DPO** 像是一位专攻“比对诊断”的专家。你不需要告诉他你的具体指标，你只需要告诉他“相比于昨天，我今天感觉更好了”（偏好数据）。他就能直接根据这些“比对”信息，调整你的治疗方案。这个过程非常直接，省去了很多中间化验环节。\n",
    "- **GRPO** 像是一位组织“专家会诊”的医生。他把你的情况（指令）告诉一群实习医生（生成一组回答），让他们各自给出诊断方案和信心度（奖励）。然后他根据这些方案在“会诊”中的相对好坏（组内归一化），来决定最终采纳哪个方向的治疗思路。他自己不需要对你的历史情况了如指掌（无需价值模型），而是依赖“集体智慧”做决策。\n",
    "\n",
    "### 7 总结与展望\n",
    "\n",
    "从 PPO 的稳定可靠，到 DPO 的简洁直接，再到 GRPO 的高效灵活，我们看到了大模型对齐技术在“效果”、“效率”和“成本”这个不可能三角中的不断探索与演进。\n",
    "\n",
    "- **PPO** 作为 RLHF 的开创性和基准性方法，其地位在短期内难以被完全撼动。它强大的灵活性和鲁棒性使其在许多前沿研究和工业应用中依然是首选。\n",
    "- **DPO** 则成功地为 RLHF “祛魅”，它证明了在许多场景下，我们并不需要复杂的强化学习框架，一个巧妙的损失函数设计就能达到甚至超越 PPO 的效果。它的出现极大地推动了 RLHF 技术的普及和民主化。\n",
    "- **GRPO** 则在 PPO 的框架内进行了精妙的“减负”，它在保持 PPO 核心优势的同时，显著降低了训练成本，并为利用非传统奖励信号（如可验证奖励）开辟了新的道路，尤其在逻辑推理等领域展现出巨大潜力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6485ae",
   "metadata": {},
   "source": [
    "\n",
    "### 2 入门级偏好对齐示例，关注偏好数据构造、质量控制与伦理边界\n",
    "\n",
    "`hjh0119/shareAI-Llama3-DPO-zh-en-emoji`\n",
    "\n",
    "精心选出了一些源于知乎、逻辑推理、弱智吧的问题作为Query，\n",
    "使用llama3 70b instruct版本采样生成。\n",
    "\n",
    "对每个query生成一个中文版本的answer和一个英文版本的answer，\n",
    "用于在llama3 8b instruct上进行语言偏好的学习。\n",
    "\n",
    "发现非常有用，整个对齐过程只需要1h左右的训练，实测性能表现却超过了大部分通过中文对话SFT后的llama3中文版本\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "# 下载数据集\n",
    "git lfs install\n",
    "git clone https://www.modelscope.cn/datasets/hjh0119/shareAI-Llama3-DPO-zh-en-emoji.git\n",
    "\n",
    "# 为了实际演示，使用脚本将该开元数据集转化为ms-swift标准输入输出\n",
    "# 形如：\n",
    "# {\"messages\": [{\"role\": \"system\", \"content\": \"你是个有用无害的助手\"}, {\"role\": \"user\", \"content\": \"告诉我明天的天气\"}, {\"role\": \"assistant\", \"content\": \"明天天气晴朗\"}], \"rejected_response\": \"我不知道\"}\n",
    "# {\"messages\": [{\"role\": \"system\", \"content\": \"你是个有用无害的数学计算器\"}, {\"role\": \"user\", \"content\": \"1+1等于几\"}, {\"role\": \"assistant\", \"content\": \"等于2\"}, {\"role\": \"user\", \"content\": \"再加1呢\"}, {\"role\": \"assistant\", \"content\": \"等于3\"}], \"rejected_response\": \"我不知道\"}\n",
    "python3 ./scripts/convert_to_jsonl.py \\\n",
    "-i ./data/shareAI-Llama3-DPO-zh-en-emoji/merged_dpo_zh_emoji.jsonl \\\n",
    "-o ./data/shareAI-Llama3-DPO-zh-en-emoji/merged_dpo_zh_emoji.converted.jsonl \\\n",
    "--user-field question \\\n",
    "--chosen-field answer_zh \\\n",
    "--reject-field answer_en\n",
    "\n",
    "# 执行训练\n",
    "# 24GiB 1h\n",
    "# It is recommended to use padding_free. For more details, please refer to:\n",
    "# https://github.com/modelscope/ms-swift/blob/main/examples/train/padding_free/dpo.sh\n",
    "CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift rlhf \\\n",
    "    --rlhf_type dpo \\\n",
    "    --model ./models/Qwen2.5-7B-Instruct \\\n",
    "    --train_type lora \\\n",
    "    --dataset data/shareAI-Llama3-DPO-zh-en-emoji/merged_dpo_zh_emoji.converted.jsonl \\\n",
    "    --split_dataset_ratio 0.01 \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 32 \\\n",
    "    --target_modules all-linear \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --eval_steps 50 \\\n",
    "    --save_steps 50 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --max_length 2048 \\\n",
    "    --output_dir output \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --rpo_alpha 0.1 \\\n",
    "    --dataset_num_proc 4\n",
    "```\n",
    "\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"output/v6-20250914-221953/images/eval_logps_chosen.png\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"output/v6-20250914-221953/images/eval_logps_rejected.png\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "<p align = \"center\">    \n",
    "<img src=\"output/v6-20250914-221953/images/eval_loss.png\"  width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift infer \\\n",
    "    --adapters output/v6-20250914-221953/checkpoint-152 \\\n",
    "    --stream true \\\n",
    "    --temperature 0 \\\n",
    "    --max_new_tokens 2048\n",
    "```\n",
    "\n",
    "\n",
    "#### 简要分析\n",
    "- **eval_loss（总体目标）**: 由 0.1325 降至 0.1233，并在后期趋稳，表示整体收敛变好。\n",
    "- **eval_nll_loss（对参考答案拟合）**: 由 1.325 降至 1.233，说明对“正确答案”的概率分配更高。\n",
    "- **accuracies（判别正确率）**: 始终为 1.0（验证集全部判断正确）。\n",
    "- **rewards 与 margin（分离度）**: chosen 持续较高、rejected 较低；margin 在中期短暂回落（~21），后期回升并稳定在 ~36，表明优劣答案区分度恢复并保持良好。\n",
    "\n",
    "结论：模型已良好收敛，后期表现稳定且优于早期，既能稳定判别偏好，又提升了对参考答案的拟合度。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3af9a4",
   "metadata": {},
   "source": [
    "## A9 评估与上线：指标体系、灰度与回滚\n",
    "构建任务正确率、遵循度、毒性与稳定性等指标体系，确定A/B与灰度发布流程及回滚触发条件；形成上线监控、异常处置与复盘机制的要点说明，确保从实验到生产的可追溯与可管控。\n",
    "\n",
    "### 一、指标体系（评什么、怎么评）\n",
    "- **任务正确率（Task Accuracy）**：回答是否正确、完整、可复现。\n",
    "  - 常用口径：精确匹配（Exact Match）、F1、基于证据的正确率（回答必须引用到的文档片段）、专家评分（Likert 1–5）。\n",
    "  - 面向开放回答：用“评测模型”打分（多维度：正确性、覆盖度、逻辑性），配合人审抽检校准偏差。\n",
    "- **指令遵循度（Instruction Following）**：回答是否严格满足约束与格式。\n",
    "  - 口径：格式遵循率（JSON/表格/字段齐全）、约束满足率（字数、语气、边界）、拒答合理率（该拒绝时能拒绝且给出理由）。\n",
    "- **毒性与合规（Safety & Compliance）**：是否出现有害、有毒或违规内容。\n",
    "  - 口径：毒性率（Toxicity Rate）、越权输出率（隐私、医疗/法律不当建议）、越狱率（被提示工程绕过防护）。\n",
    "  - 分层防护：前/后置安全审查模型 + 关键词/正则/规则库。\n",
    "- **稳定性与性能（Reliability & Performance）**：\n",
    "  - 时延：TTFT（首 token 延迟）、p50/p95/p99 响应时间；吞吐（RPS/QPS）。\n",
    "  - 稳定：超时率、错误率（5xx/4xx）、重试率、输出格式失败率。\n",
    "  - 变异：输出波动（多次生成一致性）、冷启动抖动。\n",
    "- **成本与运维（Cost & Ops）**：\n",
    "  - 每请求成本、每千 token 成本、缓存命中率、检索调用次数、第三方 API 费用。\n",
    "- **用户与业务（UX & Business）**：\n",
    "  - 用户满意度（CSAT/NPS）、人工复核占比与纠错成本、留存与转化。\n",
    "- 指标分层建议：\n",
    "  - **离线基准**（固定测试集，保证可复现）\n",
    "  - **在线护栏**（实时监控：毒性、错误、时延、成本）\n",
    "  - **人审抽检**（高优先级场景按日/周抽检，校准评测模型偏差）\n",
    "\n",
    "术语小贴士：\n",
    "- p95/p99：95%/99% 请求的时延不超过该值（衡量“尾部时延”）。\n",
    "- TTFT：Time To First Token，用户感知“开口速度”。\n",
    "- 评测模型：用强模型对回答多维打分，成本低于全量人审。\n",
    "\n",
    "### 二、A/B 测试与灰度发布（如何把风险降到最低）\n",
    "- **A/B 测试（对照实验）**：将流量随机分为两组比较新旧方案差异，确保除了“模型版本”外其他条件一致。\n",
    "  - 设计要点：\n",
    "    - 明确主指标（如任务正确率↑）与护栏指标（毒性率、时延、成本不得恶化）。\n",
    "    - 样本量计算（保证统计功效），随机化单元（用户/会话），固定分桶（避免跨天穿桶）。\n",
    "    - 实验时长覆盖业务周期（避免节假日/活动干扰）。\n",
    "  - 分析要点：显著性（p 值/置信区间）、效应量（Cohen’s d）、提前停止规则（护栏触发即停）。\n",
    "- **灰度发布（又称金丝雀发布/Canary）**：小流量逐步放大，动态健康检查。\n",
    "  - 典型节奏：影子测试（0%用户可见）→ 1% → 5% → 20% → 50% → 100%。\n",
    "  - 影子测试（Shadow）：“新模型并行生成但不回显”，离线对比质量/时延/成本，先发现问题再放量。\n",
    "  - 自动化判定：每个阶段设定放量阈值（主指标提升≥X%，护栏不恶化），否则停留或回滚。\n",
    "\n",
    "术语小贴士：\n",
    "- 护栏指标（Guardrails）：出现恶化即“叫停/回滚”的红线指标（如毒性、合规、成本上限）。\n",
    "- 固定分桶：同一用户在实验期内始终落在同一版本，消除个体差异干扰。\n",
    "\n",
    "### 三、回滚触发条件（何时立即撤回）\n",
    "- **安全/合规类**：毒性/越权/越狱率超过阈值；监管投诉或高风险样例出现。\n",
    "- **质量类**：主任务正确率显著下降；关键格式失败率上升（导致下游不可解析）。\n",
    "- **性能类**：p95/p99 时延、超时率、错误率突增；TTFT恶化明显。\n",
    "- **成本类**：单位成本越线；外部 API 费率异常上涨。\n",
    "- **运营类**：用户投诉激增、业务 KPI 下滑（转化、留存）。\n",
    "- 处置动作（自动化优先）：立即将流量切回稳定版本；切换旧提示/旧路由；降低并发、启用缓存/降级；冻结新增发布。\n",
    "\n",
    "### 四、上线监控与告警（看什么、怎么报）\n",
    "- **实时观测维度**：\n",
    "  - 请求级：QPS、TTFT、p95、错误/超时、重试、缓存命中。\n",
    "  - 质量近似：格式失败率、语义一致性/自洽率（多次生成一致）、引用一致性（RAG 回答是否引用来源）。\n",
    "  - 安全：毒性事件、拒答合理率、可疑越狱提示命中。\n",
    "  - 成本：每次请求成本、token 用量、外部 API 次数与单价。\n",
    "- **告警与阈值**：\n",
    "  - 严重级别分级（P0/P1/P2），分级通知（电话/IM/邮件）。\n",
    "  - 动态基线 + 异常检测（相对历史均值波动）。\n",
    "- **日志与隐私**：\n",
    "  - 结构化日志字段：请求ID、用户/会话ID（匿名化）、提示版本、模型版本、输入/输出摘要、引用来源、时延/成本、决策路由。\n",
    "  - 脱敏与采样：敏感信息屏蔽，生产只保留必要字段与可追溯链路。\n",
    "\n",
    "术语小贴士：\n",
    "- 语义一致性/自洽：同一问题多次生成是否相似（衡量稳定性）。\n",
    "- 引用一致性：回答声称的事实能否在知识库检索到对应证据。\n",
    "\n",
    "### 五、异常处置机制（Runbook）\n",
    "- **分级与责任**：设定值班与升级链路（谁在 15 分钟内响应、30 分钟内决策）。\n",
    "- **标准流程**：\n",
    "  1) 确认告警 → 2) 准实时止血（回滚/降级/限流/熔断） → 3) 影响评估与对外沟通 → 4) 根因分析与修复 → 5) 恢复与观察。\n",
    "- **常用技术手段**：\n",
    "  - Kill switch（全局开关）、特性开关（Feature Flag）、流量路由（老新模型并存）。\n",
    "  - 降级：关闭复杂工具/检索、降低采样/思考步数、启用缓存。\n",
    "  - 防扩散：限流、隔离问题租户/场景；动态屏蔽高风险提示模板。\n",
    "- **演练**：季度故障演习，验证回滚时延、告警有效性与人员协同。\n",
    "\n",
    "### 六、复盘与持续改进（Postmortem）\n",
    "- **无责复盘**：记录时间线、影响面、根因（5 Whys）、侥幸因素。\n",
    "- **行动项**：清晰责任人/截止日期/验收标准；新增回归测试与监控规则。\n",
    "- **知识沉淀**：更新“评测集/风险样例库/提示模板”，把故障转化为可重复监测与测试的资产。\n",
    "\n",
    "### 七、落地清单与基线 SLO 示例\n",
    "- **发布前检查清单**：\n",
    "  - 离线基准：正确率↑且毒性/成本/时延不劣于基线；关键场景通过人审。\n",
    "  - 影子测试：≥1 周业务周期，质量/时延/成本无异常。\n",
    "  - A/B 方案：样本量、随机化、指标/护栏、实验时长、停止规则均已配置。\n",
    "  - 回滚预案：一键回滚、演练通过；告警阈值与分级触发已验证。\n",
    "- **SLO（服务目标）示例**：\n",
    "  - 可用性≥99.9%；TTFT p95 ≤ 600ms，响应 p95 ≤ 2.5s。\n",
    "  - 任务正确率 ≥ 基线 +2%；格式失败率 ≤ 0.5%。\n",
    "  - 毒性率 ≤ 0.05%；越狱率 ≤ 0.1%（持续下降趋势）。\n",
    "  - 单次请求成本 ≤ 目标上限（按日/周平均监控）。\n",
    "- **自动化护栏（样例）**：\n",
    "  - 若毒性率 10 分钟滑窗 > 阈值 → 立即回滚并通知 P0。\n",
    "  - 若 p95 时延 > 阈值且错误率上升 → 自动降级与限流。\n",
    "  - 若成本/千 token 超上限 → 切换至备选路由或启用缓存策略。\n",
    "\n",
    "术语小贴士：\n",
    "- SLI/SLI/SLA：SLI 指标、SLO 目标、SLA 对外承诺（合同级）。生产优先达成 SLO，再根据业务制定 SLA。\n",
    "\n",
    "——\n",
    "\n",
    "- 关键要点\n",
    "  - 指标成体系：正确率、遵循度、安全、稳定性、成本、业务六维联动。\n",
    "  - 上线走两步：先影子后灰度；A/B 有护栏与停止规则。\n",
    "  - 监控有抓手：实时仪表盘 + 自动化告警 + 一键回滚。\n",
    "  - 闭环要完备：处置—复盘—固化到评测集与监控策略。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
