<svg xmlns="http://www.w3.org/2000/svg" width="1280" height="860" viewBox="0 0 1280 860" role="img" aria-labelledby="title desc">
  <title id="title">DeepSeek-R1 Development Pipeline</title>
  <desc id="desc">A high-level overview of DeepSeek-R1-Zero, DeepSeek-R1, and DeepSeek-R1-Distill: data inputs, reinforcement learning stages, reward signals, and outputs.</desc>
  <defs>
    <linearGradient id="bgGrad" x1="0" y1="0" x2="1" y2="1">
      <stop offset="0%" stop-color="#F7FAFC"/>
      <stop offset="100%" stop-color="#EDF2F7"/>
    </linearGradient>
    <linearGradient id="panelGrad" x1="0" y1="0" x2="0" y2="1">
      <stop offset="0%" stop-color="#FFFFFF"/>
      <stop offset="100%" stop-color="#F1F5F9"/>
    </linearGradient>
    <filter id="shadow" x="-20%" y="-20%" width="140%" height="140%">
      <feGaussianBlur in="SourceAlpha" stdDeviation="4"/>
      <feOffset dx="0" dy="2" result="offsetblur"/>
      <feComponentTransfer>
        <feFuncA type="linear" slope="0.25"/>
      </feComponentTransfer>
      <feMerge>
        <feMergeNode/>
        <feMergeNode in="SourceGraphic"/>
      </feMerge>
    </filter>
    <marker id="arrow" viewBox="0 0 10 10" refX="10" refY="5" markerWidth="8" markerHeight="8" orient="auto-start-reverse">
      <path d="M 0 0 L 10 5 L 0 10 z" fill="#475569" />
    </marker>
    <style>
      .title { font: 700 26px/1.2 system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial; fill: #0f172a; }
      .subtitle { font: 600 18px/1.2 system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial; fill: #334155; }
      .label { font: 600 14px/1.2 system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial; fill: #0f172a; }
      .body { font: 400 13px/1.5 system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial; fill: #334155; }
      .chip { font: 700 12px/1.2 ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; fill: #0f172a; }
      .note { font: 500 12px/1.2 system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial; fill: #475569; }
      .em { font: 700 13px/1.2 system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial; fill: #0f172a; }
      .kpi { font: 800 15px/1.2 system-ui, -apple-system; fill: #065f46; }
    </style>
  </defs>

  <rect x="0" y="0" width="1280" height="860" fill="url(#bgGrad)"/>

  <!-- Header -->
  <g transform="translate(40,40)">
    <text class="title">DeepSeek-R1 Development Pipeline</text>
    <text class="note" y="28">From Zero → Cold Start Reinforcement → Distillation to Smaller Models</text>
  </g>

  <!-- Legend / chips -->
  <g transform="translate(40,90)">
    <rect width="1200" height="60" rx="12" fill="url(#panelGrad)" filter="url(#shadow)"/>
    <g transform="translate(20,20)">
      <rect x="0" y="-12" width="130" height="24" rx="6" fill="#e2e8f0"/>
      <text x="10" y="5" class="chip">INPUT</text>
      <rect x="150" y="-12" width="160" height="24" rx="6" fill="#d1fae5"/>
      <text x="160" y="5" class="chip">REINFORCEMENT</text>
      <rect x="330" y="-12" width="140" height="24" rx="6" fill="#fde68a"/>
      <text x="340" y="5" class="chip">REWARDS</text>
      <rect x="490" y="-12" width="120" height="24" rx="6" fill="#e9d5ff"/>
      <text x="500" y="5" class="chip">SFT</text>
      <rect x="630" y="-12" width="150" height="24" rx="6" fill="#bae6fd"/>
      <text x="640" y="5" class="chip">OUTPUT</text>
      <rect x="800" y="-12" width="170" height="24" rx="6" fill="#fecaca"/>
      <text x="810" y="5" class="chip">HUMAN PREFERENCE</text>
    </g>
  </g>

  <!-- Column titles -->
  <g transform="translate(40,170)">
    <text class="subtitle">1) DeepSeek-R1-Zero</text>
  </g>
  <g transform="translate(450,170)">
    <text class="subtitle">2) DeepSeek-R1 (Cold Start + RL + SFT + RL)</text>
  </g>
  <g transform="translate(930,170)">
    <text class="subtitle">3) DeepSeek-R1-Distill</text>
  </g>

  <!-- Panel 1: R1-Zero -->
  <g transform="translate(40,190)">
    <rect width="360" height="560" rx="16" fill="url(#panelGrad)" filter="url(#shadow)"/>
    <g transform="translate(20,24)">
      <text class="label">Base Model</text>
      <text class="body" y="22">No SFT; start directly with RL (GRPO)</text>

      <!-- Inputs -->
      <g transform="translate(0,56)">
        <text class="label">Inputs</text>
        <text class="body" y="20">• Reasoning prompts (math, code, logic)</text>
        <text class="body" y="40">• Auto-checkable tasks</text>
      </g>

      <!-- RL Box -->
      <g transform="translate(0,120)">
        <rect width="320" height="130" rx="12" fill="#ecfdf5" stroke="#10b981"/>
        <text class="label" x="12" y="22">Reinforcement Learning (GRPO)</text>
        <text class="body" x="12" y="44">• Sample groups of outputs</text>
        <text class="body" x="12" y="64">• Optimize by relative group advantage</text>
        <text class="body" x="12" y="84">• No critic model</text>
      </g>

      <!-- Rewards -->
      <g transform="translate(0,270)">
        <rect width="320" height="110" rx="12" fill="#fffbeb" stroke="#f59e0b"/>
        <text class="label" x="12" y="22">Reward Signals</text>
        <text class="body" x="12" y="44">• Accuracy: rule-based checking</text>
        <text class="body" x="12" y="64">• Format: enforce +lt;think+gt;...+lt;/think+gt;</text>
      </g>

      <!-- Emergent Behavior -->
      <g transform="translate(0,400)">
        <rect width="320" height="120" rx="12" fill="#eef2ff" stroke="#6366f1"/>
        <text class="label" x="12" y="22">Emergent Behaviors</text>
        <text class="body" x="12" y="44">• Longer test-time reasoning</text>
        <text class="body" x="12" y="64">• Reflection and re-evaluation</text>
        <text class="body" x="12" y="84">• "Aha" moments</text>
      </g>

      <!-- Output -->
      <g transform="translate(0,540)">
        <rect width="320" height="60" rx="12" fill="#e0f2fe" stroke="#38bdf8"/>
        <text class="label" x="12" y="22">Output Checkpoint</text>
        <text class="kpi" x="12" y="44">AIME pass@1 ≈ 71% (86.7% with voting)</text>
      </g>
    </g>
  </g>

  <!-- Panel 2: R1 -->
  <g transform="translate(450,190)">
    <rect width="420" height="560" rx="16" fill="url(#panelGrad)" filter="url(#shadow)"/>
    <g transform="translate(20,24)">
      <text class="label">Cold Start SFT</text>
      <text class="body" y="22">Thousands of long CoT; readable pattern</text>
      <text class="body" y="42">Format: |special_token|+lt;reasoning+gt;|special_token|+lt;summary+gt;</text>

      <!-- RL Stage 1: Reasoning-oriented -->
      <g transform="translate(0,76)">
        <rect width="380" height="130" rx="12" fill="#ecfdf5" stroke="#10b981"/>
        <text class="label" x="12" y="22">Reinforcement Learning (Reasoning)</text>
        <text class="body" x="12" y="44">• Rule-based accuracy rewards</text>
        <text class="body" x="12" y="64">• Language consistency reward</text>
        <text class="body" x="12" y="84">• Sum of rewards for optimization</text>
      </g>

      <!-- Rejection Sampling + SFT -->
      <g transform="translate(0,226)">
        <rect width="380" height="120" rx="12" fill="#faf5ff" stroke="#a855f7"/>
        <text class="label" x="12" y="22">Rejection Sampling + SFT</text>
        <text class="body" x="12" y="44">• ~600k reasoning samples (correct, readable)</text>
        <text class="body" x="12" y="64">• ~200k non-reasoning (writing, QA, etc.)</text>
      </g>

      <!-- RL Stage 2: All-scenario alignment -->
      <g transform="translate(0,366)">
        <rect width="380" height="130" rx="12" fill="#ecfdf5" stroke="#10b981"/>
        <text class="label" x="12" y="22">Reinforcement Learning (All Scenarios)</text>
        <text class="body" x="12" y="44">• Reasoning: rule-based rewards</text>
        <text class="body" x="12" y="64">• General: preference models (helpful/harmless)</text>
        <text class="body" x="12" y="84">• Focus helpfulness on final summary</text>
      </g>

      <!-- Output -->
      <g transform="translate(0,516)">
        <rect width="380" height="60" rx="12" fill="#e0f2fe" stroke="#38bdf8"/>
        <text class="label" x="12" y="22">User-friendly Reasoning Model</text>
        <text class="body" x="12" y="44">Clear CoT, strong general capabilities</text>
      </g>
    </g>
  </g>

  <!-- Panel 3: Distill -->
  <g transform="translate(930,190)">
    <rect width="310" height="560" rx="16" fill="url(#panelGrad)" filter="url(#shadow)"/>
    <g transform="translate(20,24)">
      <text class="label">Supervised Distillation (SFT only)</text>
      <text class="body" y="22">Use ~800k curated samples from R1</text>
      <text class="body" y="42">Focus: transfer reasoning traces to smaller models</text>

      <g transform="translate(0,76)">
        <rect width="270" height="130" rx="12" fill="#eef2ff" stroke="#6366f1"/>
        <text class="label" x="12" y="22">Target Models</text>
        <text class="body" x="12" y="44">Qwen2.5: 1.5B, 7B, 14B, 32B</text>
        <text class="body" x="12" y="64">Llama: 8B, 70B instruct</text>
      </g>

      <g transform="translate(0,226)">
        <rect width="270" height="120" rx="12" fill="#fffbeb" stroke="#f59e0b"/>
        <text class="label" x="12" y="22">Training</text>
        <text class="body" x="12" y="44">• SFT only (no RL in this step)</text>
        <text class="body" x="12" y="64">• Emphasis on readable CoT</text>
      </g>

      <g transform="translate(0,366)">
        <rect width="270" height="120" rx="12" fill="#e0f2fe" stroke="#38bdf8"/>
        <text class="label" x="12" y="22">Outcome</text>
        <text class="body" x="12" y="44">Smaller models with stronger reasoning</text>
        <text class="body" x="12" y="64">Cost-effective deployment</text>
      </g>
    </g>
  </g>

  <!-- Flows between panels -->
  <g stroke="#475569" fill="none" stroke-width="2" marker-end="url(#arrow)">
    <path d="M 400 460 C 430 460 430 250 470 250"/>
    <path d="M 870 430 C 900 430 900 300 930 300"/>
  </g>

  <!-- Footer notes -->
  <g transform="translate(40,770)">
    <text class="note">Notes: GRPO = Group Relative Policy Optimization. Rewards combine rule-based accuracy, format, and language consistency; later stages incorporate preference modeling for helpfulness/harmlessness.</text>
  </g>
</svg> 